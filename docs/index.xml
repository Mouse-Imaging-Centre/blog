<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The Mouse Imaging Centre Blog</title>
    <link>/blog/</link>
    <description>Recent content on The Mouse Imaging Centre Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 10 Aug 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Highlights From APPNING and ISMRM 2018</title>
      <link>/blog/post/2018-08-10_ismrm-highlights/</link>
      <pubDate>Fri, 10 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-08-10_ismrm-highlights/</guid>
      <description>&lt;div id=&#34;jacobs-highlights&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Jacob’s Highlights&lt;/h1&gt;
&lt;p&gt;I recently attended the Workshop on Animal Population Imaging (APPNING 2018) held after the ISMRM conference in Paris. It was a well run workshop highlighting the benefits and troubles with large scale animal population imaging (&lt;a href=&#34;https://appning2018.sciencesconf.org/&#34;&gt;&lt;em&gt;https://appning2018.sciencesconf.org/&lt;/em&gt;&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Several Highlights from the workshop:&lt;/p&gt;
&lt;div id=&#34;quantitative-connectomic-histology&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quantitative Connectomic Histology&lt;/h2&gt;
&lt;p&gt;Presented by GA Johnson from Duke University&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Used Diffusion Tensor Imaging to assess how the brain was connected in the mouse. Attempt to determine the structural connectivity of mouse brain and compare with the tracer studies performed at the Allen Brain Institute
&lt;ul&gt;
&lt;li&gt;Tractograms currently have more invalid than valid bundles, and therefore they are attempting to be as accurate as possible&lt;/li&gt;
&lt;li&gt;Goal was to push the technology as hard as possible to get the best resolution and images possible&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;First scan that pushed the boundaries was a 120 direction DTI scan with b-values up to 4000 s/mm2. Full details of this sequence can be found in Calabrese et al. Cerebral Cortex 2015 25(11): 4628-37
&lt;ul&gt;
&lt;li&gt;Total scan time for that sequence was 235 hours!&lt;/li&gt;
&lt;li&gt;Ten day scan protocol was not practical long term so they decided to work on ways to speed it up.
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Less angular resolution from 120 directions to 46&lt;/li&gt;
&lt;li&gt;Use Compressed Sensing&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Final protocol using compressed sensing got their scan time down to 11 hours and 45um resolution, which was quite impressive.
&lt;ul&gt;
&lt;li&gt;The images look gorgeous and the study was well done, but scanning one mouse at a time for 11 hours is not practical for our projects. Additionally the practicality of using tractography as a ground truth for connectivity studies seems questionable.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;tech-demonstrations&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tech demonstrations&lt;/h2&gt;
&lt;p&gt;Several of the talks highlighted the individuals “toys” or techniques that they have created to help speed up their own research and they were sharing them with all of us.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SBA Composer - Web based software for 3D imaging&lt;/li&gt;
&lt;li&gt;An Electronic Collection of Vertebrate Brains - &lt;a href=&#34;https://braincatalogue.org&#34;&gt;&lt;em&gt;https://braincatalogue.org&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sammba- MRI - Imaging toolbox for small animal imaging (&lt;a href=&#34;http://sammba-mri.github.io&#34;&gt;&lt;em&gt;http://sammba-mri.github.io&lt;/em&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Dicomifier - A generic Bruker-DICOM-NIfTI convertor (&lt;a href=&#34;https://github.com/lamyj/dicomifier&#34;&gt;&lt;em&gt;https://github.com/lamyj/dicomifier&lt;/em&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;diffusion-spectroscopy&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diffusion Spectroscopy&lt;/h2&gt;
&lt;p&gt;One of the Posters discussed DW-MRS in a mouse model:&lt;/p&gt;
&lt;p&gt;Glial and axonal changes in the cuprizone mouse model investigated with diffusion magnetic resonance spectroscopy.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This was particularly interesting to me as I did my PhD on DW-MRS.&lt;/li&gt;
&lt;li&gt;This technique allows you to probe the underlying tissue microstructure, by allowing you to assess specific markers of axonal, myelin, and glial cell damage&lt;/li&gt;
&lt;li&gt;The mouse model they used was a cuprizone mouse, which has been shown to reproduce pathological features of MS&lt;/li&gt;
&lt;li&gt;The aim of the study was to compare the concentrations and ADC values of the different metabolites in the brain to see if they can determine any biomarkers of axonal, myelin, and glial cells injuries&lt;/li&gt;
&lt;li&gt;They conclude that markers like a decrease in NAA/Cr ratio with no variation in NAA ADC could reflect myelination damage, and intact axons. Additionally, they conclude that an increase of Cho and Inositol ADCs could be caused by glial cell activation and swelling&lt;/li&gt;
&lt;li&gt;While it is true that the conclusions reached in the DW-MRS study are feasible, they are a long way from providing a true biomarker and are just speculating at this point.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;leighs-highlights&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Leigh’s Highlights&lt;/h1&gt;
&lt;p&gt;I was happy to attend ISMRM 2018 in Paris. Below are a few of my favourite sessions.&lt;/p&gt;
&lt;div id=&#34;diffusion-mrs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diffusion MRS&lt;/h2&gt;
&lt;p&gt;There were a few talks on this, and it played a prominent role in the educational sessions (*&lt;a href=&#34;https://www.ismrm.org/18/program\_files/WE18AB.htm*&#34; class=&#34;uri&#34;&gt;https://www.ismrm.org/18/program\_files/WE18AB.htm*&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The goal is to measure typical diffusion parameters for individual metabolites, rather than water, which is the the more common approach.&lt;/li&gt;
&lt;li&gt;Parameters include:
&lt;ul&gt;
&lt;li&gt;apparent diffusion coefficient (ADC)&lt;/li&gt;
&lt;li&gt;fractional anisotropy (FA)&lt;/li&gt;
&lt;li&gt;axial diffusivity (AD)&lt;/li&gt;
&lt;li&gt;and radial diffusivity (RD)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;While water is present in all cell types, metabolites can be cell-type specific, allowing us to gain diffusion information about individual cell types. For example, neurons contain NAA, while glia contain tCho.&lt;/li&gt;
&lt;li&gt;One method of measuring diffusion MRS is to modify the PRESS sequence to include diffusion pulses.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Francesca Branzoli&lt;/strong&gt; - An example presented at ISMRM was the use of diffusion MRS to study multiple sclerosis patients, where the NAA axial diffusivity in the corpus callosum is different from that of water.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;normal-pediatric-brain-development&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Normal Pediatric Brain Development&lt;/h2&gt;
&lt;p&gt;This session was mainly about studying markers of brain development in young children under different developmental circumstances (*&lt;a href=&#34;https://www.ismrm.org/18/program\_files/O02.htm*&#34; class=&#34;uri&#34;&gt;https://www.ismrm.org/18/program\_files/O02.htm*&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Joseph Yang&lt;/strong&gt; - One speaker found that signal obtained from the the ratio of T1W to T2W scans was correlated with myelin content. As myelination progresses the signal from this ratio increases, but the trend is non-linear with age.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Douglas Dean&lt;/strong&gt; - Gestational cortisol levels affect myelination in the brain of unborn children. Poor cortisol regulation in mothers is associated with dispersed white matter fibers and altered white matter development in children. The authors used NODDI MRI to study white matter integrity in children, and measured cortisol levels in the mother population. Interestingly, they found that there is a large distribution of cortisol in the mother population.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sean Deoni&lt;/strong&gt; - Breastfeeding and brain development. In one study, the authors aimed to compare the white matter development in children that were either breastfed or formula-fed for three months. The authors tested three commercial formula brands against a breastfed control group. They found that myelin development in babies was impaired in two of the three formula-fed groups, as compared to the breast-fed group. However, the speaker would not say which brand gave similar results to the breastfed group.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;lectures-and-posters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Lectures and Posters&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Plenary Lecture: Point-of-Care Diagnostics: MR’s Friend or Foe &lt;strong&gt;Steven J. Schiff&lt;/strong&gt;, Sustainable Low-Field MRI for Point of Care Diagnostics (*&lt;a href=&#34;https://www.ismrm.org/18/program\_files/P03.htm*&#34; class=&#34;uri&#34;&gt;https://www.ismrm.org/18/program\_files/P03.htm*&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This talk, in part, focused on portable healthcare for developing countries - an example was a van that drove around in African nations delivering TB vaccines.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;As researchers in MRI, we tend to move towards high-field, high-resolution systems that push the limits, but are expensive.&lt;/li&gt;
&lt;li&gt;He pointed out that many diseases can be diagnosed with lower-resolution images, which do not require expensive, state-of-the-art, equipment. There is a significant need in developing countries for affordable diagnostic tools. Low field, inexpensive magnets might be able to provide a solution for this.
&lt;ul&gt;
&lt;li&gt;It would benefit many people if researchers began to think about ways to make more affordable magnets, which deliver images with lower resolution that would help in diagnosing common diseases in developing nations.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;E-Poster presented by &lt;strong&gt;Junyu Guo&lt;/strong&gt;, a student of Gene Reddick from St.Jude Children’s Hospital (*&lt;a href=&#34;https://www.ismrm.org/18/program\_files/EP21.htm\#sub5*&#34; class=&#34;uri&#34;&gt;https://www.ismrm.org/18/program\_files/EP21.htm\#sub5*&lt;/a&gt;)
&lt;ul&gt;
&lt;li&gt;Myelination occurs asymmetrically across the brain during development, left/right and front/back. This asymmetry is detectible with DTI metrics.&lt;/li&gt;
&lt;li&gt;Junyo was studying the development of the asymmetry in myelination in children after treatment with high-dose methotrexate for acute lymphoblastic leukemia.&lt;/li&gt;
&lt;li&gt;After treatment, patients have LESS asymmetry than controls of the same age.&lt;/li&gt;
&lt;li&gt;Damage to myelin slows development of natural asymmetry.&lt;/li&gt;
&lt;li&gt;Asymmetry measurements detected differences between patients and controls before traditional T2-weighted scans, usually used to detect leucoencephalopathy in this patient population.&lt;/li&gt;
&lt;li&gt;Children treated at younger ages were more severely impacted, having more significant impairment, and less asymmetry than controls.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;stephanias-highlights&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Stephania’s Highlights&lt;/h1&gt;
&lt;p&gt;Recently, I was fortunate to attend the ISMRM 2018 Annual Meeting. It was the first time I attended this meeting, so it was a rather eye-opening experience. The main topics of interest seemed to be MR Spectroscopy (MRS), Diffusion, and Machine learning applications to most fields of MRI. Most of the information presented was novel to me. So, any inaccuracies are, of course, due to my elementary knowledge of these advanced topics.&lt;/p&gt;
&lt;div id=&#34;diffusion-validation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diffusion Validation&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://www.ismrm.org/18/program_files/O72.htm&#34;&gt;&lt;em&gt;https://www.ismrm.org/18/program_files/O72.htm&lt;/em&gt;&lt;/a&gt;&lt;/em&gt;) *&lt;/p&gt;
&lt;p&gt;I chose to highlight this session because it is an important aspect of method development that is often overlooked. From all abstracts presented I have selected 3 which I found the most intriguing.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Phantoms are extensively used for testing and validation purposes. In Diffusion MRI, a lot of weight has been put primarily on anisotropic phantoms. &lt;strong&gt;Papaioannou et al.&lt;/strong&gt; from &lt;em&gt;NYU&lt;/em&gt;, introduced a permeable phantom with tunable microstructural characteristics (pore size, pore density and permeability).
&lt;ul&gt;
&lt;li&gt;Model molecular transport in the presence of permeable membranes.&lt;/li&gt;
&lt;li&gt;Time-dependent diffusion experiments were performed by varying the pore size.&lt;/li&gt;
&lt;li&gt;Experiments performed on 3 different MRI and NMR systems.&lt;/li&gt;
&lt;li&gt;Results nicely agree with theoretical expectations, for all three systems:
&lt;ul&gt;
&lt;li&gt;diffusivity time dependence in z but not in the x and y directions.&lt;/li&gt;
&lt;li&gt;kurtosis (non-monotonic) time dependence in z but not in the x and y directions.&lt;/li&gt;
&lt;li&gt;power-law scaling of diffusivity with respect to time in z.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Such a phantom, that allows a controlled modification of its microstructural properties, opens the door to studying molecular diffusion in the presence of permeable membranes, which is rather ubiquitous in living tissues. Thus, a different aspect of phantom studies in diffusion can be explored.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A classic method for validating Diffusion MR (dMR) data is comparing the results to Histology results. &lt;strong&gt;Howard et al.&lt;/strong&gt; from &lt;em&gt;The University of Oxford&lt;/em&gt;, proposed a different method for using Histology in dMR data validation. It involves joint modelling of diffusion MRI and Histology data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;When modelling dMR data the signal intensity is approximated to equal the convolution of the single-fibre response function (FRF) and the within-voxel fibre orientation distribution function (ODF).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Constrained spherical deconvolution (CSD) is commonly used to characterize the distribution of fibre orientation.&lt;/li&gt;
&lt;li&gt;Main assumption of CSD is that there is a “brain-wide” FRF that is approximated.&lt;/li&gt;
&lt;li&gt;However, this assumption is challenged by the fact that the white matter (WM) diffusion profile is dependent on many microstructural parameters (axonal density, axonal packing, myelination).&lt;/li&gt;
&lt;li&gt;Joint model (JM) allows for joint estimation of the FRF and ODF where FRF estimation is constrained by the 2D histological information on the ODF.&lt;/li&gt;
&lt;li&gt;Results obtained from the corpus callosum and the corticospinal tract:
&lt;ul&gt;
&lt;li&gt;the FRF depends on local anatomy&lt;/li&gt;
&lt;li&gt;current method of empirically estimating the FRF can bias the ODF shape.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;JM method allows for more accurate FRF and ODF estimation. It can be further enhanced by additionally using 3D information obtained by polarized light imaging for better characterization and more constrained estimation of the FRF and ODF.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Influenced by the advancements in computational neuroscience, &lt;strong&gt;Palombo et al.&lt;/strong&gt; from &lt;em&gt;University College London&lt;/em&gt;, developed a numerical simulation of molecular diffusion within realistic digitized brain cells (neurons, glia):
&lt;ul&gt;
&lt;li&gt;Numerical simulations are used in various fields for model development and assessment as well as for experimental design, mainly because they can overcome various experimental limitations.&lt;/li&gt;
&lt;li&gt;Explore the relationship between the macroscopic diffusion weighted NMR (DW-NMR) signal and the underlying tissue microstructure. Has proven complex to describe.&lt;/li&gt;
&lt;li&gt;Pipeline:
&lt;ol style=&#34;list-style-type: lower-roman&#34;&gt;
&lt;li&gt;In-house software that generates a 3D mesh of the skeletonized structure (brain cell network).&lt;/li&gt;
&lt;li&gt;Step (i) output passed through commercially available packages, CAMINO and the TREES toolbox, for Diffusion NMR simulation and cell morphology analysis respectively.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The work presented focused on reducing computational burden and ensuring correct connectivity between distinct compartments comprising the system. Further work will be done to support even more realistic conditions (dense packing of numerous 3D cell structures, varying cell surface permeability). Overall, computational validation enables testing the limits of biophysical models and even development of new models, eliminating many of the common experimental limitations.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;myelin-and-microstructural-imaging&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Myelin and Microstructural Imaging&lt;/h2&gt;
&lt;p&gt;A lot of work presented involved Magnetization Transfer Imaging (MTI) and comparison of various commonly used techniques. There are two talks I found most relevant for the work done in our lab:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;(&lt;a href=&#34;https://www.ismrm.org/18/program_files/O54.htm&#34;&gt;&lt;em&gt;https://www.ismrm.org/18/program_files/O54.htm&lt;/em&gt;&lt;/a&gt;) &lt;strong&gt;Lam et al.&lt;/strong&gt; &lt;em&gt;from UBC&lt;/em&gt;, explored how white and grey matter intensities vary by technique.
&lt;ul&gt;
&lt;li&gt;They compared:
&lt;ul&gt;
&lt;li&gt;MTI&lt;/li&gt;
&lt;li&gt;inhomogeneous MTI (ihMTI)&lt;/li&gt;
&lt;li&gt;Myelin Water Imaging (MWI).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Using samples from post-mortem healthy and multiple sclerosis (MS) human brain tissues.&lt;/li&gt;
&lt;li&gt;They showed:
&lt;ul&gt;
&lt;li&gt;for MTI and ihMTI the contrast obtained is based on different tissue properties than for MWI.&lt;/li&gt;
&lt;li&gt;Expected since MTI and ihMTI are more sensitive to dipolar couplings whereas MWI is more affected by water exchange.&lt;/li&gt;
&lt;li&gt;Histological validation of their results is necessary.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This work comes to show that, in the realm of MT and MWI, there is still some variation in techniques and the choice of which one to use depends on what molecules we wish to target.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;(&lt;/em&gt;&lt;a href=&#34;https://www.ismrm.org/18/program_files/O13.htm&#34;&gt;&lt;em&gt;https://www.ismrm.org/18/program_files/O13.htm&lt;/em&gt;&lt;/a&gt;&lt;em&gt;)&lt;/em&gt; &lt;strong&gt;Soustelle et al.&lt;/strong&gt; from &lt;em&gt;Université de Strasbourg,&lt;/em&gt; compared &lt;em&gt;quantitative MT&lt;/em&gt;, &lt;em&gt;diffusion tensor imaging (DTI)&lt;/em&gt; and &lt;em&gt;ultrashort-echo time&lt;/em&gt; measurements in a mouse model of demyelination.
&lt;ul&gt;
&lt;li&gt;A sketch of the experiment:
&lt;ul&gt;
&lt;li&gt;Cuprizone was the drug used to cause global demyelination of white and grey matter.&lt;/li&gt;
&lt;li&gt;All of the techniques used are sensitive to myelin content, but each is affected by other factors as well.&lt;/li&gt;
&lt;li&gt;Data was collected from ROIs in the corpus callosum, external capsules and cortex.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;They showed:
&lt;ul&gt;
&lt;li&gt;ultrashort-echo time measurements are more informative than the DTI diffusivity&lt;/li&gt;
&lt;li&gt;ultrashort-echo time measurements are less informative than the MTI bound-pool fraction metric.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, there is a stratification of the various myelin sensitive techniques, with MTI providing more information, despite its limitations.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;lukes-highlights&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Luke’s Highlights&lt;/h1&gt;
&lt;p&gt;ISMRM 2018 was a very enlightening experience for me. This was my first time attending an international science conference, and I would like to share some of my highlights from this conference:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Imaging Metabolism in the Developing Brain (&lt;/strong&gt;&lt;a href=&#34;https://www.ismrm.org/18/program_files/Tu02.htm&#34;&gt;&lt;strong&gt;&lt;em&gt;https://www.ismrm.org/18/program_files/Tu02.htm&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;This session was categorized as an educational session and I greatly appreciated how it differed from the scientific sessions by starting with a thorough introduction to the topic.&lt;/li&gt;
&lt;li&gt;Based on some brief survey questions asked by one of the speakers, it became clear to me that magnetic resonance spectroscopy (MRS), although very useful in the context of studying brain metabolism, does not get used much in the clinic.&lt;/li&gt;
&lt;li&gt;This session promoted the use of MRS for diagnosis and disease monitoring of neurometabolic diseases - Moyoko Tomiyasu (Kanagawa Children’s Medical Center) explained how subtle spectral abnormalities, such as elevated/reduced peaks, can reflect deficiencies in various molecular transporters and enzymes.&lt;/li&gt;
&lt;li&gt;Stefan Bluml (USC) gave an interesting talk on neuroimaging techniques used to study brain metabolism in pediatric populations. He explained that accumulation of a 13C label on glutamate can represent glucose uptake/metabolism in the brain, which were measured in adults and premature newborns. The presented data showed that premature infants displayed prolonged uptake while the adults have a plateau and subsequent decline in their glucose uptake.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Traumatic Brain Injury (&lt;/strong&gt;&lt;a href=&#34;https://www.ismrm.org/18/program_files/O09.htm&#34;&gt;&lt;strong&gt;&lt;em&gt;https://www.ismrm.org/18/program_files/O09.htm&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;)&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;One of the talks that stood out during this session was presented by Chia-Feng Lu et al (Taipei Medical University) and this talk introduced an association between concussion-induced dizziness and damage of thalamo-cortical connectivity after mild traumatic brain injury. The thalamo -cortical tracts involved were primarily connections to the primary and associated somatosensory cortices. These tracts displayed decreased functional connectivity which was often accompanied by elevated scores of Dizziness Handicap Inventory (i.e. increased dizziness).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Overall, I would highly recommend this conference to any first-timers. Despite my constant need for caffeine throughout the week (the lecture rooms were very dark!), it was very exciting to see all the different applications of MR. You can also expect a warm welcome at the special “newbie” reception that they typically host.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Models: Understanding the Error Estimates for Binary Variables</title>
      <link>/blog/post/2018-07-06_linearmodelserrors/</link>
      <pubDate>Fri, 06 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-07-06_linearmodelserrors/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(matlib)
library(knitr)
library(RColorBrewer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The purpose of this document is to understand the parameter and residuals error estimates in a basic linear regression model when working with &lt;strong&gt;binary categorical variables&lt;/strong&gt;. Recall the general model definition:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{e}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; is the &lt;strong&gt;design matrix&lt;/strong&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\beta}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\((p+1)\)&lt;/span&gt;-vector of coefficients/parameters, including the intercept parameter. The errors are normally distributed around 0 with variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[e \sim N(0,\sigma^2) \quad .\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Upon fitting the model to data, we obtain estimates for the coefficients, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;. These estimates have an associated covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_\beta\)&lt;/span&gt;, which is used for statistical inference. The covariance matrix of the parameters is calculated from the estimate for the residual standard error in the following way:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{\sigma}^2_\beta = (\mathbf{X}&amp;#39;\mathbf{X})^{-1}\hat{\sigma}^2 \quad .\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The focus of this document will be on understanding the details of this covariance matrix, specifically of the parameter standard errors and the residual standard error, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}\)&lt;/span&gt;. The squared parameter standard errors (i.e. parameter variances) are the diagonal terms in the covariance matrix, and so the two measures of variability are related to one another in the following way:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{diag}[\mathbf{\sigma}^2_\beta] = \text{diag}[(\mathbf{X}&amp;#39;\mathbf{X})^{-1}]\hat{\sigma}^2 \quad .\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The standard errors can then be obtained by taking the square root of the variances. This transformation between the residual standard error and the parameter standard errors is not trivial and depends on the number of parameters and type of variables. Recall that the estimate for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\sigma}^2 = \frac{1}{n-p-1} \sum_{i = 1}^n e^2_i \quad .\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the data includes observations of categorical variables such that it can be pooled into &lt;strong&gt;balanced groups&lt;/strong&gt; with potentially different group sample standard deviations of &lt;span class=&#34;math inline&#34;&gt;\(\sigma_g\)&lt;/span&gt;, it can be shown straightforwardly that if &lt;span class=&#34;math inline&#34;&gt;\(n \gg p\)&lt;/span&gt;, i.e. in the regime of low-dimensional data,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\sigma}^2 = \text{Ave}[\sigma^2_g] \quad .\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If the sample standard deviations of the groups are identical, then &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma} = \sigma_g\)&lt;/span&gt;. What this tells us is that the residual standard error is an estimate on the standard deviation of the groups defined by the model.&lt;/p&gt;
&lt;p&gt;We will get a sense of how these estimates vary by generating some simulated data and playing with different linear models of the data. In particular, we will simulate a &lt;strong&gt;balanced experimental design&lt;/strong&gt; consisting of independent binary categorical variables and a continuous response. We will consider three binary variables: &lt;code&gt;Genotype&lt;/code&gt;, &lt;code&gt;Anxiety&lt;/code&gt;, and &lt;code&gt;Treatment&lt;/code&gt;. The use of binary variables in linear models has the effect of pooling the data across different groups. Since there are 3 binary variables, there will be 8 separate groups, i.e. &lt;span class=&#34;math inline&#34;&gt;\(2^3\)&lt;/span&gt;. To operate well within the low-dimensitonality regime, we will use a large sample size. The data will be simulated by first generating a scaffold data frame containing the observations for the different categorical variables. The scaffold data frame will then be used to generate the response variable stochastically. We will start by generating the scaffold data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Define variables related to the experimental design. 
# Sample size
nSample &amp;lt;- 100
# Number of binary variables
nBinVar &amp;lt;- 3
# Number of distinct groups
nGroups &amp;lt;- 2^nBinVar
#Total number of observations
nObs &amp;lt;- nSample*nGroups

#Generate data frame (handy trick uisng expand.grid() and map_df())
dfGrid &amp;lt;- expand.grid(Genotype = c(&amp;quot;WT&amp;quot;,&amp;quot;MUT&amp;quot;), 
                      Anxiety = c(&amp;quot;No&amp;quot;,&amp;quot;Yes&amp;quot;), 
                      Treatment = c(&amp;quot;Placebo&amp;quot;, &amp;quot;Drug&amp;quot;))
dfSimple &amp;lt;- map_df(seq_len(nSample), ~dfGrid) %&amp;gt;% 
  mutate(Genotype = factor(Genotype, levels = c(&amp;quot;WT&amp;quot;,&amp;quot;MUT&amp;quot;)),
         Anxiety = factor(Anxiety, levels = c(&amp;quot;No&amp;quot;,&amp;quot;Yes&amp;quot;)),
         Treatment = factor(Treatment, levels = c(&amp;quot;Placebo&amp;quot;,&amp;quot;Drug&amp;quot;)))

#Verify that this worked
dfSimple %&amp;gt;% 
  group_by(Genotype, Anxiety, Treatment) %&amp;gt;% 
  count&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 4
## # Groups:   Genotype, Anxiety, Treatment [8]
##   Genotype Anxiety Treatment     n
##   &amp;lt;fct&amp;gt;    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;
## 1 WT       No      Placebo     100
## 2 WT       No      Drug        100
## 3 WT       Yes     Placebo     100
## 4 WT       Yes     Drug        100
## 5 MUT      No      Placebo     100
## 6 MUT      No      Drug        100
## 7 MUT      Yes     Placebo     100
## 8 MUT      Yes     Drug        100&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we create the distribution of the response based on the independent variables. We will generate data in which &lt;strong&gt;there are no interactions between any of the predictor variables&lt;/strong&gt;. The response will be generated using standardized units so that it can stand in for any physical variable. The main effects of the predictors on the response are taken to be &lt;span class=&#34;math inline&#34;&gt;\(2\sigma\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is the standard deviation of the normal distribution used to generate observations of the response.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Simulation parameters
meanRef &amp;lt;- 0
sigma &amp;lt;- 1
effectGenotype &amp;lt;- 2*sigma
effectAnxiety &amp;lt;- 2*sigma
effectTreatment &amp;lt;- 2*sigma

#Generate data based on experimental design. 
#In this case, no interaction between variables.
dfSimple$Response &amp;lt;- meanRef +
  effectGenotype*(as.numeric(dfSimple$Genotype)-1) +
  effectAnxiety*(as.numeric(dfSimple$Anxiety)-1) +
  effectTreatment*(as.numeric(dfSimple$Treatment)-1) +
  rnorm(nrow(dfSimple), 0, sigma)

ggplot(dfSimple, aes(x = Genotype, y = Response, col = Anxiety)) + 
  geom_jitter(width = 0.2) + 
  facet_grid(.~Treatment) + 
  scale_color_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With the data generated we can start considering different linear models to understand the error/variance estimates. In the following Sections we will consider models with and without interactions to examine how the error estimates change. The general process will be to run a given model on the simulated data and examine the details of the transformation from &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\beta\)&lt;/span&gt; for that model. In doing so we will see that a number of patterns emerge.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-interactive-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Non-interactive Models&lt;/h1&gt;
&lt;div id=&#34;model-1-intercept-term-only&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 1: Intercept Term Only&lt;/h2&gt;
&lt;p&gt;The first model is one where the only parameter is the intercept. The &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; estimate returned will be the mean of the data pooled across all groups. The distribution of the pooled response observations is as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dfSimple, aes(x = Response)) + 
  geom_histogram(binwidth = 1,
                 alpha = 0.7,
                 col = &amp;quot;black&amp;quot;,
                 fill = brewer.pal(3,&amp;quot;Set1&amp;quot;)[2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model is written as&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linMod1 &amp;lt;- lm(Response ~ 1, data = dfSimple)
summary(linMod1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Response ~ 1, data = dfSimple)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.1667 -1.4140  0.0039  1.3655  5.5253 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  2.99865    0.07092   42.28   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 2.006 on 799 degrees of freedom&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this simple case, the residual standard error is simply the standard deviation of the full data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfSimple %&amp;gt;% 
  summarise(sd(Response))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   sd(Response)
## 1     2.006061&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What about the standard error of the intercept?&lt;/p&gt;
&lt;p&gt;Since there is only one parameter for the intercept, the design matrix will just be a vector of ones. The transformation &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}&amp;#39;\mathbf{X}\)&lt;/span&gt; is then just the squared norm of the vector and will be equal to the number of observations in the data set, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i=1}^n 1 = n\)&lt;/span&gt;. The inverse operation is just that for a scalar value and we get &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\beta = \frac{\sigma}{\sqrt{n}}\)&lt;/span&gt;. Multiplying the standard error estimate by &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{n}\)&lt;/span&gt; should return the value of the residual standard error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(linMod1)$coefficients[[&amp;quot;(Intercept)&amp;quot;,&amp;quot;Std. Error&amp;quot;]]*sqrt(nObs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.006061&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-2-one-binary-predictor&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 2: One Binary Predictor&lt;/h2&gt;
&lt;p&gt;Next we add one of the binary variables as a predictor in the model. This will have the effect of pooling the data according to the different levels of that predictor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dfSimple, aes(x = Genotype, y = Response)) +
  geom_jitter(width = 0.2,
              col = brewer.pal(3,&amp;quot;Set1&amp;quot;)[2])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this case the intercept estimate will indicate the pooled mean of the wildtype group (or whatever the reference level is for the chosen predictor) and the slope estimate will indicate the difference between the wildtype mean and the pooled mean of the mutant group.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linMod2 &amp;lt;- lm(Response ~ Genotype, data = dfSimple)
summary(linMod2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Response ~ Genotype, data = dfSimple)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7239 -1.1826  0.0098  1.2532  4.5008 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.97414    0.08627   22.88   &amp;lt;2e-16 ***
## GenotypeMUT  2.04902    0.12201   16.79   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.725 on 798 degrees of freedom
## Multiple R-squared:  0.2611, Adjusted R-squared:  0.2602 
## F-statistic: 282.1 on 1 and 798 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What do the variance estimates represent? Remember that the residual standard error is an estimate of the variability across all of the data. As mentioned in the introduction, the residual standard error will be the square root of the average of the sample variances of the two groups. The group variances and the resulting standard error estimate are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfSimple %&amp;gt;% 
  group_by(Genotype) %&amp;gt;% 
  summarise(varPerGroup = var(Response)) %&amp;gt;% 
  ungroup() %&amp;gt;% 
  mutate(sigma = sqrt(mean(varPerGroup)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   Genotype varPerGroup sigma
##   &amp;lt;fct&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 WT              2.78  1.73
## 2 MUT             3.17  1.73&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which is equal to the estimate for the residuals standard error.&lt;/p&gt;
&lt;p&gt;How do the standard errors of the parameters relate to the residual standard error for this model? Let’s compute the transformation explicitly using the design matrix. First we compute &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X&amp;#39;X}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xMat &amp;lt;- model.matrix(linMod2)
t(xMat)%*%xMat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             (Intercept) GenotypeMUT
## (Intercept)         800         400
## GenotypeMUT         400         400&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Indicating the number of observations explicitly, we can see that this matrix is of the form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{X}&amp;#39;\mathbf{X} = n \cdot \begin{bmatrix} 1 &amp;amp; \frac{1}{2} \\ \frac{1}{2} &amp;amp; \frac{1}{2} \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Taking the inverse (which is a straightforward process for a 2x2 matrix such as this) and multiplying by &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, we find that the covariance matrix is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_\beta = \frac{\sigma^2}{n} \cdot \begin{bmatrix} 2 &amp;amp; -2 \\ -2 &amp;amp; 4 \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Specifically, the standard errors of the estimates are given by the square roots of the diagonal terms in this matrix (note that this isn’t a proper matrix operation but think of this as extracting the diagonal elements and then taking the square root of each of them):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{2} &amp;amp; \sqrt{4} \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A point of interest here is that the parameter errors are related to this quantity &lt;span class=&#34;math inline&#34;&gt;\(\frac{\sigma}{\sqrt{n}}\)&lt;/span&gt; but are scaled by some multiplicative factor. Notably, the slope parameter is more uncertain than the intercept. Multiplying the parameter standard errors by the appropriate multiplicative factors, we should recover the residual standard error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vec &amp;lt;- c(sqrt(nObs/2), sqrt(nObs/4))
summary(linMod2)$coefficients[,&amp;quot;Std. Error&amp;quot;]*vec&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) GenotypeMUT 
##     1.72542     1.72542&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-3-two-binary-predictors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 3: Two Binary Predictors&lt;/h2&gt;
&lt;p&gt;In our third model, we consider the effects of two binary predictors without an interaction. This will pool the data into the &lt;span class=&#34;math inline&#34;&gt;\(2^2=4\)&lt;/span&gt; groups defined by these predictors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dfSimple, aes(x = Genotype, y = Response, col = Anxiety)) +
  geom_jitter(width = 0.2) + 
  scale_color_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The model is as followed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linMod3 &amp;lt;- lm(Response ~ Genotype + Anxiety, data = dfSimple)
summary(linMod3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Response ~ Genotype + Anxiety, data = dfSimple)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.7018 -1.0455 -0.0082  1.0402  4.1122 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  0.95205    0.08512   11.19   &amp;lt;2e-16 ***
## GenotypeMUT  2.04902    0.09829   20.85   &amp;lt;2e-16 ***
## AnxietyYes   2.04417    0.09829   20.80   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.39 on 797 degrees of freedom
## Multiple R-squared:  0.5211, Adjusted R-squared:  0.5199 
## F-statistic: 433.5 on 2 and 797 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We expect that the residual standard error should be approximately equal to the average of the group standard deviations. Note that with the addition of new predictors, we will move slowly out of the regime where &lt;span class=&#34;math inline&#34;&gt;\(n \gg p\)&lt;/span&gt; holds.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfSimple %&amp;gt;% 
  group_by(Genotype, Anxiety) %&amp;gt;%
  summarise(varPerGroup = var(Response)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(sigma = sqrt(mean(varPerGroup)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 4
##   Genotype Anxiety varPerGroup sigma
##   &amp;lt;fct&amp;gt;    &amp;lt;fct&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 WT       No             1.95  1.39
## 2 WT       Yes            1.44  1.39
## 3 MUT      No             2.23  1.39
## 4 MUT      Yes            2.12  1.39&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How do the errors of the parameter estimates relate back to this?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xMat &amp;lt;- model.matrix(linMod3)
t(xMat) %*% xMat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             (Intercept) GenotypeMUT AnxietyYes
## (Intercept)         800         400        400
## GenotypeMUT         400         400        200
## AnxietyYes          400         200        400&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Explicitly indicating the number of observations, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \mathbf{X&amp;#39;X} = n \cdot \begin{bmatrix} 1 &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{2} \\ \frac{1}{2} &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{4} \\ \frac{1}{2} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{2} \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Taking the inverse (which is a tedious process for any matrix of dimension greater than 2), the covariance matrix is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_\beta = \frac{\sigma^2}{n}\cdot\begin{bmatrix} 3 &amp;amp; -2 &amp;amp; -2 \\ -2 &amp;amp; 4 &amp;amp; 0 \\ -2 &amp;amp; 0 &amp;amp; 4 \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And the standard errors are given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{3} &amp;amp; \sqrt{4} &amp;amp; \sqrt{4} \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice that this is similar to the mapping from the previous model, except that the intercept error is now estimated using &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{3}\)&lt;/span&gt; rather than &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{2}\)&lt;/span&gt;. Interestingly including an additional predictor does not change the conversion factors for the slope parameters. Applying the appropriate multiplicative factors to the error estimates, we should recover the residual standard error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vec &amp;lt;- c(sqrt(nObs/3), sqrt(nObs/4), sqrt(nObs/4))
summary(linMod3)$coefficients[,&amp;quot;Std. Error&amp;quot;]*vec&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## (Intercept) GenotypeMUT  AnxietyYes 
##    1.390042    1.390042    1.390042&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-4-three-binary-predictors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model 4: Three Binary Predictors&lt;/h2&gt;
&lt;p&gt;In order to get a clearer sense of the trend in the error estimates with regards to binary predictors, we will add the third main effect into the model. In this case the model will utilize the full 8 groups in the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dfSimple, aes(x = Genotype, y = Response, col = Anxiety)) +
  geom_jitter(width = 0.2) + 
  facet_grid(.~Treatment) +
  scale_color_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linMod4 &amp;lt;- lm(Response ~ Genotype + Anxiety + Treatment, data = dfSimple)
summary(linMod4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Response ~ Genotype + Anxiety + Treatment, data = dfSimple)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.05274 -0.64003 -0.00343  0.63785  3.12745 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   -0.03268    0.06928  -0.472    0.637    
## GenotypeMUT    2.04902    0.06928  29.574   &amp;lt;2e-16 ***
## AnxietyYes     2.04417    0.06928  29.504   &amp;lt;2e-16 ***
## TreatmentDrug  1.96946    0.06928  28.425   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.9798 on 796 degrees of freedom
## Multiple R-squared:  0.7623, Adjusted R-squared:  0.7614 
## F-statistic:   851 on 3 and 796 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that this is the proper model to describe the data based on how we’ve simulated it. In this case the intercept should describe the mean of the reference group, i.e. untreated wildtypes with no anxiety, while the slope parameters should estimate the inputs that we put into the model. The residuals standard error should describe the standard deviation of the response within the 8 different groups, which in this case amounts to the value of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; that we specified when simulating the data. The group standard deviations and their average are:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfSimple %&amp;gt;% 
  group_by(Genotype, Anxiety, Treatment) %&amp;gt;% 
  summarise(varPerGroup = var(Response)) %&amp;gt;%
  ungroup %&amp;gt;%
  mutate(sigma = sqrt(mean(varPerGroup)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 5
##   Genotype Anxiety Treatment varPerGroup sigma
##   &amp;lt;fct&amp;gt;    &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 WT       No      Placebo         0.870 0.978
## 2 WT       No      Drug            1.14  0.978
## 3 WT       Yes     Placebo         0.673 0.978
## 4 WT       Yes     Drug            0.742 0.978
## 5 MUT      No      Placebo         1.12  0.978
## 6 MUT      No      Drug            1.02  0.978
## 7 MUT      Yes     Placebo         1.02  0.978
## 8 MUT      Yes     Drug            1.08  0.978&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected the residual standard error is approximately the average of the group standard deviations.&lt;/p&gt;
&lt;p&gt;What about the parameter errors?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xMat &amp;lt;- model.matrix(linMod4)
t(xMat) %*% xMat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               (Intercept) GenotypeMUT AnxietyYes TreatmentDrug
## (Intercept)           800         400        400           400
## GenotypeMUT           400         400        200           200
## AnxietyYes            400         200        400           200
## TreatmentDrug         400         200        200           400&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which gives&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\mathbf{X&amp;#39;X} = n \cdot \begin{bmatrix} 1 &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{2} \\ \frac{1}{2} &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4} \\ \frac{1}{2} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{4} \\ \frac{1}{2} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{2}  \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The full covariance matrix is then:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_\beta = \frac{\sigma^2}{n}\cdot\begin{bmatrix} 4 &amp;amp; -2 &amp;amp; -2 &amp;amp; -2 \\ -2 &amp;amp; 4 &amp;amp; 0 &amp;amp; 0 \\ -2 &amp;amp; 0 &amp;amp; 4 &amp;amp; 0 \\ -2 &amp;amp; 0 &amp;amp; 0 &amp;amp; 4 \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And the standard errors are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{4} &amp;amp; \sqrt{4} &amp;amp; \sqrt{4} &amp;amp; \sqrt{4} \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the trend from the previous Section continues. The conversion factor for the intercept term is related to the number of parameters in the model, while the values related to the slope parameters are still simply &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{4}\)&lt;/span&gt;. We can expect that, as we continue to add binary predictors, the intercept term will be related to the number of parameters, while the slope parameters will have a conversion of &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{4}\)&lt;/span&gt;. As in the previous Sections, we can recover the residual standard error by multiplying by the appropriate factors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vec &amp;lt;- c(sqrt(nObs/4), sqrt(nObs/4), sqrt(nObs/4), sqrt(nObs/4))
summary(linMod4)$coefficients[,&amp;quot;Std. Error&amp;quot;]*vec&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   (Intercept)   GenotypeMUT    AnxietyYes TreatmentDrug 
##     0.9798371     0.9798371     0.9798371     0.9798371&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;recap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Recap&lt;/h2&gt;
&lt;p&gt;At this stage let’s compare the conversion factors from &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\beta\)&lt;/span&gt; for all non-interactive models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(Beta0 = c(1/sqrt(nObs), sqrt(2/nObs), sqrt(3/nObs), sqrt(nObs/4)),
           Beta1 = c(NA, sqrt(4/nObs), sqrt(4/nObs), sqrt(4/nObs)),
           Beta2 = c(NA, NA, sqrt(4/nObs), sqrt(4/nObs)),
           Beta3 = c(NA, NA, NA, sqrt(4/nObs)),
           row.names = c(&amp;quot;Model 1&amp;quot;, &amp;quot;Model 2&amp;quot;, &amp;quot;Model 3&amp;quot;, &amp;quot;Model 4&amp;quot;)) %&amp;gt;% kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Beta0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Beta1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Beta2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Beta3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0353553&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0500000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0707107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0612372&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0707107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0707107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14.1421356&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0707107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0707107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0707107&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As one might expect, we do see some pattern. Specifically, using the total number of observations, we can express this table as:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Beta0&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Beta1&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Beta2&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Beta3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{1}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{2}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{3}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Model 4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As mentioned previously, the multiplicative factor for the intercept error involves the square root of the number of coefficients in the model. Moreover, for the rest of the models, the multiplicative factor is only ever &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;, i.e. the mappings don’t change as we add more binary variables. This makes sense given that all of the variables are independent in these models. Note that there is no pattern when expressing these conversion factors in terms of the number of data points per group, whic we will denote &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{bmatrix} \sqrt{\frac{1}{N}} &amp;amp; &amp;amp; &amp;amp; \\ \sqrt{\frac{1}{N}}&amp;amp; \sqrt{\frac{2}{N}} &amp;amp; &amp;amp; \\ \sqrt{\frac{3}{4N}} &amp;amp; \sqrt{\frac{1}{N}} &amp;amp; \sqrt{\frac{1}{N}} &amp;amp; \\ \sqrt{\frac{1}{2N}} &amp;amp; \sqrt{\frac{1}{2N}} &amp;amp; \sqrt{\frac{1}{2N}} &amp;amp; \sqrt{\frac{1}{2N}} \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Recall that &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; here is different for each row, since the different models pool the data in different ways, and takes on values &lt;span class=&#34;math inline&#34;&gt;\(\{n, \frac{n}{2}, \frac{n}{4}, \frac{n}{8}\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interactive-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Interactive Models&lt;/h1&gt;
&lt;p&gt;In this Section we explore the influence of interactions on the parameter error estimates and the mapping from the residual standard error.&lt;/p&gt;
&lt;div id=&#34;two-binary-predictors-with-interaction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two Binary Predictors with Interaction&lt;/h2&gt;
&lt;p&gt;In this case we consider the interaction between two of the variables, &lt;code&gt;Genotype&lt;/code&gt; and &lt;code&gt;Anxiety&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linModInt &amp;lt;- lm(Response ~ Genotype*Anxiety, data = dfSimple)
summary(linModInt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Response ~ Genotype * Anxiety, data = dfSimple)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.723 -1.048 -0.014  1.044  4.091 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)             0.93091    0.09834   9.466   &amp;lt;2e-16 ***
## GenotypeMUT             2.09130    0.13908  15.037   &amp;lt;2e-16 ***
## AnxietyYes              2.08645    0.13908  15.002   &amp;lt;2e-16 ***
## GenotypeMUT:AnxietyYes -0.08456    0.19668  -0.430    0.667    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.391 on 796 degrees of freedom
## Multiple R-squared:  0.5212, Adjusted R-squared:  0.5194 
## F-statistic: 288.8 on 3 and 796 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that though we have added a new predictor, there are still only 4 groups, as in Model 3. The difference is that the mean values of these groups may be more accurately estimated. In the present case we don’t expect this model to out-perform the model without an interaction, since there is no real interaction in the data, making the interaction parameter superfluous. This means that the estimate for the residual standard error should be similar to that from the non-interactive model. If the situation were reversed however and the data truly contained an interaction, then this model would more appropriately recapitulate the group means and lead to a more accurate estimation of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The design matrix will be different either way however due to the additional interaction predictor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xMat &amp;lt;- model.matrix(linModInt)
t(xMat) %*% xMat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                        (Intercept) GenotypeMUT AnxietyYes
## (Intercept)                    800         400        400
## GenotypeMUT                    400         400        200
## AnxietyYes                     400         200        400
## GenotypeMUT:AnxietyYes         200         200        200
##                        GenotypeMUT:AnxietyYes
## (Intercept)                               200
## GenotypeMUT                               200
## AnxietyYes                                200
## GenotypeMUT:AnxietyYes                    200&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Explicitly using the observation number, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[n \cdot \begin{bmatrix} 1 &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{4} \\ \frac{1}{2} &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4} \\ \frac{1}{2} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{4} \\ \frac{1}{4} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4}  \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The covariance matrix is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^2_\beta = \frac{\sigma^2}{n}\cdot\begin{bmatrix} 4 &amp;amp; -4 &amp;amp; -4 &amp;amp; 4 \\ -4 &amp;amp; 8 &amp;amp; 4 &amp;amp; -8 \\ -4 &amp;amp; 4 &amp;amp; 8 &amp;amp; -8 \\ 4 &amp;amp; -8 &amp;amp; -8 &amp;amp; 16 \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with parameter standard errors of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{4} &amp;amp; \sqrt{8} &amp;amp; \sqrt{8} &amp;amp; \sqrt{16} \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we see a pattern change from the models without an interaction. The intercept mapping still involves a scaling factor that uses the number of parameters in the model, but the standard errors for the main effects parameters are now larger by a factor of &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{2}\)&lt;/span&gt; compared to the model without an interaction. The parameter error for the interaction is also larger than that for the main effects. These considerations will have a slight impact on the inferential side of linear modelling. Specifically, an interaction effect will always be less powerful than a main effect, and a main effect in a model with an interaction will always be less powerful than a main effect in a model without an interaction. The reason for this is that the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;-statistic is computed as &lt;span class=&#34;math inline&#34;&gt;\(t = \hat{\beta}/\sigma_\beta\)&lt;/span&gt;. Of course this depends on what model accurately describes the data. The aforementioned power of a non-interactive model will be thrown off on data with an interaction, since the estimate for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; will be larger due to the interaction in the data. These are things to keep in mind when considering which model to use.&lt;/p&gt;
&lt;p&gt;Applying the mappings to the parameter standard errors, we recover the residual standard error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;vec &amp;lt;- c(sqrt(nObs/4), sqrt(nObs/8), sqrt(nObs/8), sqrt(nObs/16))
summary(linModInt)$coefficients[,&amp;quot;Std. Error&amp;quot;]*vec&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            (Intercept)            GenotypeMUT             AnxietyYes 
##               1.390754               1.390754               1.390754 
## GenotypeMUT:AnxietyYes 
##               1.390754&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;interaction-without-main-effect&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Interaction without Main Effect&lt;/h2&gt;
&lt;p&gt;In this final Section I examine the interesting case of a model with a second order interaction but without the main effect for one of the predictors. What this model does is that it describes data in which the reference group for one of the binary variables (e.g. Wildtypes) is not influenced by observations of another variable (e.g. Anxiety). In order to get the &lt;code&gt;lm()&lt;/code&gt; function to do this properly, we have to create an explicit dummy encoding of the variable with a main effect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfSimple &amp;lt;- dfSimple %&amp;gt;% 
  mutate(GenotypeDummy = case_when(Genotype == &amp;quot;WT&amp;quot; ~ 0,
                                                          Genotype == &amp;quot;MUT&amp;quot; ~ 1))
linModInt2 &amp;lt;- lm(Response ~ GenotypeDummy + GenotypeDummy:Anxiety, data = dfSimple)
summary(linModInt2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Response ~ GenotypeDummy + GenotypeDummy:Anxiety, 
##     data = dfSimple)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.142 -1.130  0.026  1.166  4.091 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)               1.97414    0.07871  25.082  &amp;lt; 2e-16 ***
## GenotypeDummy             1.04807    0.13633   7.688  4.4e-14 ***
## GenotypeDummy:AnxietyYes  2.00189    0.15742  12.717  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.574 on 797 degrees of freedom
## Multiple R-squared:  0.3858, Adjusted R-squared:  0.3842 
## F-statistic: 250.3 on 2 and 797 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that, in comparison to the fully interactive model presented in the previous Section, the residual standard error estimate is different. This is because the model has pooled the anxiety “yes” and “no” groups for the wildtypes. Since the data we generated included a main effect of anxiety, the variance of this wildtype group will be larger than that of the other two groups (mutant-no-anxiety and mutant-yes-anxiety). Additionally, the residual standard error is no longer just the average of the group standard deviations. This is due mainly to the fact that the wildtype group in this model is twice as large as the other two groups. To demonstrate this, let’s compute the naive average of group standard deviations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;(dfTemp &amp;lt;- dfSimple %&amp;gt;% 
  mutate(NewGroups = case_when(Genotype == &amp;quot;WT&amp;quot; ~ &amp;quot;WT&amp;quot;,
                               Genotype == &amp;quot;MUT&amp;quot; &amp;amp; Anxiety == &amp;quot;No&amp;quot; ~ &amp;quot;NoMUT&amp;quot;,
                               Genotype == &amp;quot;MUT&amp;quot; &amp;amp; Anxiety == &amp;quot;Yes&amp;quot; ~ &amp;quot;YesMUT&amp;quot;)) %&amp;gt;% 
  group_by(NewGroups) %&amp;gt;%
  summarise(varPerGroup = var(Response)) %&amp;gt;% 
  ungroup %&amp;gt;% 
  mutate(sigma = sqrt(mean(varPerGroup))))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   NewGroups varPerGroup sigma
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 NoMUT            2.23  1.54
## 2 WT               2.78  1.54
## 3 YesMUT           2.12  1.54&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Observe that the wildtype standard deviation is larger than that for the other groups. The average is not equal to the residual standard error. It can be shown mathematically that in this case the residual standard error can be estimated approximately as&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt((1/4)*(dfTemp$varPerGroup[1] + dfTemp$varPerGroup[3] + 2*dfTemp$varPerGroup[2]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.574038&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An important point is that for the present data, this model is not homoscedastic, which is one of the assumptions underlying inferential statistics using linear models. To move forward we will generate a new data set in which there is no main anxiety effect, only an interaction. This makes it so that the group standard deviations will be approximately the same and put us back in the regime of homoscedasticity. Thus even though the wildtype group will have double the number of observations, the residual standard error will be approximately the average of the group standard deviations. We will ignore the presence of &lt;code&gt;Treatment&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meanRef &amp;lt;- 0
sigma &amp;lt;- 1
effectGenotype &amp;lt;- 2
effectAnxiety &amp;lt;- 2

dfSimple &amp;lt;- dfSimple %&amp;gt;%
  mutate(Response = case_when(Genotype == &amp;quot;WT&amp;quot; ~ rnorm(nrow(.),meanRef,sigma),
                              Genotype == &amp;quot;MUT&amp;quot; &amp;amp; Anxiety == &amp;quot;No&amp;quot; ~ rnorm(nrow(.),meanRef + effectGenotype, sigma),
                              Genotype == &amp;quot;MUT&amp;quot; &amp;amp; Anxiety == &amp;quot;Yes&amp;quot; ~ rnorm(nrow(.), meanRef + effectGenotype + effectAnxiety, sigma)))


ggplot(dfSimple, aes(x = Genotype, y = Response, col = Anxiety)) + 
  geom_jitter(width = 0.2) +
  scale_color_brewer(palette = &amp;quot;Set1&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Re-running the model on this new data, we find:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfSimple &amp;lt;- dfSimple %&amp;gt;% mutate(GenotypeDummy = case_when(Genotype == &amp;quot;WT&amp;quot; ~ 0,
                                                          Genotype == &amp;quot;MUT&amp;quot; ~ 1))
linModInt2 &amp;lt;- lm(Response ~ GenotypeDummy + GenotypeDummy:Anxiety, data = dfSimple)
summary(linModInt2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = Response ~ GenotypeDummy + GenotypeDummy:Anxiety, 
##     data = dfSimple)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.5477 -0.7041  0.0052  0.6270  3.2098 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)               0.04718    0.04902   0.962    0.336    
## GenotypeDummy             1.97039    0.08491  23.206   &amp;lt;2e-16 ***
## GenotypeDummy:AnxietyYes  1.99567    0.09804  20.355   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.9804 on 797 degrees of freedom
## Multiple R-squared:  0.7382, Adjusted R-squared:  0.7376 
## F-statistic:  1124 on 2 and 797 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the parameter estimates recapitulate what we put into the model. Moreover the residual standard error is now approximately equal to the input value of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. We can compute the group standard deviations to see how this relates to the residual standard error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dfSimple %&amp;gt;% 
  mutate(NewGroups = case_when(Genotype == &amp;quot;WT&amp;quot; ~ &amp;quot;WT&amp;quot;,
                               Genotype == &amp;quot;MUT&amp;quot; &amp;amp; Anxiety == &amp;quot;No&amp;quot; ~ &amp;quot;NoMUT&amp;quot;,
                               Genotype == &amp;quot;MUT&amp;quot; &amp;amp; Anxiety == &amp;quot;Yes&amp;quot; ~ &amp;quot;YesMUT&amp;quot;)) %&amp;gt;% 
  group_by(NewGroups) %&amp;gt;%
  summarise(varPerGroup = var(Response)) %&amp;gt;% 
  ungroup %&amp;gt;% 
  mutate(sigma = sqrt(mean(varPerGroup)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   NewGroups varPerGroup sigma
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 NoMUT           0.982 0.995
## 2 WT              0.878 0.995
## 3 YesMUT          1.11  0.995&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case the average is closer to the residual standard error estimate.&lt;/p&gt;
&lt;p&gt;Next we examine the mapping from &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\sigma_\beta\)&lt;/span&gt; to see how it compares to the model with a main anxiety effect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;xMat &amp;lt;- model.matrix(linModInt2)
t(xMat) %*% xMat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                          (Intercept) GenotypeDummy
## (Intercept)                      800           400
## GenotypeDummy                    400           400
## GenotypeDummy:AnxietyYes         200           200
##                          GenotypeDummy:AnxietyYes
## (Intercept)                                   200
## GenotypeDummy                                 200
## GenotypeDummy:AnxietyYes                      200&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ n \cdot \begin{bmatrix} 1 &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{4} \\ \frac{1}{2} &amp;amp; \frac{1}{2} &amp;amp; \frac{1}{4} \\ \frac{1}{4} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4} \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The covariance matrix is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\sigma^2}{n}\cdot\begin{bmatrix} 2 &amp;amp; -2 &amp;amp; 0 \\ -2 &amp;amp; 6 &amp;amp; -4 \\ 0 &amp;amp; -4 &amp;amp; 8 \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which leads to standard errors of&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{2} &amp;amp; \sqrt{6} &amp;amp; \sqrt{8} \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, comparing this model to the previous model with both main effects:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(Intercept = c(sqrt(4/nObs),sqrt(2/nObs)),
           Genotype = c(sqrt(8/nObs), sqrt(6/nObs)),
           Anxiety = c(sqrt(8/nObs), NA),
           GenotypeAnxiety = c(sqrt(16/nObs), sqrt(8/nObs)), 
           row.names = c(&amp;quot;With Main Effect&amp;quot;, &amp;quot;Without Main Effect&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                      Intercept   Genotype Anxiety GenotypeAnxiety
## With Main Effect    0.07071068 0.10000000     0.1       0.1414214
## Without Main Effect 0.05000000 0.08660254      NA       0.1000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the number of observations &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; we find:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Intercept&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Genotype&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Anxiety&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;GenotypeAnxiety&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;With Main Effect&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{8}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{8}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{16}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Without Main Effect&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{2}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{6}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{8}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The patterns from the previous Sections break down in this case. Notably the conversion factor for the intercept term is no longer related to the number of parameters in the model. The standard errors for both the main effect and interaction term are also smaller in this model compared to the model with both main effects, assuming a fixed value of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. This does require caution however, as we saw that the residual standard error may be larger for this model if there is a actually a main effect in the data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In conclusion, we recapitulate the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;-to-&lt;span class=&#34;math inline&#34;&gt;\(\sigma_\beta\)&lt;/span&gt; mappings for the different models that we considered:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Intercept&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Genotype&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Anxiety&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Treatment&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;GenotypeAnxiety&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;NumGroups&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Intercept Only&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{1}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;One Binary Variable&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{2}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Two Binary Variables&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{3}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Three Binary Variables&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Interaction With Main Effect&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{4}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{8}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{8}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{16}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Interaction Without Main Effect&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{2}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{6}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\frac{8}{n}}\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are a few things to keep in mind. First, &lt;strong&gt;the standard deviation of the different groups is captured in the residual standard error estimate&lt;/strong&gt;. Specifically if &lt;span class=&#34;math inline&#34;&gt;\(n \gg p\)&lt;/span&gt;, this estimate is approximately equal to the average of the group sample standard deviations.&lt;/p&gt;
&lt;p&gt;There is no obvious direct relationship between the standard errors of the parameters and the group standard deviations. For instance, the parameter error for the intercept is not equal to the standard error of the reference group, nor is the parameter error for the slope equal to the standard error of the non-reference group. The parameter errors depend on the non-trivial mapping &lt;span class=&#34;math inline&#34;&gt;\((\mathbf{X&amp;#39;X})^{-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;There are however some patterns in the relationship for certain models. Specifically, for balanced binary variable models without interaction, the slope parameter standard errors are always related to the residual standard error by &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{4/n}\)&lt;/span&gt;, regardless of the number of binary variables in the model. The parameter error for the intercept does change however, and &lt;strong&gt;scales with the square root of the number of parameters in the model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The patterns change when we add an interaction to the model. Comparing a two-variable model with an interaction to the corresponding model without an interaction, the parameters have larger errors in the interactive model. The interaction parameter is also the most uncertain parameter in the model. However the intercept parameter error still has a conversion factor related to the number of parameters in the model. If we remove one of the main effects from the model but maintain the interaction, all conversion factors shrink relative to the interactive model with the main effect. However this model should be used with caution as it will likely lead to grouping with uneven variances. On the other hand it can be a useful way to model data if one of the variables is not defined for one of the levels in the main effect, e.g. wildtypes without anxiety scores.&lt;/p&gt;
&lt;p&gt;More complex interactive models were not explored in depth in this document, but for completion I will include the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;-to-&lt;span class=&#34;math inline&#34;&gt;\(\sigma_\beta\)&lt;/span&gt; mappings for two models. The two-variable interaction model described previously can be augmented to include a third variable. The complete interactive model at second order is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Response} \sim \text{Genotype} + \text{Anxiety} + \text{Treatment} + \text{Genotype:Anxiety} + \text{Genotype:Treatment} + \text{Treatment:Anxiety}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The mapping for this model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{7} &amp;amp; \sqrt{12} &amp;amp; \sqrt{12} &amp;amp; \sqrt{12}&amp;amp; \sqrt{16} &amp;amp; \sqrt{16} &amp;amp; \sqrt{16} \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The interactive model at the third order is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Response} \sim \text{Genotype} + \text{Anxiety} + \text{Treatment} + \text{Genotype:Anxiety} + \text{Genotype:Treatment} + \text{Treatment:Anxiety} + \text{Genotype:Anxiety:Treatment}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The mapping for this model is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{8} &amp;amp; \sqrt{16} &amp;amp; \sqrt{16} &amp;amp; \sqrt{16}&amp;amp; \sqrt{32} &amp;amp; \sqrt{32} &amp;amp; \sqrt{32} &amp;amp; \sqrt{64} \end{bmatrix}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The one thing I will mention about these mappings is that the conversion factors for the intercept standard errors continue to be related to the number of parameters in the model. There are likely other interesting patterns in these more complex interactive models, but these will not be explored here.&lt;/p&gt;
&lt;p&gt;Ultimately these specific cases should serve to provide some intuition about how the parameter errors are estimated for a linear model. Keep in mind however that these mappings were computed for a balanced binary experimental design. Group imbalances will skew these values, though the size of these differences will depend on the degree of imbalance. Moreover the mappings will be different in the case of multi-level categorical variables and continuous numerical variables.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An overfit representation of ICLR 2018</title>
      <link>/blog/post/2018-05-30_iclr_redux/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-05-30_iclr_redux/</guid>
      <description>&lt;p&gt;I was recently extremely fortunate to attend ICLR 2018, albeit as something of an interloper. Accordingly, what follows is surely a rather atypical highlight reel. All pedantry and any inaccuracy is, of course, due to my own limited understanding of these elegant topics and the breadth of their application.&lt;/p&gt;
&lt;div id=&#34;causal-reasoning-and-graphical-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Causal reasoning and graphical models&lt;/h2&gt;
&lt;p&gt;There is a well-developed modern theory of causal inference and reasoning based on graphical models developed by Judea Pearl and others. Oft misunderstood and mostly ignored by most statisticians and practitioners, it featured prominently in both contributed papers and invited talks this year.&lt;/p&gt;
&lt;p&gt;Bernhard Schölkopf, the inventor of Support Vector Machines and largely of kernel methods in machine learning, &lt;a href=&#34;https://www.youtube.com/watch?v=4qc28RA7HLQ&#34;&gt;discussed&lt;/a&gt; advances in learning causal models, many of which he worked on, such as in the two-variable case via assumptions on the noise distributions, as well as applications of causal modelling to traditional predictive models, such as semi-supervised learning and covariate shift. I’ve since been reading &lt;a href=&#34;https://mitpress.mit.edu/books/elements-causal-inference&#34;&gt;his (open-access) book&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A &lt;a href=&#34;https://www.youtube.com/watch?v=-maBKmsORwQ&#34;&gt;talk by Suchi Saria&lt;/a&gt; focussed on large datasets in healthcare. She discussed a study involving predicting mortality given test data acquired from patients admitted to hospitals. In this setting, where the illness and subsequent treatment of the patient, as well as other variables regarding the patient and hospital, are occluded, even high-capacity predictive models based on associational data fall flat. At the same time, designing reasonable interventions in this scenario is not obviously even possible, so Saria and collaborators employed the Neyman-Rubin counterfactual framework, a more popular relative of Pearl’s, to predict outcomes in their absence.&lt;/p&gt;
&lt;p&gt;Daphne Koller - of probabilistic graphical modelling fame - held a &lt;a href=&#34;https://www.youtube.com/watch?v=N4mdV1CIpvI&#34;&gt;‘fireside chat’&lt;/a&gt; with (also distinguished!) moderator Yoshua Bengio. In addition to discussing issues of discrimination and harrassment in the machine learning and tech business communities, she devoted much of her talk to a form of career advice: advocating that ML experts work on diverse socially important problems in addition to ‘mental gymnastics’ and ends-agnostic performance improvements. This may call to mind her education work as co-founder of Coursera, but more recently she’s been working in health care - mentioning a just-announced new startup during her talk - in areas like drug discovery, and urged more people to consider this area. Notably, she sees a need for researchers at the intersection of both disciplines rather than pure stats/ML experts expecting to blindly achieve state-of-the-art results on biology datasets or pure biologists with limited understanding of the strengths and limitations of ML. Like Saria, she considers pure DNNs merely one technique out of many and sees this area as needing diverse approaches such as (unsurprisingly…) PGM/causal techniques.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://iclr.cc/Conferences/2018/Schedule?showEvent=274&#34;&gt;Tran and Blei&lt;/a&gt;, the creators of the Edward probabilistic programming language (now part of Tensorflow!), had a paper on applying causal models to GWAS studies. On the causal side of the problem, the authors consider structural models where the causal relations are modelled via neural networks, and note that Cybenko’s universal approximation theorem extends to this situation. On the inference side, evaluating the posterior is intractable, so the authors applied their recently-developed &lt;em&gt;likelihood-free variational inference&lt;/em&gt;, which involves estimating the ratio between two intractable distributions (the posterior and the variational approximation) appearing in the ELBO. I don’t yet understand the details but it’s already available in Edward. Ground truth data, however, is not, so the authors conducted simulations and compared their methods to PCA plus regression, linear mixed models, and logistic factor analysis and showed their implicit causal model to have superior performance even when few causal relationships were present. Sadly, Tran’s opinion is that inferring the causal graph itself at such a scale is likely intractable, but even so it’s clear that such models - and the authors’ work in variational approximations - could be quite valuable in neuroinformatics as well as genomics.&lt;/p&gt;
&lt;p&gt;I was impressed by the attention the subject received - which seems to have coincided with (and maybe caused) an explosion of tutorials and popularizations in the popular press - and hope that continuing interest will help to elucidate the strengths and weaknesses of causal models as well as lead to further research connecting these to other approaches (particularly, under what circumstances can purely statistical approaches recover the conclusions of such models?) as well as more classical areas like logic and reasoning.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-reasoning-and-computation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bayesian reasoning and computation&lt;/h2&gt;
&lt;p&gt;Connections between Bayesian reasoning and neural networks are wide-ranging and fruitful, and several new results were presented.&lt;/p&gt;
&lt;p&gt;One might want to use the learning abilities of NNs to improve Bayesian computation. In this vein, enter &lt;a href=&#34;https://iclr.cc/Conferences/2018/Schedule?showEvent=284&#34;&gt;Levy et al.&lt;/a&gt; on “L2HMC”: using a neural net to learn a useful volume-nonpreserving but detailed-balance-preserving transformation on phase space. (If this sounds familiar, it’s probably because this paper appeared courtesy of Chris at a recent MICe journal club.) It’s an elegant idea which can greatly improve the performance of sampling from previously challenging distributions. I wonder what the transformations look like globally and whether they’re nice/useful across (relevant) phase space or if (hard-to-discover) insufficient model capacity or training schedule - the usual bugbears - might mean that some high-dimensional distributions see no improvement (or even degradation) in some regions.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://iclr.cc/Conferences/2018/Schedule?showEvent=161&#34;&gt;Matthews et al.&lt;/a&gt; prove the convergence in distribution of the output of Bayesian DNNs with rectified-linear neurons to a Gaussian process with a certain kernel, extending work by Neal on shallow networks. As an interesting application, they show how one might attempt to avoid Gaussian process behaviour (which, they note, suggest a lack of hierarchical representation) in situations where it might be undesirable.&lt;/p&gt;
&lt;p&gt;There were many papers on GANs (Generative Adversarial Networks), which can be thought of as networks for approximating probability distributions - perhaps in situations where HMC might be computationally infeasible. It would be quite interesting if anyone has been able to relate the architecture/regularizers of any GANs to priors on the distribution to be learned. Ignorant question: are there any cases where we might be say enough about the ability of a GAN to learn a distribution that we would be able to use one for inference about parameters as one is often interested in science?&lt;/p&gt;
&lt;p&gt;Combining some of the above ideas, &lt;a href=&#34;https://iclr.cc/Conferences/2018/Schedule?showEvent=159&#34;&gt;CausalGAN&lt;/a&gt;, given a causal model, allows sampling from both observational and interventional distributions.&lt;/p&gt;
&lt;p&gt;The elegant and potentially useful &lt;a href=&#34;https://openreview.net/forum?id=Hy7fDog0b&#34;&gt;AmbientGAN&lt;/a&gt; paper considered this problem: you want to create a generative model but all your samples are corrupted by noise. Luckily, you understand the noise distribution. The authors’ solution: you create a generative model in which simulated noise is applied to the generated samples before they’re passed to the discriminator, which as usual attempts to distinguish the real from fake data. The authors prove it’s possible to recover the underlying data distribution in certain noise models; their empirical results suggest both that learning is feasible in the presence of other classes of noise and that their method is robust to a certain degree of noise misspecification.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;neuro-ml&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Neuro &amp;lt;=&amp;gt; ML&lt;/h2&gt;
&lt;p&gt;Blake Richards (UTSC) gave a more biologically-centred &lt;a href=&#34;https://www.youtube.com/watch?v=C_2Q7uKtgNs&#34;&gt;invited talk&lt;/a&gt; on creating accurate neural models of learning in the brain reflecting the lack of anatomical and physiological evidence for backpropagation - the so-called ‘credit assignment’ problem. (Question: what are the implications, if any, of these models for understanding the brain via morphometry?) On the machine learning side, these - very heuristically - suggest using microarchitectures more sophisticated than layers of ‘bare’ neurons, e.g., Hinton’s capsule networks or variations thereof.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pipeline-compilation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pipeline compilation&lt;/h2&gt;
&lt;p&gt;In the modern era of NN frameworks providing GPU execution and automatic differentiation, the first popular frameworks - among them Theano and Tensorflow - allow one to construct the computation graph as a data structure which can then be optimized in some way by the framework. However, this means - roughly - that the architecture must be known independently of the data, which poses problems for interesting networks like RNNs and GNNs. Recent frameworks like Chainer and Pytorch avoid this limitation by constructing the pipeline graph on-the-fly or ‘dynamically’, but this limits possibilities for optimizing the network.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://iclr.cc/Conferences/2018/Schedule?showEvent=520&#34;&gt;DLVM&lt;/a&gt; project (more on this in an upcoming blog post) introduces a DSL embedded in Apple’s Swift programming language and based on ideas present in the Lightweight Modular Staging (LMS) library for Scala, an intermediate representation with support for linear algebra and derivative information, and compilation steps to perform automatic differentiation as a source transformation, hosted on a (modified?) LLVM backend. DLVM is currently not actively developed, but happily that’s because one of the original authors is now working on the similar Swift for Tensorflow project at Google. At the DLVM poster, I learned from another delegate that Facebook has just released &lt;a href=&#34;https://facebook.ai/developers/tools/glow&#34;&gt;Glow&lt;/a&gt; at their own developer conference. Backing from these two ML giants supports the authors’ guess that such technologies will become ubiquitous in the next few years.&lt;/p&gt;
&lt;p&gt;Fei Wang and Tiark Rompf also workshopped a &lt;a href=&#34;https://iclr.cc/Conferences/2018/Schedule?showEvent=429&#34;&gt;paper&lt;/a&gt; on using LMS in Scala to provide a more expressive DSL for constructing static graphs. Notably, they used &lt;em&gt;delimited continuations&lt;/em&gt;, a powerful mechanism for controlling control flow, to obviate the need for an explicit tape for reverse-mode autodiff, essentially using the underlying language’s stack instead. They claim that their DSL removes the need for compiler passes or other source-to-source transformations as in the DLVM model (although I assume DLVM implements a larger set of optimizations).&lt;/p&gt;
&lt;p&gt;I intend to understand the relationships between these elegant techniques, and in particular their relation to staged metaprogramming and the rest of the compilation pipeline, in much more detail in the not-too-distant future.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-topics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other topics&lt;/h2&gt;
&lt;p&gt;Numerous very large and active subject areas like reinforcement learning, applications to audio and language processing and synthesis, and resistance to adversarial examples are entirely slighted here. Of particular interest given the prevalence of graph- theoretic methods in neuroscience, recursive and graph NNs continue to see rapid advances. A large body of work applies such networks to programming problems such as program synthesis and debugging, which will certainly benefit many scientists.&lt;/p&gt;
&lt;p&gt;Perhaps due to the relative youth of the field, even the ‘core’ methods continue to improve. For instance, &lt;a href=&#34;https://iclr.cc/Conferences/2018/Schedule?showEvent=372&#34;&gt;Kidambi et al.&lt;/a&gt; showed theoretically that several popular modifications to SGD have in general no asymptotic benefit, although they’ve developed one known method, Accelerated SGD, which provides superior convergence guarantees. I haven’t even discussed my main interest - deep CNNs - much, but there were obviously many, many papers on these, both on specific architectures/problem domains (mostly 2D images, sadly) and on more fundamental issues such as &lt;a href=&#34;https://openreview.net/forum?id=HkwBEMWCZ&#34;&gt;the topology of skip connections&lt;/a&gt; and &lt;a href=&#34;https://iclr.cc/Conferences/2018/Schedule?showEvent=510&#34;&gt;efficient architecture search&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Overall, as someone new to DNNs, I found this conference extremely useful both for discovering a number of novel technologies as well as understanding current thought in the field.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;acknowlegments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Acknowlegments&lt;/h2&gt;
&lt;p&gt;Chris Hammill read the draft of this text. Thanks especially to my supervisor, Jason Lerch, for letting me attend.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why Relative Volumes Matter</title>
      <link>/blog/post/2018-03-05_why-volumes/</link>
      <pubDate>Tue, 06 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-03-05_why-volumes/</guid>
      <description>&lt;div id=&#34;intro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;Figuring out how one group’s brain is different from another’s is a big part of neuroscience. MRI-neuroanatomy – the study of the sizes of brain regions – is a wonderful tool for this job and makes up a sizeable chunk of what we study. MRI operates in a lovely middle ground in the scales of neuroscience. It is strongly related to macroscale features of the organism, including sex, behaviour, etc; but can also inform us about the effects of microscale factors – such as gene expression.&lt;/p&gt;
&lt;p&gt;While neuroanatomy is tremendously advantageous in the study of &lt;em&gt;what&lt;/em&gt; makes one group’s brain is different from another’s, it is ill-suited to the question of &lt;em&gt;why&lt;/em&gt; they are different. It is ultimately the question of mechanism – the &lt;em&gt;why&lt;/em&gt; – that drives science forward. However, the fact that neuroanatomy can’t identify mechanism doesn’t make it useless. This blog will detail one way that neuroscience benefits from the neuroanatomy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The pursuit of &lt;em&gt;why&lt;/em&gt; begins with the question of &lt;em&gt;what&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;brain-volumes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Brain Volumes&lt;/h2&gt;
&lt;p&gt;There are two types of sizes when one typically talks about brain volumes. The first is absolute size (or absolute volume) which is typically measured in cubic millimeters. This is the true volume of a brain region. The second is relative size (or relative volume), which is the absolute volume divided by whole-brain volume. Both measures have their advantages and disadvantages, however the advantage of relative volume may not be readily apparent. Absolute volumes are built from local microscale properties such as neurons, glia, the dendritic tree, etc. It is the sum of these microscale properties that determines the absolute volume of a region and ultimately contributes to the size of the brain. Relative volumes, on the other hand, normalise to whole brain size and, therefore, don’t have any trivial local influences. This issue of locality naturally leads to a criticism that relative volumes are meaningless compared to absolutes as they can’t help us infer localized changes. This cirticism may appear true at face value, but a closer inspection of real data reveals subtle nuances that strengthen the case for relative volumes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Data&lt;/h2&gt;
&lt;p&gt;The data we will be examining – graciously provided by Lily Qiu – is neuroanatomy data comparing male and female mice over a comprehensive period of neurodevelopment spanning nine timepoints ranging from p3 (i.e 3-day-old mouse, which corresponds to a human fetus) to p65 (i.e 65-day-old mice, which corresponding a human adult). In order to evaluate whether absolute volumes or relative volumes are more useful to study sex differences in the brain, we employed techniques from machine learning to develop a classifier that predicts sex from neuroanatomical structure volumes at a particular age. First, we excluded one mouse’s neuroanatomy data and fit a &lt;a href=&#34;https://web.stanford.edu/~hastie/glmnet/glmnet_beta.html&#34;&gt;LASSO logistic regression&lt;/a&gt; to simulataneously perform feature selection and predict sex from neuroanatomy. Once trained, the model was then provided the excluded mouse’s data to test whether it could successfully predict the excluded mouse’s sex. We repeated this process for every mouse and every age, training a unique classifier each time. Importantly, each classifier was always assessed for accuracy on data it had not seen during training. We trained one set of classifiers to predict sex from absolute volumes and another set to predict sex from relative volumes. Figure 1 shows the accuracy of the classifier and we see that for most ages, classifiers trained using relative volumes predicted sex better than those trained with absolute volumes. More importantly, relative-based classifiers vastly out-performed their absolute-based counterparts at the early ages between 3-17 (p17 corresponds to a human child). While absolute volumes may have a favorable interpretation, an unbiased machine learning procedure favors studying sex differences in terms of relative volumes. Absolute volumes may actually be biased against sex differences at the early ages between p3-17.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/blog/blog/post/2018-03-05_why-volumes_files/figure-html/1-1.png&#34; alt=&#34;*Relative volumes predict sex better than absolute volumes, especially p3-p17.*&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: &lt;em&gt;Relative volumes predict sex better than absolute volumes, especially p3-p17.&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To understand why the classifier performs better with relative volumes over absolute volumes, it is instructive to look at Figure 2, which plots the Coefficient of Variation (CV) over time for the Bed Nucleus of the Stria Terminalis (BNST) measured using absolute and relative volumes. CV is the standard deviation divided by the mean and is therefore unitless. In the BNST, as well as other brain regions, we see a clear pattern where the young brain has high CV in their absolute volumes but remarkably stable CV in relative volumes, therefore explaining why classifiers based on relative volumes outperformed their absolute counterparts at young ages. This consistent low CV is what makes relative volumes so useful in studying sexual dimorphisms. The young brain has quite a lot of variability in growth between subjects that relative volumes effectively correct for. Once corrected, the subtle neuroanatomy that distinguishes males and females becomes clearer to the LASSO-based classifiers. The correction also makes statistical modelling easier as well, as illustrated by Figure 3. The greater variability in the young brain masks statistics relating to when absolute volume sexual dimorphisms emerge, thereby providing a delayed estimate as to when male BNST becomes larger than females (around p10-p17). Relative volumes, however, have sexual dimorphisms emerge much earlier (p5), which is close to and supported by extensive histological evidence. Finally, relative volumes also scale away the overall growth of the brain and can therefore be approximated using linear models. Abolute volumes, on the other hand, need to be fitted with non-linear function, especially when considering their growth throughout a comprehensive period of neurodevelopment. While, in this case, non-linear fitting is not really an issue for statistical significance, the additional degrees-of-freedom needed to properly fit non-linear curves might be too costly in studies with less subjects.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/blog/blog/post/2018-03-05_why-volumes_files/figure-html/2-1.png&#34; alt=&#34;*Relative volumes have low coefficient of variation over the neurodevelopment time period.*&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: &lt;em&gt;Relative volumes have low coefficient of variation over the neurodevelopment time period.&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;/blog/blog/post/2018-03-05_why-volumes_files/figure-html/3-1.png&#34; alt=&#34;*Volume of the BNST over time. We see with absolue volumes that sexual dimorphisms emerge between p10 and p17. Relative volumes on the other hand have dimorphisms emerging around p5. Extensive histological evidence in literature supports the timing of relative volume sexual dimorphisms. Shaded regions represent standard error estimated from linear mixed-effect models.*&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: &lt;em&gt;Volume of the BNST over time. We see with absolue volumes that sexual dimorphisms emerge between p10 and p17. Relative volumes on the other hand have dimorphisms emerging around p5. Extensive histological evidence in literature supports the timing of relative volume sexual dimorphisms. Shaded regions represent standard error estimated from linear mixed-effect models.&lt;/em&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What about the issue of locality I mentioned earlier? Absolute volumes are guaranteed to have local causes, but relative volumes do not. While it is certainly true that relative volumes may have nonlocal causes, we find that nonlocal effects are either exceedingly small in a mouse or rare in a proper image registration procedure. To illustrate relative volumes, please consider Figures 4 and 5. We showed the relative and absolute determinants for a registration of the p3 average mouse brain and the p5 average mouse brain. In both relative and absolute determinants, there is a great deal of agreement as to which structures have a high degree of change from p5 to p3 (cerebellum, cortex, olfactory bulb). Therefore, local causes that affect absolute determinants would affect relative volumes as well. In summary, both relative and absolute volumes are important for volume analysis depending on the situation. Relative volumes are useful when there is a high degree of variation in brain sizes obscuring potentially interesting effects and are generally easier to model. Absolute volumes on the other hand provide information in cannonical units that may be more intuitive to other researchers. Even with just these two measures, we can already get a glimpse into what is happening in the young mouse brain. There is a high degree of variability in the young mouse brain that becomes less noticable with time. &lt;em&gt;Why&lt;/em&gt; does this variability exist? The answer is unclear but it could be related to the uterine position and horn size (McLaurin, et al. 2015). &lt;em&gt;What&lt;/em&gt; can this young variability tell us about the adult mouse? This question can be explored with neuroanatomy.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/blog/img/why_vols_figs/supp_registration.png&#34; alt=&#34;Figure 4: Registration of a source image (p03 average) to a target (p05 average). The native images (after rigid alignment) are on the top row and their overlay is in the middle column. Poor alignment can be found in structures like the cerebellum where there is rapid neonatal growth. Affine registration scales and shears the source image to better align with target image. The affine transformation (generated from the affine registration) is applied to the source image and is shown in the second row. The overlay shows a good match between the affine-transformed source and the target images. However, zooming into the cerebellum of the affine-transformed source and target image (third row), shows that affine registration does not produce proper alignment of the cerebellum. This is illustrated by applying a red contour to the cerebellum of the target image and overlaying this contour to the source image. The non-affine registration corrects this discrepancy (fourth row) and produces the best alignment between source and target images (fifth row).&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;strong&gt;Figure 4:&lt;/strong&gt; &lt;em&gt;Registration of a source image (p03 average) to a target (p05 average). The native images (after rigid alignment) are on the top row and their overlay is in the middle column. Poor alignment can be found in structures like the cerebellum where there is rapid neonatal growth. Affine registration scales and shears the source image to better align with target image. The affine transformation (generated from the affine registration) is applied to the source image and is shown in the second row. The overlay shows a good match between the affine-transformed source and the target images. However, zooming into the cerebellum of the affine-transformed source and target image (third row), shows that affine registration does not produce proper alignment of the cerebellum. This is illustrated by applying a red contour to the cerebellum of the target image and overlaying this contour to the source image. The non-affine registration corrects this discrepancy (fourth row) and produces the best alignment between source and target images (fifth row).&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/blog/blog/img/why_vols_figs/supp_determinant.png&#34; alt=&#34;Figure 5: Visualizing deformations caused by transformation of target image using grids and determinants. Illustrated in the left figure, upon transformation of the target image to the source image (this transformation is the inverse of the transformation in Figure 4), gridlines in the target image become warped. In the top row, the gridlines warp from transformation to the source image; and in the bottom row, the gridlines warp from transformation to the affine-transformed source. Volumetric changes caused by the transformation can be qualitatively assessed by observing how the volume of a square region (region defined by the open space between gridlines) changes after transformation. Absolute determinants capture volumetric changes upon transformation between the Target and Native Source images. Relative determinants capture volumetric changes upon transformation between the Target image and Source image after affine transformation.&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; &lt;em&gt;Visualizing deformations caused by transformation of target image using grids and determinants. Illustrated in the left figure, upon transformation of the target image to the source image (this transformation is the inverse of the transformation in Figure 4), gridlines in the target image become warped. In the top row, the gridlines warp from transformation to the source image; and in the bottom row, the gridlines warp from transformation to the affine-transformed source. Volumetric changes caused by the transformation can be qualitatively assessed by observing how the volume of a square region (region defined by the open space between gridlines) changes after transformation. Absolute determinants capture volumetric changes upon transformation between the Target and Native Source images. Relative determinants capture volumetric changes upon transformation between the Target image and Source image after affine transformation.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;A major criticism of neuroanatomy analysis is that mouse models are useful for the study of cellular mechanisms; and strictly studying brain volumes does not take advantage of what makes mouse research so useful. This criticism ignores the myriad number of informative analysis that mouse models make possible and I will be describing one such analysis in the remainder of this blog. In vivo longitudinal imaging throughout mouse neurodevelopment (provided by Lily Qiu’s data), in conjunction with the control of genetic and environmental factors afforded by mouse studies, renders it possible to explore exciting analysis in brain prediction. Neuroanatomy of the young mouse brain predicts neuroanatomy of the adolescent and adult mouse brain (I will show our work in more detail in a future blogpost). By developing a computational model to predict mouse brain neuroanatomy, we found that only the first 10-17 days of neuroanatomical data are enough to make sensitive and specific predictions of individualised neuroanatomy at p36 and p65. As expected, if we provide the model with only p03 data and ask it to predict p36 or p65, the model will fail to make specific predictions because young brains’ variability is not very informative. However, the more data we provide from later in development, the better the model is at identifying features of variability unique to a mouse and predicting mature individualised neuroanatomy. It is through a combination of many timepoints and more recent timepoints that individual differences in young neuroanatomy emerge, which the model can use to make predictions about mature neuroanatomy. Our prediction accuracies were not influenced by the sex of the predicted subject (i.e. males and females are equally well predicted), however, we found that male neuroanatomy individualised (became easier to predict) significantly earlier in development than female neuroanatomy.&lt;/p&gt;
&lt;p&gt;Neuroanatomical phenotyping tends to be used to study how groups of organisms are similar. However, it is also important to study what makes organisms unique from the rest of the members of its group. The control of genetic and environmental factors limits the variability in mice making it easier for machine-learning tools to make individualised predictions. “Easier” in this context means using simple models and few subjects. While humans have far more variability compared to mice, with the advent of ever-bigger datasets collected on human neurodevelopment, it will be possible in the near future to use more advanced machine-learning tools to predict some individuality in these data sets. It might even be possible to predict the development of neuroanatomical pathologies associated with neurological disorders prior to disorder onset. Neuroanatomy research in mouse models can spur research in this exciting direction. Neuroanatomy can be a marker for individaulity: &lt;em&gt;what&lt;/em&gt; makes mice different from other mice and humans different from other human. I can hardly wait for future science to determine &lt;em&gt;why&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Preferential Spatial Gene Expression in Neuroanatomy</title>
      <link>/blog/post/2018-02-23_gene-expression/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-02-23_gene-expression/</guid>
      <description>&lt;div id=&#34;intro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;In this post I will demonstrate how to use my package &lt;code&gt;ABIgeneRMINC&lt;/code&gt; to download, read and analyze mouse brain gene expression data from the Allen Brain Institute.&lt;/p&gt;
&lt;p&gt;The Allen Brain Institute (ABI) has acquired and released genome-wide spatial gene expression maps for the mouse brain. The data is generated using &lt;em&gt;in situ&lt;/em&gt; hybridization experiments (ISH), where nucleotide probes for specific genes bind directly to mouse brain tissue. The probe binding is then marked with a biotin label that can be used to locate regions where a gene is expressed.&lt;/p&gt;
&lt;p&gt;For the analysis you will need two R packages &lt;code&gt;RMINC&lt;/code&gt; and &lt;code&gt;ABIgeneRMINC&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# devtools::install_github(repo=&amp;quot;DJFernandes/ABIgeneRMINC&amp;quot;)   # If you need to install
library(ABIgeneRMINC)
library(RMINC)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting the data&lt;/h2&gt;
&lt;p&gt;With the packages load you can now look up your favourite gene. You need to know the gene acronym though, which you can find on the NCBI database. In this case, I want to look up Bdnf. The function below queries the Allen Brain API and finds all experiments conducted with Bdnf.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fge=find.gene.experiment(&amp;#39;Bdnf&amp;#39;)
fge&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   gene   slices ExperimentID
## 1 Bdnf  coronal     79587720
## 2 Bdnf sagittal     75695642
##                                                                  URLs
## 1 http://api.brain-map.org/grid_data/download/79587720?include=energy
## 2 http://api.brain-map.org/grid_data/download/75695642?include=energy&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are in luck! There are two experiments the Allen Brain Institute ran with Bdnf, identified by ExperimentIDs 79587720 and 75695642. The former was conducted on coronal slices in the brain, and the latter on sagittal slices. We will see why this is important later on. The URLs where you can download expression data is given in the final column. You can enter them in your internet browser and file should begin to download. If you don’t want to leave the wonderful world of R just to download (I don’t blame you), we can actually download and read the data within R itself.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genedata1 = read.raw.gene(as.character(fge$URLs[1]),url = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: bitops&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is generally better to download outside R and save the file, so you don’t have to keep downloading. Obtain the path to the file, and use it as as argument to read as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genedata1 = read.raw.gene(&amp;#39;/projects/egerek/matthijs/2015-07-Allen-Brain/Allen_Gene_Expression/raw_data/coronal/Bdnf_sid79587720/energy.raw&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-the-gene-expression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizing the gene expression&lt;/h2&gt;
&lt;p&gt;The gene expression data is a 1D vector. It can easily be converted to a 3D array using the mincArray function (which we will do later). But there is an important note to talk about before going further. The 1D vector lists values going from X=Anterior-to-Posterior, Y=Superior-to-Inferior, and Z=Left-to-Right (dimensions written from fastest changing index to slowest). This is the ABI orientation. The RMINC vectors typically are 1D vectors going from X=Left-to-Right, Y=Posterior-to-Anterior, Z=Inferior-to-Superior (dimensions written from fastest changing index to slowest). This is the MNI orientation. You can make a choice as to which orientiation you want to analyze in but I will be choosing MNI orientation in this tutorial. Just make sure you are consistent with your orientations and you won’t have problems. The function below converts ABI orientation to MNI orientation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genedata1 = allenVectorTOmincVector(genedata1)
# genedata1 = mincVectorTOallenVector(genedata1)  #This is the inverse function&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we can visualize the gene expression. Below is a sagittal slice&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;image(mincArray(genedata1)[30,,], ylab=&amp;#39;Superior-Inferior&amp;#39; ,xlab=&amp;#39;Anterior-Posterior&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-02-23_gene-expression_files/figure-html/6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-an-anatomical-underlay&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adding an anatomical underlay&lt;/h2&gt;
&lt;p&gt;I am not a good mouse brain anatomist, and so I find it pretty difficult to tell from this expression heatmap where exactly the expression is in the brain. We will now overlay a background MRI template to tell us where the gene expression is and use RMINC to create slice series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anatfile=&amp;#39;/projects/egerek/matthijs/2015-07-Allen-Brain/allenCCFV3_to_dorr_registration/allenCCFV3/atlas_in_200um/Dorr_resampled_200um.mnc&amp;#39;
mincPlotSliceSeries(
  anatomy=mincArray(mincGetVolume(anatfile)),
  statistics=mincArray(genedata1),
  symmetric=FALSE,
  col=colorRampPalette(c(&amp;quot;darkgreen&amp;quot;,&amp;quot;yellowgreen&amp;quot;))(255),
  legend=&amp;quot;Bdnf Expression&amp;quot;,low=2,high=6.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-02-23_gene-expression_files/figure-html/7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Much better. I can tell there is high expression in the cortex and hippocampus.&lt;/p&gt;
&lt;p&gt;Let us also look at the other Bdnf experiment (ID: 75695642).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genedata2 = read.raw.gene(as.character(fge$URLs[2]),url = TRUE)
genedata2 = allenVectorTOmincVector(genedata2)
mincPlotSliceSeries(
  anatomy=mincArray(mincGetVolume(anatfile)),
  statistics=mincArray(genedata2),
  symmetric=FALSE,
  col=colorRampPalette(c(&amp;quot;darkgreen&amp;quot;,&amp;quot;yellowgreen&amp;quot;))(255),
  legend=&amp;quot;Bdnf Expression&amp;quot;,low=2,high=6.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-02-23_gene-expression_files/figure-html/8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that the sagittal slices only span half the brain. This was a deliberate choice by the ABI and most of the gene experiments are like this. Furthermore, slices are sampled every 200 microns for the sagittal datasets and every 100 microns for the coronal datasets. That is why we prefer using the coronal slices any chance we get, but there are still tools that help us work with sagittal data. We can reflect data across the sagittal midplane to fill in the missing hemisphere as so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genedata2.reflected=midplane.reflect(genedata2,reflect.dim=3)
mincPlotSliceSeries(
  anatomy=mincArray(mincGetVolume(anatfile)),
  statistics=mincArray(genedata2.reflected),
  symmetric=FALSE,
  col=colorRampPalette(c(&amp;quot;darkgreen&amp;quot;,&amp;quot;yellowgreen&amp;quot;))(255),
  legend=&amp;quot;Bdnf Expression&amp;quot;,low=2,high=6.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-02-23_gene-expression_files/figure-html/99-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can do better. Because there sagittal sections were only sampled every 200um, there is a lot of missing data due to slice misalignment. We can fill them in using nearest neighbour marching averages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labelfile=system.file(&amp;#39;extdata/gridAnnotation.raw&amp;#39;,package=&amp;quot;ABIgeneRMINC&amp;quot;)
mask=allenVectorTOmincVector(read.raw.gene(labelfile,labels=TRUE)&amp;gt;0)
interp.gene=interpolate.gene(genedata2.reflected,mask)

mincPlotSliceSeries(
  anatomy=mincArray(mincGetVolume(anatfile)),
  statistics=mincArray(interp.gene),
  symmetric=FALSE,
  col=colorRampPalette(c(&amp;quot;darkgreen&amp;quot;,&amp;quot;yellowgreen&amp;quot;))(255),
  legend=&amp;quot;Bdnf Expression&amp;quot;,low=2,high=6.5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-02-23_gene-expression_files/figure-html/9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Even with interpolation, the expression map is not that good.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;expression-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Expression statistics&lt;/h2&gt;
&lt;p&gt;Moving back to the coronal maps, let’s generate summary statistics for each structure in the ABI atlas.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labels=read.raw.gene(labelfile,labels=TRUE)
labels.to.sum=sort(unique(labels))
labels.to.sum=labels.to.sum[labels.to.sum!=0]

udf=unionize(grid.data=genedata1,             #vector to unionize
           labels.to.sum=labels.to.sum,       #sum all labels
           labels.grid=labels                 #the vector of labels
           )
udf=udf[order(udf$mean,decreasing=TRUE),]
head(udf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     labels        sum     mean     stdev
## 167    287  19.641589 4.910397 12.018447
## 474    875  48.372759 4.397524  6.287998
## 261    483 123.968708 3.178685  3.827543
## 171    292  34.930620 3.175511  6.002760
## 162    279   6.248917 3.124459  3.977935
## 22      50  12.193722 3.048430  1.094984&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let us read a csv with structure names in it that correspond to the label number and add that as a column in our data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labeldefs=read.csv(&amp;quot;/projects/egerek/matthijs/2015-07-Allen-Brain/Allen_Gene_Expression/labels/allen_gridlabels_structures.csv&amp;quot;) #this can be downloaded from ABI
udf$structures=labeldefs[match(labeldefs$id,udf$labels),&amp;#39;name&amp;#39;]
udf = udf[,c(&amp;#39;structures&amp;#39;,&amp;#39;labels&amp;#39;,&amp;#39;mean&amp;#39;,&amp;#39;sum&amp;#39;,&amp;#39;stdev&amp;#39;)]
head(udf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                                                    structures
## 167                                Main olfactory bulb, outer plexiform layer
## 474 Bed nuclei of the stria terminalis, anterior division, anterolateral area
## 261                                                   Visceral area, layer 6b
## 171                                                Uvula (IX), granular layer
## 162                       Retrosplenial area, lateral agranular part, layer 1
## 22        Bed nuclei of the stria terminalis, anterior division, oval nucleus
##     labels     mean        sum     stdev
## 167    287 4.910397  19.641589 12.018447
## 474    875 4.397524  48.372759  6.287998
## 261    483 3.178685 123.968708  3.827543
## 171    292 3.175511  34.930620  6.002760
## 162    279 3.124459   6.248917  3.977935
## 22      50 3.048430  12.193722  1.094984&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Voila, we can now tell which structures have high gene expression for the gene we are interested in.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outro&lt;/h2&gt;
&lt;p&gt;I plan to do tutorials on more fancy gene expression analyses in the future, but this is the base from which future tutorials will be built. I hope this gets you started on using gene expression to explore neuroanatomical phenotypes and gives you an understanding of some of the caveats associated with spatial gene expression analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Co-Clinical Trials</title>
      <link>/blog/post/2018-02-15_coclinical-trials/</link>
      <pubDate>Thu, 15 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-02-15_coclinical-trials/</guid>
      <description>&lt;div id=&#34;why-co-clinical-trials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why co-clinical trials?&lt;/h2&gt;
&lt;p&gt;Here at the Mouse Imaging Centre, a large portion of our research is related to neurodevelopmental disorders. Ultimately the goal of such research is to improve health outcomes for individuals with these disorders. One clear way to impact health outcomes for individuals with neurodevelopmental disorders is to develop new medicines. Part of my research has been on the animal side of a project aimed at using matched human and animal trials to expedite drug development for autism spectrum disorders. This approach of simultaneously merging human and animal trials is termed a co-clinical trial. The purpose of today’s post is to introduce co-clinical trials and explain why they are useful for drug development.&lt;/p&gt;
&lt;p&gt;But first I’ll introduce the standard paradigm of drug developments.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;drug-development&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Drug development&lt;/h2&gt;
&lt;p&gt;The general path of drug development currently proceeds as follows.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pre-clinical phase: first new drugs are developed in labs where they are validated in animal models&lt;/li&gt;
&lt;li&gt;Phase 1 clinical trial: new drugs are then tested for safety in a small sample of humans.&lt;/li&gt;
&lt;li&gt;Phase 2 and 3: The drugs are then tested for their primary purpose in humans.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At each phase, the new drug must pass specific criteria to pass from one step to the next&lt;span class=&#34;math inline&#34;&gt;\(^1\)&lt;/span&gt;. The problem is, this path is &lt;strong&gt;slow&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The time it takes from the first moment the drug is discovered to helping real people is upwards of 15 years&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;! Just one study can cost up to $600 million to implement &lt;span class=&#34;math inline&#34;&gt;\(^3\)&lt;/span&gt;. This doesn’t even consider the cost of failed studies that frequently preceed successful ones&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;. In fact, most trials fail.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-trials-fail&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why trials fail&lt;/h2&gt;
&lt;p&gt;A recent study showed that only 10% of drugs in phase 1 studies make it to the market &lt;span class=&#34;math inline&#34;&gt;\(^4\)&lt;/span&gt;. The biggest challenge in clinical trials is the “high rate of failure to meet primary endpoints due to poor or complex design &lt;span class=&#34;math inline&#34;&gt;\(^5\)&lt;/span&gt;.” In these cases, the drug failed to meet the “efficacy” standards that were originally set.&lt;/p&gt;
&lt;p&gt;Before a clinical trial is started, the research team (usually made up of researchers, doctors, etc etc) has to determine which exact measures will be improved with treatment, and by how much. These measures and benchmarks are the trials efficacy targets. The trial must then meet those criteria to be considered successful.&lt;/p&gt;
&lt;p&gt;Let’s take an example from Zamzow (2017)&lt;span class=&#34;math inline&#34;&gt;\(^6\)&lt;/span&gt; to illustrate this point. There was great promise for a drug called mavoglurant, a potential treatment for Fragile X Syndrome (FXS). Fragile X Syndrome is the most common single-gene cause of autism. Mavoglurant, a metabotropic glutamate receptor 5 (mGluR5) antagonist, targets the pathway that is altered in FXS. The drug had successfully reversed many of the behavioural phenotypes in pre-clinical trials using mouse models of autism.&lt;/p&gt;
&lt;p&gt;Two large pharmaceutical companies invested in clinical trials of the drug, and the autism community held their breath in anticipation. Both studies had picked a questionnaire called the Aberrant Behaviour Checklist (ABC) as the primary measure and therefore, the trial would only be considered successful if a significant proportion of individuals improved on their ABCs, regardless of improvement in other measures. Within half a year of each other, both companies marked their studies as having failed to meet efficacy. Unfortunately, for some individuals, the drug did work really well.&lt;/p&gt;
&lt;p&gt;The problem is, despite failing to meet the efficacy targets decided before the trial, the drug improved measures other than on the ABCs. The Stevenson’s, one family involved in the trial, spoke of the obvious improvements they saw in their son Taylor, who has FXS. Mavoglurant had decreased his anxiety, his language skills flourished, and he made marked improvements in many other aspects of his behaviour. Because the trial failed, he was taken off the drug and has since regressed. Now, there is no way for him to access the drug or anything similar &lt;span class=&#34;math inline&#34;&gt;\(^6\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-co-clinical-trials-can-help&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How co-clinical trials can help&lt;/h2&gt;
&lt;p&gt;Mavoglurant is just one example of trials that failed to meet targets due to mispecified efficacy standards or unexpected disease heterogeneity. And this is exactly where co-clinical trials can step in.&lt;/p&gt;
&lt;p&gt;A pioneering example where the co-clinical trial design came to the rescue was a clinical trial of a drug called selumetinib for lung cancer. The drug, like many others, failed to meet its specified efficacy measures. However, concurrent research in mouse models of the disease suggested that the treatment effected subpopulations differently. Because the researchers had adopted the design of a co-clinical trial, they were able to use the data from the animal work to inform the analysis of the human data. Specifically, multiple genetic mouse models were treated with selumetinib and only one of the lines responded positively to treatment. When the researchers substratified the human patients by their cancer genotype, they also grouped into responders vs non-responders. The drug was then considered a successful treatment option for individuals with that cancer genotype.&lt;/p&gt;
&lt;p&gt;This case highlights that when there is heterogeneity in your study population, treatment response may be lost &lt;span class=&#34;math inline&#34;&gt;\(^7\)&lt;/span&gt;. Perhaps, there is a subset of individual’s like Taylor Stevenson &lt;span class=&#34;math inline&#34;&gt;\(^6\)&lt;/span&gt;, the boy with FXS, who do respond to treatment. A co-clinical trial, in which mouse models of autism are treated alongside individuals with autism, could potentially identify these subtypes, enhancing our understanding of autism and treatment options for individuals.&lt;/p&gt;
&lt;p&gt;The co-clinical methodology can be extended to numerous other fields, beyond autism and cancer. Any disease or disorder, in which the population presentation is heterogenous, and our understanding of the mechanisms are limited, could greatly benefit from the new perspective gained from co-clinical trials.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(^1\)&lt;/span&gt;: U.S. Food and Drug Administration. (Unknown year) Step 3: Clinical Research, The Drug Development Porcess &lt;a href=&#34;https://www.fda.gov/ForPatients/Approvals/Drugs/ucm405622.htm#Clinical_Research_Phase_Studies&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(^2\)&lt;/span&gt;: Ralf Huss. (2016) The High Price Of Failed Clinical Trials: Time To Rethink The Model, Clinical Leader &lt;a href=&#34;https://www.clinicalleader.com/doc/the-high-price-of-failed-clinical-trials-time-to-rethink-the-model-0001&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(^3\)&lt;/span&gt;: Institute of Medicine (US) Forum on Drug Discover…. (2010) Challenges and Opportunities: Workshop Summary, Transforming Clinical Research in the United States &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/books/NBK50888/&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(^4\)&lt;/span&gt;: Unknown author. (2016) Clinical Development Success Rates 2006-2015, Unknown source &lt;a href=&#34;https://www.bio.org/sites/default/files/Clinical%20Development%20Success%20Rates%202006-2015%20-%20BIO,%20Biomedtracker,%20Amplion%202016.pdf&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(^5\)&lt;/span&gt;: Andrew Burrows. (2016) Report: The 8 biggest challenges facing clinical trial professionals, Unknown source &lt;a href=&#34;https://knect365.com/clinical-trials-innovation/article/e414c9b9-8de9-4525-8fe6-fe6264d36df0/report-biggest-challenges-clinical-trials-pt-1&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(^6\)&lt;/span&gt;: Rachel Zamzow. (2017) The Flawed Designs of Drug Trials for Autism, Unknown source &lt;a href=&#34;https://www.theatlantic.com/health/archive/2017/02/autism-drugs/516855/&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(^7\)&lt;/span&gt;: Chen, Zhao and Cheng, Katherine and Walton, Zandra…. (2012) A murine lung cancer co-clinical trial identifies genetic modifiers of therapeutic response, Nature &lt;a href=&#34;https://www.nature.com/articles/nature10937&#34;&gt;link&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Thanks to Chris Hammill for helpful edits on the first version of this post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Finding and playing with peaks in RMINC</title>
      <link>/blog/post/2018-02-08_peaks-intro/</link>
      <pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-02-08_peaks-intro/</guid>
      <description>&lt;script src=&#34;/blog/blog/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/blog/blog/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/blog/blog/rmarkdown-libs/datatables-css/datatables-crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/blog/blog/rmarkdown-libs/datatables-binding/datatables.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/blog/blog/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;/blog/blog/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/blog/blog/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/blog/blog/rmarkdown-libs/crosstalk/css/crosstalk.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/blog/blog/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;So, peaks. When producing a statistical map, it’s good to get a report of the peaks (i.e. most significant findings). RMINC has had this support for a while now, though it has remained somewhat hidden. Here’s a bit of an intro, then.&lt;/p&gt;
&lt;p&gt;I will walk through the example we used from the Mouse Imaging Summer School in 2017, which is data from this paper:&lt;/p&gt;
&lt;p&gt;de Guzman AE, Gazdzinski LM, Alsop RJ, Stewart JM, Jaffray DA, Wong CS, Nieman BJ. Treatment age, dose and sex determine neuroanatomical outcome in irradiated juvenile mice. Radiat Res. 2015 May;183(5):541–9.&lt;/p&gt;
&lt;p&gt;To keep it simple, however, I’ll only look at sex differences in that dataset for now.&lt;/p&gt;
&lt;p&gt;Let’s start - load the libraries and read in the csv file that describes the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;suppressMessages(library(RMINC))
gf &amp;lt;- read.csv(&amp;quot;/hpf/largeprojects/MICe/jason/MISS2017/intro-stats/fixed_datatable_IRdose.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And run a linear model relating the Jacobian determinants to sex and radiation dose. I’ll use the segmentations file as a mask; it’ll be needed later on anyway.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;labelFile &amp;lt;- &amp;quot;/hpf/largeprojects/MICe/jason/MISS2017/intro-stats/atlas-registration/pipeline-18-08-2017-at-07-08-48_processed/nlin-3/voted.mnc&amp;quot;
vs &amp;lt;- mincLm(Jacobfile_scaled0.2 ~ Sex + Dose, gf, mask = labelFile)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Method: lm
## Number of volumes: 41
## Volume sizes: 152 320 225
## N: 41 P: 3
## In slice 
##  0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151 
## Done&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some more data preparation: read in the background anatomy file …&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;anatFile &amp;lt;- &amp;quot;/projects/moush/lbernas/Irradiation_behaviour_project/fixed_build_masked_23mar13_nlin/nlin-3.mnc&amp;quot;
anatVol &amp;lt;- mincArray(mincGetVolume(anatFile))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;… and show the results at a somewhat arbitrary threshold.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mincPlotSliceSeries(anatVol, mincArray(vs, &amp;quot;tvalue-SexM&amp;quot;), anatLow=10, anatHigh=15, low=2, high=6, symmetric = T,
                    begin=50, end=-50)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-02-08_peaks-intro_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;At this point we’ve run a linear model and visually assessed the results. Now we can locate the peak findings, using the &lt;code&gt;mincFindPeaks&lt;/code&gt; command.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;peaks &amp;lt;- mincFindPeaks(vs, &amp;quot;tvalue-SexM&amp;quot;, minDistance = 1, threshold = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Writing column tvalue-SexM to file /tmp/Rtmpm8tCfQ/file65323a5d369f.mnc 
## Range: 10.627142 -5.694668&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;mincFindPeaks&lt;/code&gt; uses the &lt;code&gt;find_peaks&lt;/code&gt; command from the MINC toolkit under the hood. You pass in the output of one of the RMINC modelling commands (mincLm in this case, but can be anything), along with the column from that model you want to get peaks from. You can then set the minimum distance between peaks (in mm) - i.e. how far apart do two statistical peaks have to be to be included? - as well as the threshold to be considered a peak. Optionally thresholds can be different for positive and negative peaks; as always, see &lt;code&gt;?mincFindPeaks&lt;/code&gt; for more detail.&lt;/p&gt;
&lt;p&gt;This is what we have at this point:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;peaks&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;,&#34;11&#34;,&#34;12&#34;,&#34;13&#34;,&#34;14&#34;,&#34;15&#34;,&#34;16&#34;,&#34;17&#34;,&#34;18&#34;,&#34;19&#34;,&#34;20&#34;,&#34;21&#34;,&#34;22&#34;,&#34;23&#34;,&#34;24&#34;,&#34;25&#34;,&#34;26&#34;,&#34;27&#34;,&#34;28&#34;,&#34;29&#34;,&#34;30&#34;,&#34;31&#34;,&#34;32&#34;,&#34;33&#34;,&#34;34&#34;,&#34;35&#34;,&#34;36&#34;,&#34;37&#34;],[75,154,104,124,94,176,158,102,74,131,122,67,78,105,107,51,110,60,138,85,148,38,56,104,120,143,29,107,30,168,159,32,122,70,89,175,107],[157,156,193,193,184,116,121,146,108,186,144,171,135,140,171,129,46,211,228,124,203,164,74,221,222,230,172,59,143,121,72,189,161,92,66,156,141],[50,52,56,52,75,83,83,105,96,75,108,41,52,67,107,62,45,114,136,63,132,58,60,80,125,105,96,116,65,47,91,76,131,61,34,127,46],[-2.126,2.298,-0.502,0.618,-1.062,3.53,2.522,-0.614,-2.182,1.01,0.506,-2.574,-1.958,-0.446,-0.334,-3.47,-0.165999999999999,-2.966,1.402,-1.566,1.962,-4.198,-3.19,-0.502,0.394,1.682,-4.702,-0.334,-4.646,3.082,2.578,-4.534,0.506,-2.406,-1.342,3.474,-0.334],[0.546000000000001,0.49,2.562,2.562,2.058,-1.75,-1.47,-0.0699999999999985,-2.198,2.17,-0.181999999999999,1.33,-0.685999999999999,-0.406,1.33,-1.022,-5.67,3.57,4.522,-1.302,3.122,0.938000000000001,-4.102,4.13,4.186,4.634,1.386,-4.942,-0.238,-1.47,-4.214,2.338,0.770000000000001,-3.094,-4.55,0.49,-0.35],[-1.456,-1.344,-1.12,-1.344,-0.056,0.392,0.392,1.624,1.12,-0.056,1.792,-1.96,-1.344,-0.504,1.736,-0.784,-1.736,2.128,3.36,-0.728,3.136,-1.008,-0.896,0.224,2.744,1.624,1.12,2.24,-0.616,-1.624,0.84,0,3.08,-0.84,-2.352,2.856,-1.68],[10.63,9.435,8.88,7.064,5.521,5.373,5.37,5.357,5.318,5.234,5.203,4.965,4.357,4.324,4.205,4.152,4.039,-5.695,-4.836,-4.832,-4.791,-4.711,-4.632,-4.612,-4.507,-4.506,-4.504,-4.439,-4.392,-4.345,-4.278,-4.209,-4.162,-4.142,-4.088,-4.054,-4.011]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;d1&lt;\/th&gt;\n      &lt;th&gt;d2&lt;\/th&gt;\n      &lt;th&gt;d3&lt;\/th&gt;\n      &lt;th&gt;x&lt;\/th&gt;\n      &lt;th&gt;y&lt;\/th&gt;\n      &lt;th&gt;z&lt;\/th&gt;\n      &lt;th&gt;value&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;pageLength&#34;:5,&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:[1,2,3,4,5,6,7]},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false,&#34;lengthMenu&#34;:[5,10,25,50,100]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;There are 7 columns; the first three give the coordinates in voxel space, the next three the coordinates in world space, and then the peak value itself. There’s a further helpful command that labels the peaks with the atlas location within which the peak is located:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;defsFile &amp;lt;- &amp;quot;/hpf/largeprojects/MICe/tools/atlases/Dorr_2008_Steadman_2013_Ullmann_2013_Richards_2011_Qiu_2016_Egan_2015_40micron/mappings/DSURQE_40micron_R_mapping.csv&amp;quot;
peaks &amp;lt;- mincLabelPeaks(peaks, labelFile, defsFile)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in peaks$label[i] &amp;lt;- mdefs$Structure[mdefs$value == peaks
## $label[i]]: number of items to replace is not a multiple of replacement
## length

## Warning in peaks$label[i] &amp;lt;- mdefs$Structure[mdefs$value == peaks
## $label[i]]: number of items to replace is not a multiple of replacement
## length

## Warning in peaks$label[i] &amp;lt;- mdefs$Structure[mdefs$value == peaks
## $label[i]]: number of items to replace is not a multiple of replacement
## length

## Warning in peaks$label[i] &amp;lt;- mdefs$Structure[mdefs$value == peaks
## $label[i]]: number of items to replace is not a multiple of replacement
## length

## Warning in peaks$label[i] &amp;lt;- mdefs$Structure[mdefs$value == peaks
## $label[i]]: number of items to replace is not a multiple of replacement
## length&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This adds an extra column containing the name of the label for each peak:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;peaks&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;,&#34;11&#34;,&#34;12&#34;,&#34;13&#34;,&#34;14&#34;,&#34;15&#34;,&#34;16&#34;,&#34;17&#34;,&#34;18&#34;,&#34;19&#34;,&#34;20&#34;,&#34;21&#34;,&#34;22&#34;,&#34;23&#34;,&#34;24&#34;,&#34;25&#34;,&#34;26&#34;,&#34;27&#34;,&#34;28&#34;,&#34;29&#34;,&#34;30&#34;,&#34;31&#34;,&#34;32&#34;,&#34;33&#34;,&#34;34&#34;,&#34;35&#34;,&#34;36&#34;,&#34;37&#34;],[75,154,104,124,94,176,158,102,74,131,122,67,78,105,107,51,110,60,138,85,148,38,56,104,120,143,29,107,30,168,159,32,122,70,89,175,107],[157,156,193,193,184,116,121,146,108,186,144,171,135,140,171,129,46,211,228,124,203,164,74,221,222,230,172,59,143,121,72,189,161,92,66,156,141],[50,52,56,52,75,83,83,105,96,75,108,41,52,67,107,62,45,114,136,63,132,58,60,80,125,105,96,116,65,47,91,76,131,61,34,127,46],[-2.126,2.298,-0.502,0.618,-1.062,3.53,2.522,-0.614,-2.182,1.01,0.506,-2.574,-1.958,-0.446,-0.334,-3.47,-0.165999999999999,-2.966,1.402,-1.566,1.962,-4.198,-3.19,-0.502,0.394,1.682,-4.702,-0.334,-4.646,3.082,2.578,-4.534,0.506,-2.406,-1.342,3.474,-0.334],[0.546000000000001,0.49,2.562,2.562,2.058,-1.75,-1.47,-0.0699999999999985,-2.198,2.17,-0.181999999999999,1.33,-0.685999999999999,-0.406,1.33,-1.022,-5.67,3.57,4.522,-1.302,3.122,0.938000000000001,-4.102,4.13,4.186,4.634,1.386,-4.942,-0.238,-1.47,-4.214,2.338,0.770000000000001,-3.094,-4.55,0.49,-0.35],[-1.456,-1.344,-1.12,-1.344,-0.056,0.392,0.392,1.624,1.12,-0.056,1.792,-1.96,-1.344,-0.504,1.736,-0.784,-1.736,2.128,3.36,-0.728,3.136,-1.008,-0.896,0.224,2.744,1.624,1.12,2.24,-0.616,-1.624,0.84,0,3.08,-0.84,-2.352,2.856,-1.68],[10.63,9.435,8.88,7.064,5.521,5.373,5.37,5.357,5.318,5.234,5.203,4.965,4.357,4.324,4.205,4.152,4.039,-5.695,-4.836,-4.832,-4.791,-4.711,-4.632,-4.612,-4.507,-4.506,-4.504,-4.439,-4.392,-4.345,-4.278,-4.209,-4.162,-4.142,-4.088,-4.054,-4.011],[&#34;left Medial amygdala&#34;,&#34;right Medial amygdala&#34;,&#34;left hypothalamus&#34;,&#34;right hypothalamus&#34;,&#34;left bed nucleus of stria terminalis&#34;,&#34;right subiculum&#34;,&#34;right MoDG&#34;,&#34;left LMol&#34;,&#34;left pre-para subiculum&#34;,&#34;right bed nucleus of stria terminalis&#34;,&#34;right CA1Rad&#34;,&#34;left amygdala&#34;,&#34;left CA3Py Inner&#34;,&#34;left fasciculus retroflexus&#34;,&#34;left CA30r&#34;,&#34;left CA1Py&#34;,&#34; medulla&#34;,&#34;left Primary somatosensory cortex&#34;,&#34;right Frontal association cortex&#34;,&#34; midbrain&#34;,&#34;right Primary motor cortex&#34;,&#34;left Piriform cortex&#34;,&#34;left paramedian lobule (lobule 7)&#34;,&#34;left Dorsal tenia tecta&#34;,&#34;right Cingulate cortex: area 32&#34;,&#34;right Lateral orbital cortex&#34;,&#34;left Secondary somatosensory cortex&#34;,&#34; lobule 6: declive&#34;,&#34;left Dorsolateral entorhinal cortex&#34;,&#34;right Caudomedial entorhinal cortex&#34;,&#34;right crus 1: ansiform lobule (lobule 6)&#34;,&#34;left Insular region: not subdivided&#34;,&#34;right Cingulate cortex: area 24b&#39;&#34;,&#34; medulla&#34;,&#34; medulla&#34;,&#34;right Primary somatosensory cortex: barrel field&#34;,&#34;left mammillary bodies&#34;]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;d1&lt;\/th&gt;\n      &lt;th&gt;d2&lt;\/th&gt;\n      &lt;th&gt;d3&lt;\/th&gt;\n      &lt;th&gt;x&lt;\/th&gt;\n      &lt;th&gt;y&lt;\/th&gt;\n      &lt;th&gt;z&lt;\/th&gt;\n      &lt;th&gt;value&lt;\/th&gt;\n      &lt;th&gt;label&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;pageLength&#34;:5,&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:[1,2,3,4,5,6,7]},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false,&#34;lengthMenu&#34;:[5,10,25,50,100]}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;At this point you have all the info about the most significant peaks in the dataset. There is one additional useful command, a shortcut to plot each peak. Plotting the two most positive and the two most negative peaks, for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;opar &amp;lt;- par(mfrow=c(2,2), mar=c(0,0,0,0))
nP &amp;lt;- nrow(peaks)
for (i in c(1:2, nP:(nP-1))) {
  mincPlotPeak(peaks[i,], anatVol, mincArray(vs, &amp;quot;tvalue-SexM&amp;quot;), anatLow=10, anatHigh=15, low=2, high=6, symmetric=T)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-02-08_peaks-intro_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;par(opar)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There is an additional argument to &lt;code&gt;mincPlotPeak&lt;/code&gt; - a function to create a plot of the peak. This is illustrated below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load ggplot2 for plotting
library(ggplot2)
# the plotting function; needs to take a single argument, the peak
p &amp;lt;- function(peak) {
  # read in the data for that particular peak
  gf$voxel &amp;lt;- mincGetWorldVoxel(gf$Jacobfile_scaled0.2, peak[&amp;quot;x&amp;quot;], peak[&amp;quot;y&amp;quot;], peak[&amp;quot;z&amp;quot;])
  # and create a box-plot; also read in info from the peak for a meaningful title
  ggplot(gf) + aes(x=Sex, y=exp(voxel)) + geom_boxplot() + 
    ggtitle(paste(&amp;quot;Peak:&amp;quot;, peak[&amp;quot;label&amp;quot;]), subtitle = paste(&amp;quot;T statistic:&amp;quot;, peak[&amp;quot;value&amp;quot;]))
}
# and plot the peak with its plot
mincPlotPeak(peaks[1,], anatVol, mincArray(vs, &amp;quot;tvalue-SexM&amp;quot;), anatLow=10, anatHigh=15, low=2, high=6, 
             symmetric=T, plotFunction = p)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Don&amp;#39;t know how to automatically pick scale for object of type mincVoxel/vector. Defaulting to continuous.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/2018-02-08_peaks-intro_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And that’s it. A quick survey for how to extract peaks, view them in a table, and create figures of the most significant findings.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Model Selection with PSIS-LOO</title>
      <link>/blog/post/2018-01-31_loo-intro/</link>
      <pubDate>Wed, 31 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-01-31_loo-intro/</guid>
      <description>&lt;div id=&#34;pitch&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pitch&lt;/h2&gt;
&lt;p&gt;In this post I’d like to provide an overview of Pareto-Smoothed Importance Sampling (PSIS-LOO) and how it can be used for bayesian model selection. Everything I discuss regarding this technique can be found in more detail in &lt;a href=&#34;https://arxiv.org/pdf/1507.04544.pdf&#34;&gt;Vehtari, Gelman, and Gabry (2016)&lt;/a&gt;. To lead up to PSIS-LOO I will introduce Akaike’s Information Criterion (AIC) to lay the foundation for model selection in general, then cover the expected log predictive density, the corner stone of bayesian model selection.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;intro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;Early in my masters I was introduced to the idea of model selection. The idea stuck, and has been formative in how I think about science. Running against the grain of hypothesis testing, model selection seemed a more natural way to think about what we do in science.&lt;/p&gt;
&lt;p&gt;Model selection stands apart from standard null hypothesis testing, where we have a single operating (null) model and seek data such that we can judge our model sufficiently unlikely.&lt;/p&gt;
&lt;p&gt;Model selection on the other hand assumes that we have many potential models that could be generating our data, and provides tools to help us choose which are more likely.&lt;/p&gt;
&lt;p&gt;Once we have decided to entertain the idea that there are many plausible models for our data, we have to decide how to compare our models.&lt;/p&gt;
&lt;p&gt;In most cases the first tool for comparison you encounter is Akaike’s An Information Criterion (AIC, also called, Akaike’s Information Criterion). AIC balances the likelihood of the data given the model and the complexity of the model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;aic&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;AIC&lt;/h2&gt;
&lt;p&gt;The normal formulation for Akaike’s Information Criterion is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ -2\ln[{p(y | \theta)}] + 2k \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;but we can pull out the distracting -2 out and get&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \ln[{p(y | \theta)}] - k \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the data we have observed, &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; are our estimated parameters, and k is the number of parameters.&lt;/p&gt;
&lt;p&gt;We can read the second version as the log likelihood minus the number of parameters. When doing AIC based model comparison you can choose the model that maximizes this quantity.&lt;/p&gt;
&lt;p&gt;AIC is the sum of two components&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The log-likelihood (goodness of fit)&lt;/li&gt;
&lt;li&gt;A penalty for model size.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The log-likelihood is a natural choice for goodness of fit. if the model fits the data well, the data will be considered likely, and the log-likelihood will be high relatively high.&lt;/p&gt;
&lt;p&gt;The penalty term k is equivalent to adding an independent observation that the model gives a probability of &lt;span class=&#34;math inline&#34;&gt;\(1/e\)&lt;/span&gt; (about 1/3), for each parameter you add. Alternatively you can imagine the penalty as dividing your likelihood by &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt; for every parameter you add.&lt;/p&gt;
&lt;p&gt;The whole reason we need to penalize is because the future is uncertain, and there is a risk of overfitting our data. Models with fewer parameters tend to generalize better, but more satisfying would be to estimate how well the model will perform in the future and use that directly. For this we need to consider how our score function (the likelihood) behaves under a potential model for the future. This leads to the specification of the expected log predictive density (ELPD).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;elpd&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ELPD&lt;/h2&gt;
&lt;p&gt;The expected log predictive density is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sum_i \int p_{t}(\tilde{y}_i) \ln{p(\tilde{y}_i | y)} d\tilde{y}_i \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(p_{t}\)&lt;/span&gt; is the true density of future observations, &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}_i\)&lt;/span&gt; is a future data point. Since &lt;span class=&#34;math inline&#34;&gt;\(p_{t}\)&lt;/span&gt; is unknown, we’re going to need to double dip in our data to get a guess as to what future data are going to look like. This strategy is called &lt;span class=&#34;math inline&#34;&gt;\(\mathit{M}_{closed}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Fortunately we have a strategy for producing fake new data and computing the likelihood at the same time. For this we’re going to reach for the standard machine learning approach of cross validation.&lt;/p&gt;
&lt;p&gt;We’ll treat some of our data as observed, and we’ll treat the rest like new data. Taking this to the extreme where we leave out one data point we get leave-one-out (loo) cross validation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;LOO&lt;/h2&gt;
&lt;p&gt;So now we have a strategy for imagining &lt;span class=&#34;math inline&#34;&gt;\(p_{t}\)&lt;/span&gt; which is to pick an observation at random from our data set. Then we need the likelihood our model would assign that datum if it hadn’t been observed. The naive approach would be to refit our model to the held out data, but this is way too expensive computationally. Ideally we wouldn’t need to refit the model at all - if only we knew how to reweight the likelihood as though the datum were unobserved. But such powerful magic surely can’t exist.&lt;/p&gt;
&lt;p&gt;But of course now I tell you that in fact it does!&lt;/p&gt;
&lt;p&gt;The trick has been known since the 1990’s and it is called importance sampling, and it is one the most striking results I know of.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;importance-sampling-loo&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importance Sampling LOO&lt;/h2&gt;
&lt;p&gt;Since we’re bayesian, we have samples from the posterior distribution of our model. Each of these samples implies a likelihood for each of our data points. Above I promised you a way to approximate the likelihood our model would given a datum if we hadn’t observed that datum. So let’s try to compute this for a single data point. Take point one for example.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\int p(y_1 | \theta) d\theta\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since we’re working with samples we’re going to move from an integral to an average over samples.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{S} \sum_s{ p(y_1 | \theta_s) }\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And now we want to reweight these posterior samples as though &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; hadn’t been observed.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{\sum_s{w_s}} \sum_s{ w_s p(y_1 | \theta_s) }\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we want to give weights to each posterior draw such that the weighting adjusts the posterior to what it would have been if &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; hadn’t been observed.&lt;/p&gt;
&lt;p&gt;So what should this weighting be? Take a moment and try to guess.&lt;/p&gt;
&lt;p&gt;Here’s a hint, if &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt; wasn’t observed do you think it would be assigned as high a probability?&lt;/p&gt;
&lt;p&gt;Well, obviously not you say. So what should the weighting be?&lt;/p&gt;
&lt;p&gt;It’s &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{p(y_1 | \theta_s)}\)&lt;/span&gt; !!!&lt;/p&gt;
&lt;p&gt;The sample weight is just the inverse of the probability that &lt;em&gt;that&lt;/em&gt; posterior draw gave to the held out point.&lt;/p&gt;
&lt;p&gt;When I first read this my brain made a little popping noise, probably audible to my coworkers, as it exploded.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/img/brain-exploding-psis.png&#34; width=&#34;500px&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-pareto-part&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Pareto Part&lt;/h2&gt;
&lt;p&gt;So we’re not quite done: there’s the pareto smoothing part of this. Importance sampling has a well know draw back, in that it is very noisy. The sampling weights we get are very heavy tailed, and it isn’t uncommon to get a single posterior sample where the held out datum was assigned very low probability dominating the IS adjusted posterior. So we need to smooth out the tails.&lt;/p&gt;
&lt;p&gt;It turns out that the upper tail of the the importance weights fit a generalized Pareto distribution nicely. This lends itself to smoothing.&lt;/p&gt;
&lt;p&gt;So to Pareto smooth our weights, we can fit a generalized pareto distribution to, say, the upper 20% of our importance weights. Then we can use the quantile of each weight to predict a smoothed approximation for that weight from the fitted distribution. We can then replace the upper tail weights with their smoothed weight and we’re done.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;all-together-now&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;All Together Now&lt;/h2&gt;
&lt;p&gt;Now we have the likelihood of each datum in the counterfactual world where it wasn’t observe. We can now average over all the smoother re-weighted posterior draws to get the loo ELPD&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \sum_i \ln \left( \frac{1}{\sum_s w_s^i} \sum_s w_s^i p(y_i | \theta_s) \right)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With the loo ELPD in hand, we can compute the difference between models. The model with the highest ELPD is the best.&lt;/p&gt;
&lt;p&gt;And there you have it, bayesian model selection using the leave-one-out expected log predictive density. But of course, the story doesn’t end there. With ELPDs computed we &lt;em&gt;could&lt;/em&gt; just pick the best model, but maybe we’d like to do inference over all the model weighted somehow by their score. But these are ideas for another post.&lt;/p&gt;
&lt;p&gt;Well I hoped you enjoyed learning about Pareto-Smoothed Importance Sampling. Code for doing this is all implemented in the wonderful &lt;a href=&#34;https://cran.r-project.org/web/packages/loo/index.html&#34;&gt;loo package&lt;/a&gt; for R. Happy model selecting!&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;I’d like to thank Dulcie Vousden and Ben Darwin for reading and commenting on an earlier version of this post.&lt;/p&gt;
&lt;p&gt;I’d like to thank Aki Vehtari for correctimg error in an earlier version of this post. I had mistakenly claimed the generalized pareto distribution was fit to the data &lt;em&gt;not&lt;/em&gt; in the upper tail of weights.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>/blog/post/linearmodels/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/linearmodels/</guid>
      <description>&lt;div id=&#34;S1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Preamble&lt;/h1&gt;
&lt;p&gt;The purpose of this post is to elucidate some of the concepts associated with statistical linear models.&lt;/p&gt;
&lt;p&gt;Let’s start by loading some libraries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(datasets)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;S2&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background Theory&lt;/h1&gt;
&lt;p&gt;The basic idea is as follows:&lt;/p&gt;
&lt;p&gt;Given two variables, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, for which we’ve measured a set of data points &lt;span class=&#34;math inline&#34;&gt;\(\{x_i, y_i\}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(i = 1, ..., n\)&lt;/span&gt;, we want to estimate a function, &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_i = f(x_i) + \epsilon_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for each data point &lt;span class=&#34;math inline&#34;&gt;\((x_i,y_i)\)&lt;/span&gt;. Here &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_i\)&lt;/span&gt; is the error in data point &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; compared to the predicted value &lt;span class=&#34;math inline&#34;&gt;\(f(x_i)\)&lt;/span&gt;. Specifically,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\epsilon_i = y_i - f(x_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We don’t know &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, but the simplest functional form that we can assume is that of a linear function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[f(x) = \beta_0 + \beta_1x \]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the intercept of the line and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the slope associated with the variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Thus for each data point &lt;span class=&#34;math inline&#34;&gt;\(x_i\)&lt;/span&gt;, we predict a value &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}_i = f(x_i)\)&lt;/span&gt; so that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y}_i = \beta_0 + \beta_1x_i\]&lt;/span&gt; This is known as &lt;strong&gt;simple linear regression&lt;/strong&gt;. Having specified the form of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;, the problem becomes one of optimizing the free parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; so that the predicted or trend line best describes the real data &lt;span class=&#34;math inline&#34;&gt;\(\{x_i, y_i\}\)&lt;/span&gt;. This is usually accomplished using the “ordinary least-squares” method, in which we minimize the sum of squared errors with respect to the free parameters. Explicitly, if we write the sum of squared errors as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\chi^2 = \sum_{i=1}^n{\epsilon_i^2} = \sum_{i=1}^n{(y_i - \hat{y}_i)^2} = \sum_{i=1}^n{(y_i - \beta_0 - \beta_1x_i)^2}\]&lt;/span&gt; we want to determine &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; such that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial\chi^2}{\partial\beta_0} = 0\]&lt;/span&gt; and &lt;span class=&#34;math display&#34;&gt;\[\frac{\partial\chi^2}{\partial\beta_1} = 0\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This can be solved analytically. In practice we let the computer do it for us.&lt;/p&gt;
&lt;p&gt;Moving on , there’s no reason we need to restrict ourselves to one predictor for the response variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, so we can include multiple variables &lt;span class=&#34;math inline&#34;&gt;\(\{x_1, x_2, ..., x_N\}\)&lt;/span&gt; in our model. Here &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the total number of regressors that are included. This is known as &lt;strong&gt;multiple linear regression&lt;/strong&gt;. The model then becomes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \sum_{a=1}^N{} \beta_ax_a\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; is an index summing over the predictors (rather than over the data points themselves as in the expression for &lt;span class=&#34;math inline&#34;&gt;\(\chi^2\)&lt;/span&gt; above). Here I’ve expressed the model in terms of the variables, rather than individual data points. For an individual data point &lt;span class=&#34;math inline&#34;&gt;\(\{x_i,y_i\}\)&lt;/span&gt;, we could write this as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y}_i = \beta_0 + \sum_{a=1}^N{} \beta_ax_{a,i}\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x_{a,i}\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th data point of the &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;th variable (e.g. the height, which is the variable, of a specific person). The two are identical representations. The optimization process for multiple linear regression is the same as that for simple linear regression, only involving more derivatives.&lt;/p&gt;
&lt;p&gt;Alright that should be enough background math. In the next Section, we’ll look at this in practice.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;S3&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simple Linear Regression in Practice&lt;/h1&gt;
&lt;p&gt;We’ll be working with the &lt;code&gt;mtcars&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The description of these variables can be found &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html&#34;&gt;here&lt;/a&gt;. We’re going to start by looking at the relationship between two continuous variables. Specifically, I’ve chosen to examine the relationship between car weight, &lt;code&gt;wt&lt;/code&gt;, and fuel efficiency, &lt;code&gt;mpg&lt;/code&gt;. Let’s start by creating a scatter plot to look at how the fuel efficiency varies with car weight.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p.mpg_vs_wt &amp;lt;- ggplot(mtcars, aes(x=wt,y=mpg)) + geom_point()
p.mpg_vs_wt + 
  labs(x = &amp;quot;Weight (1000 lbs)&amp;quot;,
       y = &amp;quot;Miles per Gallon&amp;quot;,
       title = &amp;quot;Scatter Plot of MPG vs. Weight&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/LinearModels_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is as we would expect, since less energy is needed to move lighter cars. Moreover we suspect that this data might be well suited to a simple linear model. As described above, the model we will be building is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_0 + \beta_1x + \epsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1x\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the &lt;code&gt;mpg&lt;/code&gt; variable in this case and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the &lt;code&gt;wt&lt;/code&gt; variable. In &lt;code&gt;R&lt;/code&gt; formula notation, we can express this as &lt;code&gt;mpg ~ wt&lt;/code&gt;, where &lt;code&gt;~&lt;/code&gt; means “is modelled by”. The way to build a linear model in &lt;code&gt;R&lt;/code&gt; is using the &lt;code&gt;lm()&lt;/code&gt; function, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_vs_wt &amp;lt;- summary(lm(mpg ~ wt,data=mtcars))
print(mpg_vs_wt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = mpg ~ wt, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5432 -2.3647 -0.1252  1.4096  6.8727 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  37.2851     1.8776  19.858  &amp;lt; 2e-16 ***
## wt           -5.3445     0.5591  -9.559 1.29e-10 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 3.046 on 30 degrees of freedom
## Multiple R-squared:  0.7528, Adjusted R-squared:  0.7446 
## F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The focus of this document will be on interpreting the &lt;code&gt;Estimate&lt;/code&gt; column of the &lt;code&gt;Coefficients&lt;/code&gt; table. Let’s pull this table from the output:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(mpg_vs_wt$coefficients)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Estimate Std. Error   t value     Pr(&amp;gt;|t|)
## (Intercept) 37.285126   1.877627 19.857575 8.241799e-19
## wt          -5.344472   0.559101 -9.559044 1.293959e-10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is fairly straightforward to interpret. Looking at the &lt;code&gt;Estimate&lt;/code&gt; column, the &lt;code&gt;(Intercept)&lt;/code&gt; value describes the value of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; in our model, while the &lt;code&gt;wt&lt;/code&gt; value describes the slope associated with the &lt;code&gt;wt&lt;/code&gt; variable, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;. Our fitted model looks like this:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = 37.3- 5.3x\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the &lt;code&gt;wt&lt;/code&gt; variable and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the &lt;code&gt;mpg&lt;/code&gt; variable, as mentioned above.&lt;/p&gt;
&lt;p&gt;Let’s see how this looks.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p.mpg_vs_wt + 
  labs(x = &amp;quot;Weight (1000 lbs)&amp;quot;,
       y = &amp;quot;Miles per Gallon&amp;quot;,
       title = &amp;quot;Simple Linear Model: MPG vs. Weight&amp;quot;) + 
  geom_abline(intercept = mpg_vs_wt$coefficients[&amp;quot;(Intercept)&amp;quot;,&amp;quot;Estimate&amp;quot;], 
              slope = mpg_vs_wt$coefficients[&amp;quot;wt&amp;quot;, &amp;quot;Estimate&amp;quot;]) + 
  coord_cartesian(xlim=c(0,6),ylim = c(5,40))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/LinearModels_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;That looks pretty good. A second-order polynomial might better capture the data at the lower and higher &lt;code&gt;wt&lt;/code&gt; values, but we won’t go into that here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;S4&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simple Linear Regression with Categorical Variables&lt;/h1&gt;
&lt;p&gt;In the previous Section, we looked at how simple linear regression works when the predictor is a continuous variable, like &lt;code&gt;wt&lt;/code&gt;. Here we will examine what happens when we model categorical variables. Recall that a categorical variable is a variable that takes on discrete, usually non-numerical, values. For example, sex is a categorical variable, with the values being male or female.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;mtcars&lt;/code&gt; dataset, we’ll look at the &lt;code&gt;am&lt;/code&gt; variable, which describes whether the car has manual or automatic transmission. Let’s explicitly express this as a factor and display the unique values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mtcars$am &amp;lt;- as.factor(mtcars$am)
unique(mtcars$am)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 0
## Levels: 0 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So &lt;code&gt;am&lt;/code&gt; has two possible values. Manual transmission is encoded as &lt;code&gt;am = 1&lt;/code&gt; while automatic transmission is encoded as &lt;code&gt;am = 0&lt;/code&gt; (refer to the &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html&#34;&gt;dataset description&lt;/a&gt;). As we did above, we can examine how &lt;code&gt;wt&lt;/code&gt; varies with the &lt;code&gt;am&lt;/code&gt; variable. Again we are fitting a model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1A\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the &lt;code&gt;wt&lt;/code&gt; variable and &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is the &lt;code&gt;am&lt;/code&gt; variable. Keep in mind that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is binary. In &lt;code&gt;R&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_vs_am &amp;lt;- summary(lm(mpg ~ am, data=mtcars ))
print(mpg_vs_am$coefficients)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Estimate Std. Error   t value     Pr(&amp;gt;|t|)
## (Intercept) 17.147368   1.124603 15.247492 1.133983e-15
## am1          7.244939   1.764422  4.106127 2.850207e-04&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus we see that &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; = 17.1 and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; = 7.2. Note here that in the &lt;code&gt;Coefficients&lt;/code&gt; table, the slope estimate is associated with the label &lt;code&gt;am1&lt;/code&gt;. This means that it displays the slope going from &lt;code&gt;am = 0&lt;/code&gt; to &lt;code&gt;am = 1&lt;/code&gt;. This is perhaps expected in this case since 1 is greater than 0, but in general categorical variables will not have numerical values. Consider again a variable describing sex. The two instances are “Male” and “Female”. There is no specific order in which to compute the slope. When building a model with categorical variables such as these, &lt;code&gt;R&lt;/code&gt; will implicitly assign values of 0 and 1 to the levels of the variable. By default, &lt;code&gt;R&lt;/code&gt;, assigns the &lt;strong&gt;reference level&lt;/strong&gt;, i.e. a value of 0, to the value that is &lt;strong&gt;lowest in alphabetical order&lt;/strong&gt;. For the sex variable, “Female” would be associated with 0, and “Male” with 1. So when running &lt;code&gt;lm()&lt;/code&gt; on such a model and printing the output, the &lt;code&gt;Coefficients&lt;/code&gt; table will have a row with a name like &lt;code&gt;sexMale&lt;/code&gt;. The &lt;code&gt;Estimate&lt;/code&gt; value associated with this describes the slope of the model going from &lt;code&gt;sex=Female&lt;/code&gt; (which is implicitly defined as 0) to &lt;code&gt;sex=Male&lt;/code&gt; (which is implicitly defined as 1). This might seem unnecessary right now, but it becomes important when trying to interpret the output from more complex models, as we’ll see below.&lt;/p&gt;
&lt;p&gt;As above, we can plot this data and model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mtcars, aes(x=am,y=mpg)) + 
  geom_jitter(width=0.1) + 
  geom_smooth(method=&amp;quot;lm&amp;quot;, formula = y~x, aes(group=1), se=F) + 
  labs(x = &amp;quot;Transmission (0 = automatic)&amp;quot;,
       y = &amp;quot;Miles per Gallon&amp;quot;,
       title = &amp;quot;Simple Linear Model: MPG vs. Transmission&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/LinearModels_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;S5&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multiple Linear Regression in Practice&lt;/h1&gt;
&lt;p&gt;In this Section we will combine the models built in the two previous Sections using multiple linear regression. Specifically, we will model &lt;code&gt;mpg&lt;/code&gt; by &lt;code&gt;wt&lt;/code&gt; and &lt;code&gt;am&lt;/code&gt; together.&lt;/p&gt;
&lt;p&gt;Let’s start by taking a look at the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mtcars, aes(x=wt, y=mpg, col=am)) + 
  geom_point() +  
   labs(x = &amp;quot;Weight (1000 lbs)&amp;quot;,
       y = &amp;quot;Miles per Gallon&amp;quot;,
       title = &amp;quot;Fuel Efficiency, Weight, and Transmission&amp;quot;,
       colour=&amp;quot;Transmission&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/LinearModels_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, I’ve plotted &lt;code&gt;mpg&lt;/code&gt; against &lt;code&gt;wt&lt;/code&gt; using a scatter plot, but I’ve mapped the colour aesthetic to the &lt;code&gt;am&lt;/code&gt; variable to see the group variation.&lt;/p&gt;
&lt;p&gt;Mathematically, the model we will use looks like&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1x + \beta_2A\]&lt;/span&gt; where, as above, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the &lt;code&gt;mpg&lt;/code&gt; variable, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the &lt;code&gt;wt&lt;/code&gt; continuous variable, and &lt;code&gt;A&lt;/code&gt; is the categorical &lt;code&gt;am&lt;/code&gt; variable. &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the slope associated with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; is the slope associated with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. Recall that &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is only 0 or 1.&lt;/p&gt;
&lt;p&gt;Let’s go ahead and build the model in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_MLR &amp;lt;- summary(lm(mpg ~ wt + am, data=mtcars))
print(mpg_MLR$coefficients)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Estimate Std. Error     t value     Pr(&amp;gt;|t|)
## (Intercept) 37.32155131  3.0546385 12.21799285 5.843477e-13
## wt          -5.35281145  0.7882438 -6.79080719 1.867415e-07
## am1         -0.02361522  1.5456453 -0.01527855 9.879146e-01&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, keeping in mind the mathematical formula, we see that &lt;code&gt;(Intercept)&lt;/code&gt; corresponds to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; = 37.32, &lt;code&gt;wt&lt;/code&gt; corresponds to &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; = -5.35 and &lt;code&gt;am1&lt;/code&gt; corresponds to &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; = -0.02. So how do we interpret this? How could we sketch this model on a scatter plot of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; vs &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; like the one above? The way to go about it is to examine the two cases for our categorical variables &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; = &lt;code&gt;am&lt;/code&gt;. Recall that, regardless of what our factor levels are (0/1, Female/Male), &lt;code&gt;R&lt;/code&gt; always encodes categorical variables as 0 and 1. Consequently we can always examine our mathematical model by setting the corresponding variable to 0 or 1. For &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; = &lt;code&gt;am&lt;/code&gt; = 0, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1x\]&lt;/span&gt; In this case, when &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; = 0, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the intercept of the line relating &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the slope associated with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. On the scatter plot, we would draw a line with this slope and intercept. What about when &lt;span class=&#34;math inline&#34;&gt;\(A = 1\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1x + \beta_2\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[= (\beta_0 + \beta_2) + \beta_1x\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[= \beta_0&amp;#39; + \beta_1x\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We find that we actually have a new intercept value &lt;span class=&#34;math inline&#34;&gt;\(\beta_0&amp;#39; = \beta_0 + \beta_2\)&lt;/span&gt;, but the same slope &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;. Thus the trend line associated with &lt;span class=&#34;math inline&#34;&gt;\(A = 1\)&lt;/span&gt; has a different intercept than that associated with &lt;span class=&#34;math inline&#34;&gt;\(A = 0\)&lt;/span&gt;. What we’ve discovered is that the &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; parameter tells us the &lt;strong&gt;difference in the intercept values&lt;/strong&gt; between the &lt;code&gt;am = 0&lt;/code&gt; and &lt;code&gt;am = 1&lt;/code&gt; groups, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\beta_2 = \beta_0&amp;#39; - \beta_0\)&lt;/span&gt;. The &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; parameter tells us the slope of the two lines.&lt;/p&gt;
&lt;p&gt;Let’s see what this looks like.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta0 &amp;lt;- mpg_MLR$coefficients[&amp;quot;(Intercept)&amp;quot;,&amp;quot;Estimate&amp;quot;]
beta1 &amp;lt;- mpg_MLR$coefficients[&amp;quot;wt&amp;quot;,&amp;quot;Estimate&amp;quot;]
beta0_prime &amp;lt;- mpg_MLR$coefficients[&amp;quot;(Intercept)&amp;quot;,&amp;quot;Estimate&amp;quot;] + 
                mpg_MLR$coefficients[&amp;quot;am1&amp;quot;,&amp;quot;Estimate&amp;quot;]

ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&amp;quot;red&amp;quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1, 
              col=&amp;quot;blue&amp;quot;,
              alpha=0.5) + 
  labs(x = &amp;quot;Weight (1000 lbs)&amp;quot;,
       y = &amp;quot;Miles per Gallon&amp;quot;,
       title = &amp;quot;Multiple Linear Model: MPG ~ Weight + Transmission&amp;quot;,
       colour=&amp;quot;Transmission&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/LinearModels_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are actually two lines in this plot, a blue one and a red one. Given how small &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; is we can’t see much of a difference, but the blue trend line should be slightly lower than the red line. Their slopes are the same. Let’s zoom in to be sure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&amp;quot;red&amp;quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1, 
              col=&amp;quot;blue&amp;quot;,
              alpha=0.5) + 
  coord_cartesian(xlim = c(3.4,3.5),ylim = c(17.5,19.5)) + 
    labs(x = &amp;quot;Weight (1000 lbs)&amp;quot;,
       y = &amp;quot;Miles per Gallon&amp;quot;,
       title = &amp;quot;Multiple Linear Model: MPG ~ Weight + Transmission (Zoomed In)&amp;quot;,
       colour=&amp;quot;Transmission&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/LinearModels_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There we go. This is the effect of a multiple linear model with no interactions. In general the trend lines for the two groups don’t have to be so close together. It all depends on the data. In this case we see that, when we impose a fixed slope (using &lt;code&gt;mpg ~ wt + am&lt;/code&gt;), the trend lines describing the two groups are basically the same. Next, we’ll look at how adding interactions to our model results in slope variation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;S6&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multiple Linear Regression with Interactions&lt;/h1&gt;
&lt;p&gt;In the previous Section we examined the use of multiple linear regression to model a response variable in terms of continuous and categorical predictors. We can take this a step further by including an &lt;strong&gt;interaction&lt;/strong&gt; in our model. What does this mean? An interaction describes how a change in one predictor influences change in another predictor. In mathematics this is typically expressed by including a product or more complex term in an equation. A simple product of two variables is the simplest interaction term that we can write down. Consider once again our model of &lt;code&gt;mpg&lt;/code&gt; vs. &lt;code&gt;wt&lt;/code&gt; and &lt;code&gt;am&lt;/code&gt;. This time we will add an interaction:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1x + \beta_2A + \beta_3xA\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The model is as above, with the exception of the new interaction term &lt;span class=&#34;math inline&#34;&gt;\(\beta_3xA\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(xA\)&lt;/span&gt; is the product interaction while &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; is the model parameter associated with this interaction.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;An important note&lt;/strong&gt;: In &lt;code&gt;R&lt;/code&gt; formula notation, interactions are expressed as &lt;code&gt;wt*am&lt;/code&gt;. This includes all terms in the model, i.e. &lt;code&gt;wt*am&lt;/code&gt; = &lt;code&gt;1 + wt + am + wt:am&lt;/code&gt; where &lt;code&gt;1&lt;/code&gt; stands in for the “variable” associated with the &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; parameter, and the &lt;strong&gt;colon denotes a product&lt;/strong&gt;. We can thus write our full interactive model as &lt;code&gt;mpg ~ wt*am&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s run this model through &lt;code&gt;lm()&lt;/code&gt; and see what happens.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_int &amp;lt;- summary(lm(mpg ~ wt*am,data=mtcars))
print(mpg_int$coefficients)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Estimate Std. Error   t value     Pr(&amp;gt;|t|)
## (Intercept) 31.416055  3.0201093 10.402291 4.001043e-11
## wt          -3.785908  0.7856478 -4.818836 4.551182e-05
## am1         14.878423  4.2640422  3.489277 1.621034e-03
## wt:am1      -5.298360  1.4446993 -3.667449 1.017148e-03&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have four rows for our four parameters. Again each row corresponds to one &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameter: &lt;code&gt;(Intercept)&lt;/code&gt; corresponds to &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; = 31.42, &lt;code&gt;wt&lt;/code&gt; corresponds to &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; = -3.79, &lt;code&gt;am1&lt;/code&gt; corresponds to &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; = 14.88 and &lt;code&gt;wt:am1&lt;/code&gt; corresponds to &lt;span class=&#34;math inline&#34;&gt;\(\beta_3\)&lt;/span&gt; = -5.3. Let’s interpret this, as we did in the previous Section, by examining the different values of the categorical variable &lt;code&gt;am&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; = &lt;code&gt;am&lt;/code&gt; = 0, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1x\]&lt;/span&gt; which is just our simple model. So &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; describes the intercept when &lt;span class=&#34;math inline&#34;&gt;\(A = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; describes the slope associated with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(A = 0\)&lt;/span&gt;. What happens when &lt;span class=&#34;math inline&#34;&gt;\(A = 1\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1x + \beta_2 + \beta_3x\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[= (\beta_0 + \beta_2) + (\beta_1 + \beta_3)x\]&lt;/span&gt; &lt;span class=&#34;math display&#34;&gt;\[= \beta_0&amp;#39; + \beta_1&amp;#39;x\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So we actually have a new intercept and a new slope! The new intercept is the same as in the previous Section: &lt;span class=&#34;math inline&#34;&gt;\(\beta_0&amp;#39; = \beta_0 + \beta_2\)&lt;/span&gt;. The new slope is &lt;span class=&#34;math inline&#34;&gt;\(\beta_1&amp;#39; = \beta_1 + \beta_3\)&lt;/span&gt;. Therefore, a simple product interaction like this one causes the slope of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to change as we move from &lt;span class=&#34;math inline&#34;&gt;\(A = 0\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(A = 1\)&lt;/span&gt;. Displaying this on a scatter plot of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we would have two lines with different intercepts and different slopes.&lt;/p&gt;
&lt;p&gt;Let’s take a look.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;beta0 = mpg_int$coefficients[&amp;quot;(Intercept)&amp;quot;,&amp;quot;Estimate&amp;quot;]
beta1 = mpg_int$coefficients[&amp;quot;wt&amp;quot;,&amp;quot;Estimate&amp;quot;]
beta0_prime = mpg_int$coefficients[&amp;quot;(Intercept)&amp;quot;,&amp;quot;Estimate&amp;quot;] + 
                mpg_int$coefficients[&amp;quot;am1&amp;quot;,&amp;quot;Estimate&amp;quot;]
beta1_prime = mpg_int$coefficients[&amp;quot;wt&amp;quot;,&amp;quot;Estimate&amp;quot;] + 
                mpg_int$coefficients[&amp;quot;wt:am1&amp;quot;,&amp;quot;Estimate&amp;quot;]

ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&amp;quot;red&amp;quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1_prime, 
              col=&amp;quot;blue&amp;quot;,
              alpha=0.5) + 
  coord_cartesian(xlim=c(0,6),
                  ylim = c(0,50)) +
  labs(x = &amp;quot;Weight (1000 lbs)&amp;quot;,
       y = &amp;quot;Miles per Gallon&amp;quot;,
       title = &amp;quot;Weight-Transmission Interaction&amp;quot;,
       colour=&amp;quot;Transmission&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/LinearModels_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Perfect. Compared to the non-interactive model in the previous Section, we see that adding an interaction (and thus allowing the slope of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to vary between groups) better characterizes the data. If the data was truly not well characterized by an interaction, the equal-slope model of the previous Section would perform just as well as an interactive model of this kind. Of course this would have to be determined by examining the statistics associated with the models.&lt;/p&gt;
&lt;p&gt;This concludes the majority of what I wanted to cover. In the next Section I’ll go into some of the heavier mathematical details regarding linear models. Read ahead at your own peril.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;S7&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mathematical Embellishments&lt;/h1&gt;
&lt;p&gt;The previous analysis was focussed on examining linear models built with a continuous and categorical predictor. We’re by no means restricted to this however. We can build a linear model with as many variables as we’d like. As we saw above, the case of one continuous predictor and one categorical predictor can be visualized fairly easily using a scatter plot, where the continous data is mapped to the &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axes and the categorical data is mapped to a colour/shape/style aesthetic. This becomes harder to do when we’re examining models that use multiple continuous predictors, e.g. something of the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we have an interactive multiple linear regression with two continuous regressors &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. A practical example of this would be modelling the &lt;code&gt;mpg&lt;/code&gt; variable in terms of the &lt;code&gt;wt&lt;/code&gt; and horsepower, &lt;code&gt;hp&lt;/code&gt;. Let’s plot this in the same way we did above, where the &lt;code&gt;hp&lt;/code&gt; variable is mapped to the colour aesthetic.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(mtcars, aes(x=wt, y=mpg, col=hp)) + 
  geom_point() +  
   labs(x = &amp;quot;Weight (1000 lbs)&amp;quot;,
       y = &amp;quot;Miles per Gallon&amp;quot;,
       title = &amp;quot;Fuel Efficiency, Weight, and Horsepower&amp;quot;,
       colour=&amp;quot;Transmission&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/LinearModels_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given the continuous nature of the &lt;code&gt;hp&lt;/code&gt; variable, this is difficult to interpret by eye. That doesn’t invalidate the model however, and we can still estimate the optimal &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mpg_hp &amp;lt;- summary(lm(mpg ~ wt*hp, data=mtcars))
print(mpg_hp$coefficients)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                Estimate Std. Error   t value     Pr(&amp;gt;|t|)
## (Intercept) 49.80842343 3.60515580 13.815887 5.005761e-14
## wt          -8.21662430 1.26970814 -6.471270 5.199287e-07
## hp          -0.12010209 0.02469835 -4.862758 4.036243e-05
## wt:hp        0.02784815 0.00741958  3.753332 8.108307e-04&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The rows in this table still represent the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; parameters in the model, as before, but it is much harder to interpret this result in the context of a scatter plot of &lt;code&gt;mpg&lt;/code&gt; vs. &lt;code&gt;wt&lt;/code&gt;. We can’t just set &lt;code&gt;hp = 0&lt;/code&gt; and &lt;code&gt;hp = 1&lt;/code&gt; as we did previously. The equivalent here would be setting &lt;code&gt;hp&lt;/code&gt; to an infinite number of incremental values. This isn’t the right way to think about this. This sort of brings us face to face with what the multiple linear model is actually saying. Consider again simple linear regression:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1x\]&lt;/span&gt; This is clearly the expression for a line. The variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is modelled as a linear function of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. As we know, we can plot this easily on a two-dimensional space, where &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; form the axes. Moving to multiple regression, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In Sections &lt;a href=&#34;#S5&#34;&gt;5&lt;/a&gt; and &lt;a href=&#34;#S6&#34;&gt;6&lt;/a&gt;, we interpreted this model &lt;strong&gt;within the context of the scatter plot of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/strong&gt;. With &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; as a categorical variable, this allowed us to interpret &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; as the difference in intercept values on this scatter plot. Mathematically, however, this expression describes a &lt;strong&gt;two-dimensional plane&lt;/strong&gt; characterised by the variables &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. We see that the intercept of the plane is &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, i.e. the value of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; at which both the parametric variables are 0. Moreover, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is the slope of the plane with respect to &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt; is the slope of the plane with respect to &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. Specifically,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_1 = \frac{\partial\hat{y}}{\partial x_1}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_2 = \frac{\partial\hat{y}}{\partial x_2}\]&lt;/span&gt; We can visualize this two-dimensional plane in a three-dimensional space, where the third axis is represented by the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; variable. You can do this easily by grabbing a piece of paper, or preferably a rigid flat object, and orienting it in front of you in real space. You can imagine the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; axis as the vertical axis, and the &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; axes as the horizontal axes. The idea of a flat plane in a multi-dimensional space extends to any number of predictors in our model, provided that the model is non-interactive. Given &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; predictors &lt;span class=&#34;math inline&#34;&gt;\(\{x_1, x_2, ..., x_N\}\)&lt;/span&gt;, a model of the form&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \sum_{a=1}^N{\beta_ax_a}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;describes a N-dimensional hyperplane embedded in a (N+1)-dimensional space. Crazy. This is undoubtedly true of continuous variables, but is a little bit more nuanced for a categorical variable. Going back to our model of &lt;code&gt;mpg&lt;/code&gt; vs. &lt;code&gt;wt&lt;/code&gt; and &lt;code&gt;am&lt;/code&gt;, we can still imagine this in a three-dimensional space. The axes of the space are &lt;code&gt;mpg&lt;/code&gt;,&lt;code&gt;wt&lt;/code&gt; and &lt;code&gt;am&lt;/code&gt;, but notice that we aren’t actually dealing with a plane in this case, since &lt;code&gt;am&lt;/code&gt; only takes on binary values. Rather we are dealing with two different lines embedded in this three-dimensional space. One line will occur at &lt;code&gt;am = 0&lt;/code&gt;, while the other occurs at &lt;code&gt;am = 1&lt;/code&gt;. Given this context, we can re-interpret the scatter plots from Section &lt;a href=&#34;#S5&#34;&gt;5&lt;/a&gt; and &lt;a href=&#34;#S6&#34;&gt;6&lt;/a&gt;. Here is the scatter plot and model from Section &lt;a href=&#34;$S6&#34;&gt;6&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/blog/blog/post/LinearModels_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Within the proper context of a three-dimensional space, this two-dimensional plot is actually the &lt;strong&gt;projection of the binary &lt;code&gt;am&lt;/code&gt; axis onto &lt;code&gt;am = 0&lt;/code&gt;&lt;/strong&gt;. Imagine the red line existing in your computer screen and the blue line actually existing outside of your computer screen, closer to you. I’ve used the interactive model here since the plot is nicer than that for the non-interactive model, but this leads us into a discussion of interactions with continuous variables.&lt;/p&gt;
&lt;p&gt;Examining the plot above, you might already guess where this is going. Let’s consider a model with a simple product interaction of two continuous variables, like the one at the beginning of this Section:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We saw that when &lt;span class=&#34;math inline&#34;&gt;\(\beta_3 = 0\)&lt;/span&gt;, the model describes a two-dimensional plane in three-dimensions. What does the interaction do to this plane? Notice that the product interaction is actually a second-order term in the expression for &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}\)&lt;/span&gt;. From univariate calculus, we know that second order terms are responsible for the curvature of a function. The same is true in multivariate calculus. The result is that the plane is no longer a plane, but rather a &lt;strong&gt;curved&lt;/strong&gt; two-dimensional surface (or manifold, if you want to be fancy), embedded in a three-dimensional space. The nature of this curvature is unique as well, since it involves coupling between the two regressors, i.e. the surface changes in the &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; direction as we move along the &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; direction, and vice versa. This is apparent when looking at the partial derivatives:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial \hat{y}}{\partial x_1} = \beta_1 + \beta_3x_2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{\partial \hat{y}}{\partial x_2} = \beta_2 + \beta_3x_1 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can further characterize the modelled surface if we’d like. For instance, since there aren’t any single-variable higher order terms, e.g. &lt;span class=&#34;math inline&#34;&gt;\(x_1^2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(x_1^3\)&lt;/span&gt;, etc., we know that, for a given value of one of the predictors, the response variable varies linearly with the other predictor. You can verify this by setting one of the predictors to 0. Moreover since there are no terms of order higher than 2, we know that this surface has a constant curvature. This can be verified by computing the 2nd order partial derivatives. These steps maybe aren’t necessary, but they give us an idea as to how to interpret the effect of a simple product interaction.&lt;/p&gt;
&lt;p&gt;In general, a multiple linear regression with &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; predictors and various interactions between those predictors will describe a curved surface or manifold in a (N+1)-dimensional space. The more complex the interactions between the predictors, the more elaborate the surface will be. In analogy to Section &lt;a href=&#34;#S2&#34;&gt;2&lt;/a&gt; such a surface will be characterized by a function, &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, so that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = f[\{x_a\}_{a=1}^N] + \epsilon\]&lt;/span&gt; for a response variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and predictors &lt;span class=&#34;math inline&#34;&gt;\(x_a\)&lt;/span&gt;. Estimating complex surfaces such as these is the purpose of most statistical and machine learning techniques.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>StanCon Highlights</title>
      <link>/blog/post/2018-01-13_stancon-highlights/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-01-13_stancon-highlights/</guid>
      <description>&lt;p&gt;Hi readers,&lt;/p&gt;
&lt;p&gt;Recently I got back from StanCon 2018 Ansilomar. I had a little time waiting for one of my flights and I thought I’d reflect on the conference. Last year I was lucky enough to go to the first StanCon and it was nice to be able to see how the conference has grown. This year it was three days of tutorials, talks, and networking. I had a blast.&lt;/p&gt;
&lt;p&gt;Here are some of my personal highlights, I’ll update with links to the slides/videos as they become available.&lt;/p&gt;
&lt;div id=&#34;tutorials&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tutorials:&lt;/h2&gt;
&lt;div id=&#34;learning-about-stan-under-the-hood&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Learning about stan under the hood&lt;/h3&gt;
&lt;p&gt;Dan Lee and Charles Margossian taught back-to-back sessions about the c++ internals of Stan. Charles taught how to add a new function to the stan math library. I enjoyed this because late last year I was trying to hack some new functionality into the stan math library, and had to learn this the hard way. Charles made it look so simple. Dan gave a tour of some of the c++ code that runs the sampler builds the computational contexts for the model. This has inspired me and I hope to try writing some simple autodiff programs using the underlying autodiff library. I had been toying with writing these kinds of program using Haskell’s AD library, but the linear algebra story there isn’t as obvious (or at least I struggled with it).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;advanced-hierarchical-models&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Advanced Hierarchical Models&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://mc-stan.org/events/stancon2018/AHM/AHM3.pdf&#34;&gt;Slides&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Ben Goodrich taught a triplet of advanced hierarchical modelling sessions. The most advanced the sessions got was introducing the multivariate normal non-centered parameterization (which I have already been using in my models), but it was very useful to see into the thought processes of an expert. Some tips I caught through the sessions were to use Paul Burkner’s &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/index.html&#34;&gt;brms package&lt;/a&gt; to auto-generate hierarchical model code. Ben recommended starting with auto-generated code when writing models, and then riffing on top, which seems like a great idea to me. Another point he emphasized was to use prior prediction as a sanity check, which I know has been recommended elsewhere, but seems like good advice.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-selection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model Selection&lt;/h3&gt;
&lt;p&gt;Probably the most attended session of the conference was Aki Vehtari’s model selection session. This was a perfect follow-up to his last-minute talk on regularized horseshoe priors from the day before. Aki introduced us to the expected log predictive density (ELPD), the bayesian answer to information criteria. He introduced us to the three approaches to dealing with the fact that the true future distribution is unknown:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathit{M}_{open}\)&lt;/span&gt; - The normal approach of using the leave-one-out distribution as an approximation for the future&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathit{M}_{closed}\)&lt;/span&gt; - This one wasn’t covered in detail.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathit{M}_{completed}\)&lt;/span&gt; - Using a trusted distribution to approximate the future.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The applicability of &lt;span class=&#34;math inline&#34;&gt;\(\mathit{M}_{completed}\)&lt;/span&gt; isn’t immediately obvious, where would you get a distribution you trust more than your posterior. The answer is you that essentially do in the model selection case! If your full model is suitably regularized, the full model is more trustworthy than the sub-models. This can be used to get more precise cross-validation estimates for the sub-model allowing you to identify irrelevant predictors.&lt;/p&gt;
&lt;p&gt;To make this computationally feasibly Aki presented the projection predictive method. Fit your full model, probably regularized with horseshoe priors, then fit sub-models such that they minimize the KL divergence between them and the full model (projection). Then pick the sub-model with the lowest KL divergence! He has a package on github called &lt;a href=&#34;https://github.com/stan-dev/projpred&#34;&gt;projpred&lt;/a&gt; for doing this.&lt;/p&gt;
&lt;p&gt;I was already planning on writing a post about PSIS-LOO the importance sampling based approximate cross validation done in the &lt;a href=&#34;https://cran.r-project.org/web/packages/loo/index.html&#34;&gt;loo package&lt;/a&gt;. I’ll spend more time explaining ELPD then. Another exciting tidbit from the talk, the loo package is getting a 2.0 version coming soon.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;talks&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Talks&lt;/h2&gt;
&lt;div id=&#34;were-leaving-one-out-wrong&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;We’re leaving-one-out wrong&lt;/h3&gt;
&lt;p&gt;Sophia Rabe-Hesketh gave a great first talk of the conference on leave-one-out cross-validation. Coming back to the ELPD, it looks like the way we’re computing them now doesn’t make sense for hierarchical models. Take for example a model where you have a random group: when you imagine the future distribution as your loo distribution, you are imagining drawing new subjects for your observed groups, but presumably your groups are random because in the future you’d like to sample new groups. In this case you can’t use the standard loo distribution to compute your ELPD, you need to generate marginal likelihoods and use those as a mixed-predictive distribution for computing ELPD. Sophia called this LOCO sampling, because it is akin to leaving one cluster (group) out at a time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;joint-models-are-in-rstanarm&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Joint Models are in rstanarm&lt;/h3&gt;
&lt;p&gt;Sam Brilleman gave a really cool talk on using coupled models for predicting time to events. For example using the trajectory of a biomarker to predict an adverse health event. By sharing parameters between the two models (proportional hazards for the event, hierarchical model for biomarkers) you share information between the two data types. In collaboration with Ben Goodrich this is now easy in rstanarm. Very cool.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;even-facebook-likes-stan&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Even facebook likes stan&lt;/h3&gt;
&lt;p&gt;Sean Taylor and Ben Letham from facebook came to talk about their awesome forecasting tool &lt;a href=&#34;https://github.com/facebook/prophet&#34;&gt;prophet&lt;/a&gt;. I saw the release for this a while back but hadn’t given it too much attention. Prophet emerged as a solution to many people wanting to do quite similar forecasting tasks at facebook, but without necessarily having the expertise. Most of facebook’s user data seemed to have commonality of features: day of the week trends, month trends, holiday effects, smooth temporal trajectories, so prophet made these models as simple and usable as possible. They reduce the time series to a standard regression after controlling for their multiple seasonalities, so they don’t need to futz with autoregression. Great example of simple outperforming complex.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stan-without-the-blockiness&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stan Without The Blockiness&lt;/h3&gt;
&lt;p&gt;Maria Gorinova had one of the best received talks (in my opinion). She introduced her blockless version of the stan language as an f# dsl (&lt;a href=&#34;https://github.com/mgorinova/SlicStan-Paper&#34;&gt;SlicStan&lt;/a&gt;). This seems like a step in the right direction for turning stan into a language people will want to write. SlicStan does away with the blocks and uses information flow analysis to determine which block code belongs in. Since blocks are executed at different frequencies SlicStan can choose the block executed the least frequently, so it is self optimizing at least at the block assignment level.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stan-gets-physical&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stan gets physical&lt;/h3&gt;
&lt;p&gt;I’m going to lump two great talks together. Ben Bales and Talia Weiss both gave awesome talks on using stan for physics problems. Ben showed how to use ringing frequencies and stan to estimate elastic constants for super-alloy materials. Talia showed how she used stan to assess the probability of quantum weirdness in the MINOS data as violations of the Leggett-Garg inequality.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;causality-in-stan&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Causality in Stan&lt;/h3&gt;
&lt;p&gt;Leah Comment gave an exciting talk about using Robins’ G-formula for causal effect estimation in stan. Causality has been a big open frontier in my learning since I picked up Pearl’s book last winter, and it’s great to see some practical examples using tools I know. Very excited to dig in to her notebooks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Distance&lt;/h3&gt;
&lt;p&gt;Susan Holmes gave a talk &lt;span class=&#34;citation&#34;&gt;@statwonk&lt;/span&gt; on twitter described as electric. I couldn’t agree more. I’ve been thinking on and off about distances in statistics for a little while now, especially as I’m currently trying to implement a gaussian process for some of my data. Choice of kernel is one thing, but choice of metric is a whole other ball game. Susan really hammered down a lot of the key ideas and showed me there are lots of people thinking about this already. Can’t wait to check out her student’s package: &lt;a href=&#34;https://github.com/nlhuong/buds&#34;&gt;buds&lt;/a&gt; for bayesian unidimensional scaling. The talk was a whirlwind, but one of the most interesting things I learned was the “horseshoe effect” in which in multidimensional scaling one-dimensional gradients appear as horseshoe-like curves. Another great idea was to register PCA basis vectors &lt;em&gt;a la&lt;/em&gt; image analysis to control for non-determinism in vector signs before doing analysis. And the quote of the conference “If you can find the right distance for your data, you can probably solve your problem”. That one is going to stick with me.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;andrews-tele-talk&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Andrew’s Tele-Talk&lt;/h3&gt;
&lt;p&gt;Andrew Gelman gave the closing talk for the conference remotely. Amusingly the audience was relocated to a chapel for the last round of talks. Andrew gave us a reminder to be humble in our work - in not so gentle terms. He then went on to talk about what he wants from stan in the future. One central theme was scaling bayesian inference up to larger probles. He pointed us to one of his papers “Expectation Propagation as a way of life”. This seems like it is important, and may be the bridge that will help us get to running bayesian models on large neuroimaging datasets. I’ve got some homework to do.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;outro&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outro&lt;/h2&gt;
&lt;p&gt;As you can see the conference was jam packed with interesting material. Seeing everyone doing such cool stuff with stan is inspiring, I’m excited to get back and see what I can incorporate into my work.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Welcome</title>
      <link>/blog/post/welcome/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/welcome/</guid>
      <description>&lt;p&gt;Hello World!&lt;/p&gt;
&lt;p&gt;Welcome to the new Mouse Imaging Centre Blog.&lt;/p&gt;
&lt;p&gt;We created this blog as a place to share things we’ve been thinking about lately. This blog will feature a mixture of technical writing about things we do in our lab including imaging, statistics, and biology. The blog will feature contributions from our staff, students, and PIs.&lt;/p&gt;
&lt;p&gt;We hope you enjoy our writings.&lt;/p&gt;
&lt;p&gt;To learn more about us please visit our website at &lt;a href=&#34;http://www.mouseimaging.ca/&#34; class=&#34;uri&#34;&gt;http://www.mouseimaging.ca/&lt;/a&gt; or check us out on github &lt;a href=&#34;https://github.com/Mouse-Imaging-Centre&#34; class=&#34;uri&#34;&gt;https://github.com/Mouse-Imaging-Centre&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/blog/about/</link>
      <pubDate>Sun, 20 Aug 2017 21:38:52 +0800</pubDate>
      
      <guid>/blog/about/</guid>
      <description>&lt;div id=&#34;about-this-blog&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About this blog&lt;/h2&gt;
&lt;p&gt;This is the blog for the Mouse Imaging Centre. Here you’ll find a collection of our thoughts on imaging, statistics, and biology, written by our staff, students, and PIs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;about-the-us&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;About the us&lt;/h2&gt;
&lt;p&gt;The Mouse Imaging Centre operates out of the Hospital for Sick Children in Toronto, our mission is to use high-throughput mouse imaging to answer important questions about biology. For more about us please visit our &lt;a href=&#34;http://www.mouseimaging.ca/&#34;&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
