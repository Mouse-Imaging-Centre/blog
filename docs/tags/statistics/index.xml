<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on The Mouse Imaging Centre Blog</title>
    <link>/blog/tags/statistics/</link>
    <description>Recent content in Statistics on The Mouse Imaging Centre Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 06 Jul 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/blog/tags/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Linear Models: Understanding the Error Estimates for Binary Variables</title>
      <link>/blog/post/2018-07-06_linearmodelserrors/</link>
      <pubDate>Fri, 06 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-07-06_linearmodelserrors/</guid>
      <description>Introduction library(tidyverse) library(matlib) library(knitr) library(RColorBrewer) The purpose of this document is to understand the parameter and residuals error estimates in a basic linear regression model when working with binary categorical variables. Recall the general model definition:
\[ \mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{e}\]
where \(\mathbf{X}\) is the design matrix and \(\mathbf{\beta}\) is a \((p+1)\)-vector of coefficients/parameters, including the intercept parameter. The errors are normally distributed around 0 with variance \(\sigma^2\):
\[e \sim N(0,\sigma^2) \quad .</description>
    </item>
    
    <item>
      <title>An overfit representation of ICLR 2018</title>
      <link>/blog/post/2018-05-30_iclr_redux/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-05-30_iclr_redux/</guid>
      <description>I was recently extremely fortunate to attend ICLR 2018, albeit as something of an interloper. Accordingly, what follows is surely a rather atypical highlight reel. All pedantry and any inaccuracy is, of course, due to my own limited understanding of these elegant topics and the breadth of their application.
Causal reasoning and graphical models There is a well-developed modern theory of causal inference and reasoning based on graphical models developed by Judea Pearl and others.</description>
    </item>
    
    <item>
      <title>Bayesian Model Selection with PSIS-LOO</title>
      <link>/blog/post/2018-01-31_loo-intro/</link>
      <pubDate>Wed, 31 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-01-31_loo-intro/</guid>
      <description>Pitch In this post I’d like to provide an overview of Pareto-Smoothed Importance Sampling (PSIS-LOO) and how it can be used for bayesian model selection. Everything I discuss regarding this technique can be found in more detail in Vehtari, Gelman, and Gabry (2016). To lead up to PSIS-LOO I will introduce Akaike’s Information Criterion (AIC) to lay the foundation for model selection in general, then cover the expected log predictive density, the corner stone of bayesian model selection.</description>
    </item>
    
  </channel>
</rss>