<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">  
  <channel>
    <title>Linear Models on The Mouse Imaging Centre Blog</title>
    <link>/blog/tags/linear-models/</link>
    <description>Recent content in Linear Models on The Mouse Imaging Centre Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 06 Jul 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/blog/tags/linear-models/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Linear Models: Understanding the Error Estimates for Binary Variables</title>
      <link>/blog/post/2018-07-06_linearmodelserrors/</link>
      <pubDate>Fri, 06 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-07-06_linearmodelserrors/</guid>
      <description><![CDATA[
      <div id="introduction" class="section level1">
<h1>Introduction</h1>
<pre class="r"><code>library(tidyverse)
library(matlib)
library(knitr)
library(RColorBrewer)</code></pre>
<p>The purpose of this document is to understand the parameter and residuals error estimates in a basic linear regression model when working with <strong>binary categorical variables</strong>. Recall the general model definition:</p>
<p><span class="math display">\[ \mathbf{y} = \mathbf{X}\mathbf{\beta} + \mathbf{e}\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}\)</span> is the <strong>design matrix</strong> and <span class="math inline">\(\mathbf{\beta}\)</span> is a <span class="math inline">\((p+1)\)</span>-vector of coefficients/parameters, including the intercept parameter. The errors are normally distributed around 0 with variance <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[e \sim N(0,\sigma^2) \quad .\]</span></p>
<p>Upon fitting the model to data, we obtain estimates for the coefficients, <span class="math inline">\(\hat{\beta}\)</span>. These estimates have an associated covariance matrix <span class="math inline">\(\sigma^2_\beta\)</span>, which is used for statistical inference. The covariance matrix of the parameters is calculated from the estimate for the residual standard error in the following way:</p>
<p><span class="math display">\[\mathbf{\sigma}^2_\beta = (\mathbf{X}&#39;\mathbf{X})^{-1}\hat{\sigma}^2 \quad .\]</span></p>
<p>The focus of this document will be on understanding the details of this covariance matrix, specifically of the parameter standard errors and the residual standard error, <span class="math inline">\(\hat{\sigma}\)</span>. The squared parameter standard errors (i.e. parameter variances) are the diagonal terms in the covariance matrix, and so the two measures of variability are related to one another in the following way:</p>
<p><span class="math display">\[\text{diag}[\mathbf{\sigma}^2_\beta] = \text{diag}[(\mathbf{X}&#39;\mathbf{X})^{-1}]\hat{\sigma}^2 \quad .\]</span></p>
<p>The standard errors can then be obtained by taking the square root of the variances. This transformation between the residual standard error and the parameter standard errors is not trivial and depends on the number of parameters and type of variables. Recall that the estimate for <span class="math inline">\(\sigma\)</span> is given by</p>
<p><span class="math display">\[\hat{\sigma}^2 = \frac{1}{n-p-1} \sum_{i = 1}^n e^2_i \quad .\]</span></p>
<p>If the data includes observations of categorical variables such that it can be pooled into <strong>balanced groups</strong> with potentially different group sample standard deviations of <span class="math inline">\(\sigma_g\)</span>, it can be shown straightforwardly that if <span class="math inline">\(n \gg p\)</span>, i.e. in the regime of low-dimensional data,</p>
<p><span class="math display">\[\hat{\sigma}^2 = \text{Ave}[\sigma^2_g] \quad .\]</span></p>
<p>If the sample standard deviations of the groups are identical, then <span class="math inline">\(\hat{\sigma} = \sigma_g\)</span>. What this tells us is that the residual standard error is an estimate on the standard deviation of the groups defined by the model.</p>
<p>We will get a sense of how these estimates vary by generating some simulated data and playing with different linear models of the data. In particular, we will simulate a <strong>balanced experimental design</strong> consisting of independent binary categorical variables and a continuous response. We will consider three binary variables: <code>Genotype</code>, <code>Anxiety</code>, and <code>Treatment</code>. The use of binary variables in linear models has the effect of pooling the data across different groups. Since there are 3 binary variables, there will be 8 separate groups, i.e. <span class="math inline">\(2^3\)</span>. To operate well within the low-dimensitonality regime, we will use a large sample size. The data will be simulated by first generating a scaffold data frame containing the observations for the different categorical variables. The scaffold data frame will then be used to generate the response variable stochastically. We will start by generating the scaffold data frame.</p>
<pre class="r"><code>#Define variables related to the experimental design. 
# Sample size
nSample &lt;- 100
# Number of binary variables
nBinVar &lt;- 3
# Number of distinct groups
nGroups &lt;- 2^nBinVar
#Total number of observations
nObs &lt;- nSample*nGroups

#Generate data frame (handy trick uisng expand.grid() and map_df())
dfGrid &lt;- expand.grid(Genotype = c(&quot;WT&quot;,&quot;MUT&quot;), 
                      Anxiety = c(&quot;No&quot;,&quot;Yes&quot;), 
                      Treatment = c(&quot;Placebo&quot;, &quot;Drug&quot;))
dfSimple &lt;- map_df(seq_len(nSample), ~dfGrid) %&gt;% 
  mutate(Genotype = factor(Genotype, levels = c(&quot;WT&quot;,&quot;MUT&quot;)),
         Anxiety = factor(Anxiety, levels = c(&quot;No&quot;,&quot;Yes&quot;)),
         Treatment = factor(Treatment, levels = c(&quot;Placebo&quot;,&quot;Drug&quot;)))

#Verify that this worked
dfSimple %&gt;% 
  group_by(Genotype, Anxiety, Treatment) %&gt;% 
  count</code></pre>
<pre><code>## # A tibble: 8 x 4
## # Groups:   Genotype, Anxiety, Treatment [8]
##   Genotype Anxiety Treatment     n
##   &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;
## 1 WT       No      Placebo     100
## 2 WT       No      Drug        100
## 3 WT       Yes     Placebo     100
## 4 WT       Yes     Drug        100
## 5 MUT      No      Placebo     100
## 6 MUT      No      Drug        100
## 7 MUT      Yes     Placebo     100
## 8 MUT      Yes     Drug        100</code></pre>
<p>Now we create the distribution of the response based on the independent variables. We will generate data in which <strong>there are no interactions between any of the predictor variables</strong>. The response will be generated using standardized units so that it can stand in for any physical variable. The main effects of the predictors on the response are taken to be <span class="math inline">\(2\sigma\)</span>, where <span class="math inline">\(\sigma\)</span> is the standard deviation of the normal distribution used to generate observations of the response.</p>
<pre class="r"><code>#Simulation parameters
meanRef &lt;- 0
sigma &lt;- 1
effectGenotype &lt;- 2*sigma
effectAnxiety &lt;- 2*sigma
effectTreatment &lt;- 2*sigma

#Generate data based on experimental design. 
#In this case, no interaction between variables.
dfSimple$Response &lt;- meanRef +
  effectGenotype*(as.numeric(dfSimple$Genotype)-1) +
  effectAnxiety*(as.numeric(dfSimple$Anxiety)-1) +
  effectTreatment*(as.numeric(dfSimple$Treatment)-1) +
  rnorm(nrow(dfSimple), 0, sigma)

ggplot(dfSimple, aes(x = Genotype, y = Response, col = Anxiety)) + 
  geom_jitter(width = 0.2) + 
  facet_grid(.~Treatment) + 
  scale_color_brewer(palette = &quot;Set1&quot;)</code></pre>
<p><img src="/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>With the data generated we can start considering different linear models to understand the error/variance estimates. In the following Sections we will consider models with and without interactions to examine how the error estimates change. The general process will be to run a given model on the simulated data and examine the details of the transformation from <span class="math inline">\(\sigma\)</span> to <span class="math inline">\(\sigma_\beta\)</span> for that model. In doing so we will see that a number of patterns emerge.</p>
</div>
<div id="non-interactive-models" class="section level1">
<h1>Non-interactive Models</h1>
<div id="model-1-intercept-term-only" class="section level2">
<h2>Model 1: Intercept Term Only</h2>
<p>The first model is one where the only parameter is the intercept. The <span class="math inline">\(\beta\)</span> estimate returned will be the mean of the data pooled across all groups. The distribution of the pooled response observations is as follows:</p>
<pre class="r"><code>ggplot(dfSimple, aes(x = Response)) + 
  geom_histogram(binwidth = 1,
                 alpha = 0.7,
                 col = &quot;black&quot;,
                 fill = brewer.pal(3,&quot;Set1&quot;)[2])</code></pre>
<p><img src="/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The model is written as</p>
<pre class="r"><code>linMod1 &lt;- lm(Response ~ 1, data = dfSimple)
summary(linMod1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Response ~ 1, data = dfSimple)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.9321 -1.4254  0.0717  1.3874  5.6359 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.04683    0.07026   43.36   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.987 on 799 degrees of freedom</code></pre>
<p>In this simple case, the residual standard error is simply the standard deviation of the full data:</p>
<pre class="r"><code>dfSimple %&gt;% 
  summarise(sd(Response))</code></pre>
<pre><code>##   sd(Response)
## 1     1.987383</code></pre>
<p>What about the standard error of the intercept?</p>
<p>Since there is only one parameter for the intercept, the design matrix will just be a vector of ones. The transformation <span class="math inline">\(\mathbf{X}&#39;\mathbf{X}\)</span> is then just the squared norm of the vector and will be equal to the number of observations in the data set, i.e. <span class="math inline">\(\sum_{i=1}^n 1 = n\)</span>. The inverse operation is just that for a scalar value and we get <span class="math inline">\(\sigma_\beta = \frac{\sigma}{\sqrt{n}}\)</span>. Multiplying the standard error estimate by <span class="math inline">\(\sqrt{n}\)</span> should return the value of the residual standard error:</p>
<pre class="r"><code>summary(linMod1)$coefficients[[&quot;(Intercept)&quot;,&quot;Std. Error&quot;]]*sqrt(nObs)</code></pre>
<pre><code>## [1] 1.987383</code></pre>
</div>
<div id="model-2-one-binary-predictor" class="section level2">
<h2>Model 2: One Binary Predictor</h2>
<p>Next we add one of the binary variables as a predictor in the model. This will have the effect of pooling the data according to the different levels of that predictor.</p>
<pre class="r"><code>ggplot(dfSimple, aes(x = Genotype, y = Response)) +
  geom_jitter(width = 0.2,
              col = brewer.pal(3,&quot;Set1&quot;)[2])</code></pre>
<p><img src="/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>In this case the intercept estimate will indicate the pooled mean of the wildtype group (or whatever the reference level is for the chosen predictor) and the slope estimate will indicate the difference between the wildtype mean and the pooled mean of the mutant group.</p>
<pre class="r"><code>linMod2 &lt;- lm(Response ~ Genotype, data = dfSimple)
summary(linMod2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Response ~ Genotype, data = dfSimple)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.933 -1.212 -0.024  1.215  5.291 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.04567    0.08588   23.82   &lt;2e-16 ***
## GenotypeMUT  2.00231    0.12145   16.49   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.718 on 798 degrees of freedom
## Multiple R-squared:  0.2541, Adjusted R-squared:  0.2532 
## F-statistic: 271.8 on 1 and 798 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>What do the variance estimates represent? Remember that the residual standard error is an estimate of the variability across all of the data. As mentioned in the introduction, the residual standard error will be the square root of the average of the sample variances of the two groups. The group variances and the resulting standard error estimate are:</p>
<pre class="r"><code>dfSimple %&gt;% 
  group_by(Genotype) %&gt;% 
  summarise(varPerGroup = var(Response)) %&gt;% 
  ungroup() %&gt;% 
  mutate(sigma = sqrt(mean(varPerGroup)))</code></pre>
<pre><code>## # A tibble: 2 x 3
##   Genotype varPerGroup sigma
##   &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt;
## 1 WT              3.01  1.72
## 2 MUT             2.89  1.72</code></pre>
<p>Which is equal to the estimate for the residuals standard error.</p>
<p>How do the standard errors of the parameters relate to the residual standard error for this model? Let’s compute the transformation explicitly using the design matrix. First we compute <span class="math inline">\(\mathbf{X&#39;X}\)</span>:</p>
<pre class="r"><code>xMat &lt;- model.matrix(linMod2)
t(xMat)%*%xMat</code></pre>
<pre><code>##             (Intercept) GenotypeMUT
## (Intercept)         800         400
## GenotypeMUT         400         400</code></pre>
<p>Indicating the number of observations explicitly, we can see that this matrix is of the form:</p>
<p><span class="math display">\[\mathbf{X}&#39;\mathbf{X} = n \cdot \begin{bmatrix} 1 &amp; \frac{1}{2} \\ \frac{1}{2} &amp; \frac{1}{2} \end{bmatrix}\]</span></p>
<p>Taking the inverse (which is a straightforward process for a 2x2 matrix such as this) and multiplying by <span class="math inline">\(\sigma\)</span>, we find that the covariance matrix is</p>
<p><span class="math display">\[\sigma^2_\beta = \frac{\sigma^2}{n} \cdot \begin{bmatrix} 2 &amp; -2 \\ -2 &amp; 4 \end{bmatrix}\]</span></p>
<p>Specifically, the standard errors of the estimates are given by the square roots of the diagonal terms in this matrix (note that this isn’t a proper matrix operation but think of this as extracting the diagonal elements and then taking the square root of each of them):</p>
<p><span class="math display">\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{2} &amp; \sqrt{4} \end{bmatrix}\]</span></p>
<p>A point of interest here is that the parameter errors are related to this quantity <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span> but are scaled by some multiplicative factor. Notably, the slope parameter is more uncertain than the intercept. Multiplying the parameter standard errors by the appropriate multiplicative factors, we should recover the residual standard error:</p>
<pre class="r"><code>vec &lt;- c(sqrt(nObs/2), sqrt(nObs/4))
summary(linMod2)$coefficients[,&quot;Std. Error&quot;]*vec</code></pre>
<pre><code>## (Intercept) GenotypeMUT 
##    1.717502    1.717502</code></pre>
</div>
<div id="model-3-two-binary-predictors" class="section level2">
<h2>Model 3: Two Binary Predictors</h2>
<p>In our third model, we consider the effects of two binary predictors without an interaction. This will pool the data into the <span class="math inline">\(2^2=4\)</span> groups defined by these predictors:</p>
<pre class="r"><code>ggplot(dfSimple, aes(x = Genotype, y = Response, col = Anxiety)) +
  geom_jitter(width = 0.2) + 
  scale_color_brewer(palette = &quot;Set1&quot;)</code></pre>
<p><img src="/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>The model is as followed:</p>
<pre class="r"><code>linMod3 &lt;- lm(Response ~ Genotype + Anxiety, data = dfSimple)
summary(linMod3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Response ~ Genotype + Anxiety, data = dfSimple)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.940 -1.058  0.046  1.026  4.298 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.05280    0.08582   12.27   &lt;2e-16 ***
## GenotypeMUT  2.00231    0.09910   20.21   &lt;2e-16 ***
## AnxietyYes   1.98574    0.09910   20.04   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.401 on 797 degrees of freedom
## Multiple R-squared:  0.504,  Adjusted R-squared:  0.5027 
## F-statistic: 404.9 on 2 and 797 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We expect that the residual standard error should be approximately equal to the average of the group standard deviations. Note that with the addition of new predictors, we will move slowly out of the regime where <span class="math inline">\(n \gg p\)</span> holds.</p>
<pre class="r"><code>dfSimple %&gt;% 
  group_by(Genotype, Anxiety) %&gt;%
  summarise(varPerGroup = var(Response)) %&gt;%
  ungroup() %&gt;%
  mutate(sigma = sqrt(mean(varPerGroup)))</code></pre>
<pre><code>## # A tibble: 4 x 4
##   Genotype Anxiety varPerGroup sigma
##   &lt;fct&gt;    &lt;fct&gt;         &lt;dbl&gt; &lt;dbl&gt;
## 1 WT       No             1.70  1.40
## 2 WT       Yes            2.05  1.40
## 3 MUT      No             2.05  1.40
## 4 MUT      Yes            2.04  1.40</code></pre>
<p>How do the errors of the parameter estimates relate back to this?</p>
<pre class="r"><code>xMat &lt;- model.matrix(linMod3)
t(xMat) %*% xMat</code></pre>
<pre><code>##             (Intercept) GenotypeMUT AnxietyYes
## (Intercept)         800         400        400
## GenotypeMUT         400         400        200
## AnxietyYes          400         200        400</code></pre>
<p>Explicitly indicating the number of observations, we have:</p>
<p><span class="math display">\[ \mathbf{X&#39;X} = n \cdot \begin{bmatrix} 1 &amp; \frac{1}{2} &amp; \frac{1}{2} \\ \frac{1}{2} &amp; \frac{1}{2} &amp; \frac{1}{4} \\ \frac{1}{2} &amp; \frac{1}{4} &amp; \frac{1}{2} \end{bmatrix}\]</span></p>
<p>Taking the inverse (which is a tedious process for any matrix of dimension greater than 2), the covariance matrix is</p>
<p><span class="math display">\[\sigma^2_\beta = \frac{\sigma^2}{n}\cdot\begin{bmatrix} 3 &amp; -2 &amp; -2 \\ -2 &amp; 4 &amp; 0 \\ -2 &amp; 0 &amp; 4 \end{bmatrix}\]</span></p>
<p>And the standard errors are given by:</p>
<p><span class="math display">\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{3} &amp; \sqrt{4} &amp; \sqrt{4} \end{bmatrix}\]</span></p>
<p>Notice that this is similar to the mapping from the previous model, except that the intercept error is now estimated using <span class="math inline">\(\sqrt{3}\)</span> rather than <span class="math inline">\(\sqrt{2}\)</span>. Interestingly including an additional predictor does not change the conversion factors for the slope parameters. Applying the appropriate multiplicative factors to the error estimates, we should recover the residual standard error:</p>
<pre class="r"><code>vec &lt;- c(sqrt(nObs/3), sqrt(nObs/4), sqrt(nObs/4))
summary(linMod3)$coefficients[,&quot;Std. Error&quot;]*vec</code></pre>
<pre><code>## (Intercept) GenotypeMUT  AnxietyYes 
##    1.401431    1.401431    1.401431</code></pre>
</div>
<div id="model-4-three-binary-predictors" class="section level2">
<h2>Model 4: Three Binary Predictors</h2>
<p>In order to get a clearer sense of the trend in the error estimates with regards to binary predictors, we will add the third main effect into the model. In this case the model will utilize the full 8 groups in the data.</p>
<pre class="r"><code>ggplot(dfSimple, aes(x = Genotype, y = Response, col = Anxiety)) +
  geom_jitter(width = 0.2) + 
  facet_grid(.~Treatment) +
  scale_color_brewer(palette = &quot;Set1&quot;)</code></pre>
<p><img src="/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Here is the model:</p>
<pre class="r"><code>linMod4 &lt;- lm(Response ~ Genotype + Anxiety + Treatment, data = dfSimple)
summary(linMod4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Response ~ Genotype + Anxiety + Treatment, data = dfSimple)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.9908 -0.6887 -0.0230  0.6759  3.3164 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    0.07123    0.07065   1.008    0.314    
## GenotypeMUT    2.00231    0.07065  28.343   &lt;2e-16 ***
## AnxietyYes     1.98574    0.07065  28.109   &lt;2e-16 ***
## TreatmentDrug  1.96314    0.07065  27.789   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9991 on 796 degrees of freedom
## Multiple R-squared:  0.7482, Adjusted R-squared:  0.7473 
## F-statistic: 788.5 on 3 and 796 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Note that this is the proper model to describe the data based on how we’ve simulated it. In this case the intercept should describe the mean of the reference group, i.e. untreated wildtypes with no anxiety, while the slope parameters should estimate the inputs that we put into the model. The residuals standard error should describe the standard deviation of the response within the 8 different groups, which in this case amounts to the value of <span class="math inline">\(\sigma\)</span> that we specified when simulating the data. The group standard deviations and their average are:</p>
<pre class="r"><code>dfSimple %&gt;% 
  group_by(Genotype, Anxiety, Treatment) %&gt;% 
  summarise(varPerGroup = var(Response)) %&gt;%
  ungroup %&gt;%
  mutate(sigma = sqrt(mean(varPerGroup)))</code></pre>
<pre><code>## # A tibble: 8 x 5
##   Genotype Anxiety Treatment varPerGroup sigma
##   &lt;fct&gt;    &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt; &lt;dbl&gt;
## 1 WT       No      Placebo         1.01  0.996
## 2 WT       No      Drug            0.857 0.996
## 3 WT       Yes     Placebo         1.04  0.996
## 4 WT       Yes     Drug            1.11  0.996
## 5 MUT      No      Placebo         0.995 0.996
## 6 MUT      No      Drug            0.748 0.996
## 7 MUT      Yes     Placebo         1.14  0.996
## 8 MUT      Yes     Drug            1.04  0.996</code></pre>
<p>As expected the residual standard error is approximately the average of the group standard deviations.</p>
<p>What about the parameter errors?</p>
<pre class="r"><code>xMat &lt;- model.matrix(linMod4)
t(xMat) %*% xMat</code></pre>
<pre><code>##               (Intercept) GenotypeMUT AnxietyYes TreatmentDrug
## (Intercept)           800         400        400           400
## GenotypeMUT           400         400        200           200
## AnxietyYes            400         200        400           200
## TreatmentDrug         400         200        200           400</code></pre>
<p>Which gives</p>
<p><span class="math display">\[\mathbf{X&#39;X} = n \cdot \begin{bmatrix} 1 &amp; \frac{1}{2} &amp; \frac{1}{2} &amp; \frac{1}{2} \\ \frac{1}{2} &amp; \frac{1}{2} &amp; \frac{1}{4} &amp; \frac{1}{4} \\ \frac{1}{2} &amp; \frac{1}{4} &amp; \frac{1}{2} &amp; \frac{1}{4} \\ \frac{1}{2} &amp; \frac{1}{4} &amp; \frac{1}{4} &amp; \frac{1}{2}  \end{bmatrix}\]</span></p>
<p>The full covariance matrix is then:</p>
<p><span class="math display">\[\sigma^2_\beta = \frac{\sigma^2}{n}\cdot\begin{bmatrix} 4 &amp; -2 &amp; -2 &amp; -2 \\ -2 &amp; 4 &amp; 0 &amp; 0 \\ -2 &amp; 0 &amp; 4 &amp; 0 \\ -2 &amp; 0 &amp; 0 &amp; 4 \end{bmatrix}\]</span></p>
<p>And the standard errors are:</p>
<p><span class="math display">\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{4} &amp; \sqrt{4} &amp; \sqrt{4} &amp; \sqrt{4} \end{bmatrix}\]</span></p>
<p>As we can see, the trend from the previous Section continues. The conversion factor for the intercept term is related to the number of parameters in the model, while the values related to the slope parameters are still simply <span class="math inline">\(\sqrt{4}\)</span>. We can expect that, as we continue to add binary predictors, the intercept term will be related to the number of parameters, while the slope parameters will have a conversion of <span class="math inline">\(\sqrt{4}\)</span>. As in the previous Sections, we can recover the residual standard error by multiplying by the appropriate factors:</p>
<pre class="r"><code>vec &lt;- c(sqrt(nObs/4), sqrt(nObs/4), sqrt(nObs/4), sqrt(nObs/4))
summary(linMod4)$coefficients[,&quot;Std. Error&quot;]*vec</code></pre>
<pre><code>##   (Intercept)   GenotypeMUT    AnxietyYes TreatmentDrug 
##     0.9990751     0.9990751     0.9990751     0.9990751</code></pre>
</div>
<div id="recap" class="section level2">
<h2>Recap</h2>
<p>At this stage let’s compare the conversion factors from <span class="math inline">\(\sigma\)</span> to <span class="math inline">\(\sigma_\beta\)</span> for all non-interactive models.</p>
<pre class="r"><code>data.frame(Beta0 = c(1/sqrt(nObs), sqrt(2/nObs), sqrt(3/nObs), sqrt(nObs/4)),
           Beta1 = c(NA, sqrt(4/nObs), sqrt(4/nObs), sqrt(4/nObs)),
           Beta2 = c(NA, NA, sqrt(4/nObs), sqrt(4/nObs)),
           Beta3 = c(NA, NA, NA, sqrt(4/nObs)),
           row.names = c(&quot;Model 1&quot;, &quot;Model 2&quot;, &quot;Model 3&quot;, &quot;Model 4&quot;)) %&gt;% kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Beta0</th>
<th align="right">Beta1</th>
<th align="right">Beta2</th>
<th align="right">Beta3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Model 1</td>
<td align="right">0.0353553</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">Model 2</td>
<td align="right">0.0500000</td>
<td align="right">0.0707107</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="odd">
<td align="left">Model 3</td>
<td align="right">0.0612372</td>
<td align="right">0.0707107</td>
<td align="right">0.0707107</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">Model 4</td>
<td align="right">14.1421356</td>
<td align="right">0.0707107</td>
<td align="right">0.0707107</td>
<td align="right">0.0707107</td>
</tr>
</tbody>
</table>
<p>As one might expect, we do see some pattern. Specifically, using the total number of observations, we can express this table as:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Beta0</th>
<th align="left">Beta1</th>
<th align="left">Beta2</th>
<th align="left">Beta3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Model 1</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{1}{n}}\)</span></td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
<tr class="even">
<td align="left">Model 2</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{2}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left">NA</td>
<td align="left">NA</td>
</tr>
<tr class="odd">
<td align="left">Model 3</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{3}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left">NA</td>
</tr>
<tr class="even">
<td align="left">Model 4</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
</tr>
</tbody>
</table>
<p>As mentioned previously, the multiplicative factor for the intercept error involves the square root of the number of coefficients in the model. Moreover, for the rest of the models, the multiplicative factor is only ever <span class="math inline">\(\sqrt{\frac{4}{n}}\)</span>, i.e. the mappings don’t change as we add more binary variables. This makes sense given that all of the variables are independent in these models. Note that there is no pattern when expressing these conversion factors in terms of the number of data points per group, whic we will denote <span class="math inline">\(N\)</span>:</p>
<p><span class="math display">\[\begin{bmatrix} \sqrt{\frac{1}{N}} &amp; &amp; &amp; \\ \sqrt{\frac{1}{N}}&amp; \sqrt{\frac{2}{N}} &amp; &amp; \\ \sqrt{\frac{3}{4N}} &amp; \sqrt{\frac{1}{N}} &amp; \sqrt{\frac{1}{N}} &amp; \\ \sqrt{\frac{1}{2N}} &amp; \sqrt{\frac{1}{2N}} &amp; \sqrt{\frac{1}{2N}} &amp; \sqrt{\frac{1}{2N}} \end{bmatrix}\]</span></p>
<p>Recall that <span class="math inline">\(N\)</span> here is different for each row, since the different models pool the data in different ways, and takes on values <span class="math inline">\(\{n, \frac{n}{2}, \frac{n}{4}, \frac{n}{8}\}\)</span>.</p>
</div>
</div>
<div id="interactive-models" class="section level1">
<h1>Interactive Models</h1>
<p>In this Section we explore the influence of interactions on the parameter error estimates and the mapping from the residual standard error.</p>
<div id="two-binary-predictors-with-interaction" class="section level2">
<h2>Two Binary Predictors with Interaction</h2>
<p>In this case we consider the interaction between two of the variables, <code>Genotype</code> and <code>Anxiety</code>.</p>
<pre class="r"><code>linModInt &lt;- lm(Response ~ Genotype*Anxiety, data = dfSimple)
summary(linModInt)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Response ~ Genotype * Anxiety, data = dfSimple)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.0109 -1.0434  0.0248  1.0528  4.2269 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             0.98172    0.09903   9.913   &lt;2e-16 ***
## GenotypeMUT             2.14447    0.14005  15.312   &lt;2e-16 ***
## AnxietyYes              2.12790    0.14005  15.194   &lt;2e-16 ***
## GenotypeMUT:AnxietyYes -0.28431    0.19806  -1.435    0.152    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.4 on 796 degrees of freedom
## Multiple R-squared:  0.5053, Adjusted R-squared:  0.5034 
## F-statistic:   271 on 3 and 796 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Note that though we have added a new predictor, there are still only 4 groups, as in Model 3. The difference is that the mean values of these groups may be more accurately estimated. In the present case we don’t expect this model to out-perform the model without an interaction, since there is no real interaction in the data, making the interaction parameter superfluous. This means that the estimate for the residual standard error should be similar to that from the non-interactive model. If the situation were reversed however and the data truly contained an interaction, then this model would more appropriately recapitulate the group means and lead to a more accurate estimation of <span class="math inline">\(\sigma\)</span>.</p>
<p>The design matrix will be different either way however due to the additional interaction predictor.</p>
<pre class="r"><code>xMat &lt;- model.matrix(linModInt)
t(xMat) %*% xMat</code></pre>
<pre><code>##                        (Intercept) GenotypeMUT AnxietyYes
## (Intercept)                    800         400        400
## GenotypeMUT                    400         400        200
## AnxietyYes                     400         200        400
## GenotypeMUT:AnxietyYes         200         200        200
##                        GenotypeMUT:AnxietyYes
## (Intercept)                               200
## GenotypeMUT                               200
## AnxietyYes                                200
## GenotypeMUT:AnxietyYes                    200</code></pre>
<p>Explicitly using the observation number, we have:</p>
<p><span class="math display">\[n \cdot \begin{bmatrix} 1 &amp; \frac{1}{2} &amp; \frac{1}{2} &amp; \frac{1}{4} \\ \frac{1}{2} &amp; \frac{1}{2} &amp; \frac{1}{4} &amp; \frac{1}{4} \\ \frac{1}{2} &amp; \frac{1}{4} &amp; \frac{1}{2} &amp; \frac{1}{4} \\ \frac{1}{4} &amp; \frac{1}{4} &amp; \frac{1}{4} &amp; \frac{1}{4}  \end{bmatrix}\]</span></p>
<p>The covariance matrix is:</p>
<p><span class="math display">\[\sigma^2_\beta = \frac{\sigma^2}{n}\cdot\begin{bmatrix} 4 &amp; -4 &amp; -4 &amp; 4 \\ -4 &amp; 8 &amp; 4 &amp; -8 \\ -4 &amp; 4 &amp; 8 &amp; -8 \\ 4 &amp; -8 &amp; -8 &amp; 16 \end{bmatrix}\]</span></p>
<p>with parameter standard errors of</p>
<p><span class="math display">\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{4} &amp; \sqrt{8} &amp; \sqrt{8} &amp; \sqrt{16} \end{bmatrix}\]</span></p>
<p>Here we see a pattern change from the models without an interaction. The intercept mapping still involves a scaling factor that uses the number of parameters in the model, but the standard errors for the main effects parameters are now larger by a factor of <span class="math inline">\(\sqrt{2}\)</span> compared to the model without an interaction. The parameter error for the interaction is also larger than that for the main effects. These considerations will have a slight impact on the inferential side of linear modelling. Specifically, an interaction effect will always be less powerful than a main effect, and a main effect in a model with an interaction will always be less powerful than a main effect in a model without an interaction. The reason for this is that the <span class="math inline">\(t\)</span>-statistic is computed as <span class="math inline">\(t = \hat{\beta}/\sigma_\beta\)</span>. Of course this depends on what model accurately describes the data. The aforementioned power of a non-interactive model will be thrown off on data with an interaction, since the estimate for <span class="math inline">\(\sigma\)</span> will be larger due to the interaction in the data. These are things to keep in mind when considering which model to use.</p>
<p>Applying the mappings to the parameter standard errors, we recover the residual standard error:</p>
<pre class="r"><code>vec &lt;- c(sqrt(nObs/4), sqrt(nObs/8), sqrt(nObs/8), sqrt(nObs/16))
summary(linModInt)$coefficients[,&quot;Std. Error&quot;]*vec</code></pre>
<pre><code>##            (Intercept)            GenotypeMUT             AnxietyYes 
##               1.400499               1.400499               1.400499 
## GenotypeMUT:AnxietyYes 
##               1.400499</code></pre>
</div>
<div id="interaction-without-main-effect" class="section level2">
<h2>Interaction without Main Effect</h2>
<p>In this final Section I examine the interesting case of a model with a second order interaction but without the main effect for one of the predictors. What this model does is that it describes data in which the reference group for one of the binary variables (e.g. Wildtypes) is not influenced by observations of another variable (e.g. Anxiety). In order to get the <code>lm()</code> function to do this properly, we have to create an explicit dummy encoding of the variable with a main effect.</p>
<pre class="r"><code>dfSimple &lt;- dfSimple %&gt;% 
  mutate(GenotypeDummy = case_when(Genotype == &quot;WT&quot; ~ 0,
                                                          Genotype == &quot;MUT&quot; ~ 1))
linModInt2 &lt;- lm(Response ~ GenotypeDummy + GenotypeDummy:Anxiety, data = dfSimple)
summary(linModInt2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Response ~ GenotypeDummy + GenotypeDummy:Anxiety, 
##     data = dfSimple)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.0109 -1.1560  0.0171  1.1692  5.2909 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)               2.04567    0.07948  25.737  &lt; 2e-16 ***
## GenotypeDummy             1.08052    0.13767   7.849 1.35e-14 ***
## GenotypeDummy:AnxietyYes  1.84359    0.15897  11.597  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.59 on 797 degrees of freedom
## Multiple R-squared:  0.3618, Adjusted R-squared:  0.3602 
## F-statistic: 225.9 on 2 and 797 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Note that, in comparison to the fully interactive model presented in the previous Section, the residual standard error estimate is different. This is because the model has pooled the anxiety “yes” and “no” groups for the wildtypes. Since the data we generated included a main effect of anxiety, the variance of this wildtype group will be larger than that of the other two groups (mutant-no-anxiety and mutant-yes-anxiety). Additionally, the residual standard error is no longer just the average of the group standard deviations. This is due mainly to the fact that the wildtype group in this model is twice as large as the other two groups. To demonstrate this, let’s compute the naive average of group standard deviations:</p>
<pre class="r"><code>(dfTemp &lt;- dfSimple %&gt;% 
  mutate(NewGroups = case_when(Genotype == &quot;WT&quot; ~ &quot;WT&quot;,
                               Genotype == &quot;MUT&quot; &amp; Anxiety == &quot;No&quot; ~ &quot;NoMUT&quot;,
                               Genotype == &quot;MUT&quot; &amp; Anxiety == &quot;Yes&quot; ~ &quot;YesMUT&quot;)) %&gt;% 
  group_by(NewGroups) %&gt;%
  summarise(varPerGroup = var(Response)) %&gt;% 
  ungroup %&gt;% 
  mutate(sigma = sqrt(mean(varPerGroup))))</code></pre>
<pre><code>## # A tibble: 3 x 3
##   NewGroups varPerGroup sigma
##   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;
## 1 NoMUT            2.05  1.54
## 2 WT               3.01  1.54
## 3 YesMUT           2.04  1.54</code></pre>
<p>Observe that the wildtype standard deviation is larger than that for the other groups. The average is not equal to the residual standard error. It can be shown mathematically that in this case the residual standard error can be estimated approximately as</p>
<pre class="r"><code>sqrt((1/4)*(dfTemp$varPerGroup[1] + dfTemp$varPerGroup[3] + 2*dfTemp$varPerGroup[2]))</code></pre>
<pre><code>## [1] 1.589483</code></pre>
<p>An important point is that for the present data, this model is not homoscedastic, which is one of the assumptions underlying inferential statistics using linear models. To move forward we will generate a new data set in which there is no main anxiety effect, only an interaction. This makes it so that the group standard deviations will be approximately the same and put us back in the regime of homoscedasticity. Thus even though the wildtype group will have double the number of observations, the residual standard error will be approximately the average of the group standard deviations. We will ignore the presence of <code>Treatment</code>.</p>
<pre class="r"><code>meanRef &lt;- 0
sigma &lt;- 1
effectGenotype &lt;- 2
effectAnxiety &lt;- 2

dfSimple &lt;- dfSimple %&gt;%
  mutate(Response = case_when(Genotype == &quot;WT&quot; ~ rnorm(nrow(.),meanRef,sigma),
                              Genotype == &quot;MUT&quot; &amp; Anxiety == &quot;No&quot; ~ rnorm(nrow(.),meanRef + effectGenotype, sigma),
                              Genotype == &quot;MUT&quot; &amp; Anxiety == &quot;Yes&quot; ~ rnorm(nrow(.), meanRef + effectGenotype + effectAnxiety, sigma)))


ggplot(dfSimple, aes(x = Genotype, y = Response, col = Anxiety)) + 
  geom_jitter(width = 0.2) +
  scale_color_brewer(palette = &quot;Set1&quot;)</code></pre>
<p><img src="/blog/post/2018-07-06_LinearModelsErrors_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Re-running the model on this new data, we find:</p>
<pre class="r"><code>dfSimple &lt;- dfSimple %&gt;% mutate(GenotypeDummy = case_when(Genotype == &quot;WT&quot; ~ 0,
                                                          Genotype == &quot;MUT&quot; ~ 1))
linModInt2 &lt;- lm(Response ~ GenotypeDummy + GenotypeDummy:Anxiety, data = dfSimple)
summary(linModInt2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Response ~ GenotypeDummy + GenotypeDummy:Anxiety, 
##     data = dfSimple)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.96592 -0.65534  0.00185  0.65837  3.05810 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)              -0.02255    0.04830  -0.467    0.641    
## GenotypeDummy             2.00184    0.08366  23.927   &lt;2e-16 ***
## GenotypeDummy:AnxietyYes  1.98251    0.09661  20.522   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9661 on 797 degrees of freedom
## Multiple R-squared:  0.746,  Adjusted R-squared:  0.7454 
## F-statistic:  1170 on 2 and 797 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Note that the parameter estimates recapitulate what we put into the model. Moreover the residual standard error is now approximately equal to the input value of <span class="math inline">\(\sigma\)</span>. We can compute the group standard deviations to see how this relates to the residual standard error:</p>
<pre class="r"><code>dfSimple %&gt;% 
  mutate(NewGroups = case_when(Genotype == &quot;WT&quot; ~ &quot;WT&quot;,
                               Genotype == &quot;MUT&quot; &amp; Anxiety == &quot;No&quot; ~ &quot;NoMUT&quot;,
                               Genotype == &quot;MUT&quot; &amp; Anxiety == &quot;Yes&quot; ~ &quot;YesMUT&quot;)) %&gt;% 
  group_by(NewGroups) %&gt;%
  summarise(varPerGroup = var(Response)) %&gt;% 
  ungroup %&gt;% 
  mutate(sigma = sqrt(mean(varPerGroup)))</code></pre>
<pre><code>## # A tibble: 3 x 3
##   NewGroups varPerGroup sigma
##   &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;
## 1 NoMUT           0.846 0.963
## 2 WT              0.953 0.963
## 3 YesMUT          0.980 0.963</code></pre>
<p>In this case the average is closer to the residual standard error estimate.</p>
<p>Next we examine the mapping from <span class="math inline">\(\sigma\)</span> to <span class="math inline">\(\sigma_\beta\)</span> to see how it compares to the model with a main anxiety effect.</p>
<pre class="r"><code>xMat &lt;- model.matrix(linModInt2)
t(xMat) %*% xMat</code></pre>
<pre><code>##                          (Intercept) GenotypeDummy
## (Intercept)                      800           400
## GenotypeDummy                    400           400
## GenotypeDummy:AnxietyYes         200           200
##                          GenotypeDummy:AnxietyYes
## (Intercept)                                   200
## GenotypeDummy                                 200
## GenotypeDummy:AnxietyYes                      200</code></pre>
<p><span class="math display">\[ n \cdot \begin{bmatrix} 1 &amp; \frac{1}{2} &amp; \frac{1}{4} \\ \frac{1}{2} &amp; \frac{1}{2} &amp; \frac{1}{4} \\ \frac{1}{4} &amp; \frac{1}{4} &amp; \frac{1}{4} \end{bmatrix}\]</span></p>
<p>The covariance matrix is</p>
<p><span class="math display">\[\frac{\sigma^2}{n}\cdot\begin{bmatrix} 2 &amp; -2 &amp; 0 \\ -2 &amp; 6 &amp; -4 \\ 0 &amp; -4 &amp; 8 \end{bmatrix}\]</span></p>
<p>which leads to standard errors of</p>
<p><span class="math display">\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{2} &amp; \sqrt{6} &amp; \sqrt{8} \end{bmatrix}\]</span></p>
<p>Now, comparing this model to the previous model with both main effects:</p>
<pre class="r"><code>data.frame(Intercept = c(sqrt(4/nObs),sqrt(2/nObs)),
           Genotype = c(sqrt(8/nObs), sqrt(6/nObs)),
           Anxiety = c(sqrt(8/nObs), NA),
           GenotypeAnxiety = c(sqrt(16/nObs), sqrt(8/nObs)), 
           row.names = c(&quot;With Main Effect&quot;, &quot;Without Main Effect&quot;))</code></pre>
<pre><code>##                      Intercept   Genotype Anxiety GenotypeAnxiety
## With Main Effect    0.07071068 0.10000000     0.1       0.1414214
## Without Main Effect 0.05000000 0.08660254      NA       0.1000000</code></pre>
<p>Using the number of observations <span class="math inline">\(n\)</span> we find:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Intercept</th>
<th align="left">Genotype</th>
<th align="left">Anxiety</th>
<th align="left">GenotypeAnxiety</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">With Main Effect</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{8}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{8}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{16}{n}}\)</span></td>
</tr>
<tr class="even">
<td align="left">Without Main Effect</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{2}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{6}{n}}\)</span></td>
<td align="left"></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{8}{n}}\)</span></td>
</tr>
</tbody>
</table>
<p>The patterns from the previous Sections break down in this case. Notably the conversion factor for the intercept term is no longer related to the number of parameters in the model. The standard errors for both the main effect and interaction term are also smaller in this model compared to the model with both main effects, assuming a fixed value of <span class="math inline">\(\sigma\)</span>. This does require caution however, as we saw that the residual standard error may be larger for this model if there is a actually a main effect in the data.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>In conclusion, we recapitulate the <span class="math inline">\(\sigma\)</span>-to-<span class="math inline">\(\sigma_\beta\)</span> mappings for the different models that we considered:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Intercept</th>
<th align="left">Genotype</th>
<th align="left">Anxiety</th>
<th align="left">Treatment</th>
<th align="left">GenotypeAnxiety</th>
<th align="right">NumGroups</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Intercept Only</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{1}{n}}\)</span></td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="right">1</td>
</tr>
<tr class="even">
<td align="left">One Binary Variable</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{2}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="right">2</td>
</tr>
<tr class="odd">
<td align="left">Two Binary Variables</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{3}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="left">Three Binary Variables</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left">NA</td>
<td align="right">8</td>
</tr>
<tr class="odd">
<td align="left">Interaction With Main Effect</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{4}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{8}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{8}{n}}\)</span></td>
<td align="left">NA</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{16}{n}}\)</span></td>
<td align="right">4</td>
</tr>
<tr class="even">
<td align="left">Interaction Without Main Effect</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{2}{n}}\)</span></td>
<td align="left"><span class="math inline">\(\sqrt{\frac{6}{n}}\)</span></td>
<td align="left">NA</td>
<td align="left">NA</td>
<td align="left"><span class="math inline">\(\sqrt{\frac{8}{n}}\)</span></td>
<td align="right">3</td>
</tr>
</tbody>
</table>
<p>There are a few things to keep in mind. First, <strong>the standard deviation of the different groups is captured in the residual standard error estimate</strong>. Specifically if <span class="math inline">\(n \gg p\)</span>, this estimate is approximately equal to the average of the group sample standard deviations.</p>
<p>There is no obvious direct relationship between the standard errors of the parameters and the group standard deviations. For instance, the parameter error for the intercept is not equal to the standard error of the reference group, nor is the parameter error for the slope equal to the standard error of the non-reference group. The parameter errors depend on the non-trivial mapping <span class="math inline">\((\mathbf{X&#39;X})^{-1}\)</span>.</p>
<p>There are however some patterns in the relationship for certain models. Specifically, for balanced binary variable models without interaction, the slope parameter standard errors are always related to the residual standard error by <span class="math inline">\(\sqrt{4/n}\)</span>, regardless of the number of binary variables in the model. The parameter error for the intercept does change however, and <strong>scales with the square root of the number of parameters in the model</strong>.</p>
<p>The patterns change when we add an interaction to the model. Comparing a two-variable model with an interaction to the corresponding model without an interaction, the parameters have larger errors in the interactive model. The interaction parameter is also the most uncertain parameter in the model. However the intercept parameter error still has a conversion factor related to the number of parameters in the model. If we remove one of the main effects from the model but maintain the interaction, all conversion factors shrink relative to the interactive model with the main effect. However this model should be used with caution as it will likely lead to grouping with uneven variances. On the other hand it can be a useful way to model data if one of the variables is not defined for one of the levels in the main effect, e.g. wildtypes without anxiety scores.</p>
<p>More complex interactive models were not explored in depth in this document, but for completion I will include the <span class="math inline">\(\sigma\)</span>-to-<span class="math inline">\(\sigma_\beta\)</span> mappings for two models. The two-variable interaction model described previously can be augmented to include a third variable. The complete interactive model at second order is as follows:</p>
<p><span class="math display">\[\text{Response} \sim \text{Genotype} + \text{Anxiety} + \text{Treatment} + \text{Genotype:Anxiety} + \text{Genotype:Treatment} + \text{Treatment:Anxiety}\]</span></p>
<p>The mapping for this model is:</p>
<p><span class="math display">\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{7} &amp; \sqrt{12} &amp; \sqrt{12} &amp; \sqrt{12}&amp; \sqrt{16} &amp; \sqrt{16} &amp; \sqrt{16} \end{bmatrix}\]</span></p>
<p>The interactive model at the third order is:</p>
<p><span class="math display">\[\text{Response} \sim \text{Genotype} + \text{Anxiety} + \text{Treatment} + \text{Genotype:Anxiety} + \text{Genotype:Treatment} + \text{Treatment:Anxiety} + \text{Genotype:Anxiety:Treatment}\]</span></p>
<p>The mapping for this model is:</p>
<p><span class="math display">\[\sigma_\beta = \frac{\sigma}{\sqrt{n}}\begin{bmatrix} \sqrt{8} &amp; \sqrt{16} &amp; \sqrt{16} &amp; \sqrt{16}&amp; \sqrt{32} &amp; \sqrt{32} &amp; \sqrt{32} &amp; \sqrt{64} \end{bmatrix}\]</span></p>
<p>The one thing I will mention about these mappings is that the conversion factors for the intercept standard errors continue to be related to the number of parameters in the model. There are likely other interesting patterns in these more complex interactive models, but these will not be explored here.</p>
<p>Ultimately these specific cases should serve to provide some intuition about how the parameter errors are estimated for a linear model. Keep in mind however that these mappings were computed for a balanced binary experimental design. Group imbalances will skew these values, though the size of these differences will depend on the degree of imbalance. Moreover the mappings will be different in the case of multi-level categorical variables and continuous numerical variables.</p>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>/blog/post/linearmodels/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/linearmodels/</guid>
      <description><![CDATA[
      <div id="S1" class="section level1">
<h1>Preamble</h1>
<p>The purpose of this post is to elucidate some of the concepts associated with statistical linear models.</p>
<p>Let’s start by loading some libraries.</p>
<pre class="r"><code>library(ggplot2)
library(datasets)</code></pre>
</div>
<div id="S2" class="section level1">
<h1>Background Theory</h1>
<p>The basic idea is as follows:</p>
<p>Given two variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, for which we’ve measured a set of data points <span class="math inline">\(\{x_i, y_i\}\)</span> with <span class="math inline">\(i = 1, ..., n\)</span>, we want to estimate a function, <span class="math inline">\(f(x)\)</span>, such that</p>
<p><span class="math display">\[y_i = f(x_i) + \epsilon_i\]</span></p>
<p>for each data point <span class="math inline">\((x_i,y_i)\)</span>. Here <span class="math inline">\(\epsilon_i\)</span> is the error in data point <span class="math inline">\(y_i\)</span> compared to the predicted value <span class="math inline">\(f(x_i)\)</span>. Specifically,</p>
<p><span class="math display">\[\epsilon_i = y_i - f(x_i)\]</span></p>
<p>We don’t know <span class="math inline">\(f(x)\)</span>, but the simplest functional form that we can assume is that of a linear function:</p>
<p><span class="math display">\[f(x) = \beta_0 + \beta_1x \]</span> where <span class="math inline">\(\beta_0\)</span> is the intercept of the line and <span class="math inline">\(\beta_1\)</span> is the slope associated with the variable <span class="math inline">\(x\)</span>. Thus for each data point <span class="math inline">\(x_i\)</span>, we predict a value <span class="math inline">\(\hat{y}_i = f(x_i)\)</span> so that</p>
<p><span class="math display">\[\hat{y}_i = \beta_0 + \beta_1x_i\]</span> This is known as <strong>simple linear regression</strong>. Having specified the form of <span class="math inline">\(f(x)\)</span>, the problem becomes one of optimizing the free parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> so that the predicted or trend line best describes the real data <span class="math inline">\(\{x_i, y_i\}\)</span>. This is usually accomplished using the “ordinary least-squares” method, in which we minimize the sum of squared errors with respect to the free parameters. Explicitly, if we write the sum of squared errors as</p>
<p><span class="math display">\[\chi^2 = \sum_{i=1}^n{\epsilon_i^2} = \sum_{i=1}^n{(y_i - \hat{y}_i)^2} = \sum_{i=1}^n{(y_i - \beta_0 - \beta_1x_i)^2}\]</span> we want to determine <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that</p>
<p><span class="math display">\[\frac{\partial\chi^2}{\partial\beta_0} = 0\]</span> and <span class="math display">\[\frac{\partial\chi^2}{\partial\beta_1} = 0\]</span></p>
<p>This can be solved analytically. In practice we let the computer do it for us.</p>
<p>Moving on , there’s no reason we need to restrict ourselves to one predictor for the response variable <span class="math inline">\(y\)</span>, so we can include multiple variables <span class="math inline">\(\{x_1, x_2, ..., x_N\}\)</span> in our model. Here <span class="math inline">\(N\)</span> is the total number of regressors that are included. This is known as <strong>multiple linear regression</strong>. The model then becomes</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \sum_{a=1}^N{} \beta_ax_a\]</span> where <span class="math inline">\(a\)</span> is an index summing over the predictors (rather than over the data points themselves as in the expression for <span class="math inline">\(\chi^2\)</span> above). Here I’ve expressed the model in terms of the variables, rather than individual data points. For an individual data point <span class="math inline">\(\{x_i,y_i\}\)</span>, we could write this as</p>
<p><span class="math display">\[\hat{y}_i = \beta_0 + \sum_{a=1}^N{} \beta_ax_{a,i}\]</span> where <span class="math inline">\(x_{a,i}\)</span> is the <span class="math inline">\(i\)</span>th data point of the <span class="math inline">\(a\)</span>th variable (e.g. the height, which is the variable, of a specific person). The two are identical representations. The optimization process for multiple linear regression is the same as that for simple linear regression, only involving more derivatives.</p>
<p>Alright that should be enough background math. In the next Section, we’ll look at this in practice.</p>
</div>
<div id="S3" class="section level1">
<h1>Simple Linear Regression in Practice</h1>
<p>We’ll be working with the <code>mtcars</code> dataset.</p>
<pre class="r"><code>str(mtcars)</code></pre>
<pre><code>## &#39;data.frame&#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</code></pre>
<p>The description of these variables can be found <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html">here</a>. We’re going to start by looking at the relationship between two continuous variables. Specifically, I’ve chosen to examine the relationship between car weight, <code>wt</code>, and fuel efficiency, <code>mpg</code>. Let’s start by creating a scatter plot to look at how the fuel efficiency varies with car weight.</p>
<pre class="r"><code>p.mpg_vs_wt &lt;- ggplot(mtcars, aes(x=wt,y=mpg)) + geom_point()
p.mpg_vs_wt + 
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Scatter Plot of MPG vs. Weight&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>This is as we would expect, since less energy is needed to move lighter cars. Moreover we suspect that this data might be well suited to a simple linear model. As described above, the model we will be building is</p>
<p><span class="math display">\[y = \beta_0 + \beta_1x + \epsilon\]</span></p>
<p>or</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the <code>mpg</code> variable in this case and <span class="math inline">\(x\)</span> is the <code>wt</code> variable. In <code>R</code> formula notation, we can express this as <code>mpg ~ wt</code>, where <code>~</code> means “is modelled by”. The way to build a linear model in <code>R</code> is using the <code>lm()</code> function, as follows:</p>
<pre class="r"><code>mpg_vs_wt &lt;- summary(lm(mpg ~ wt,data=mtcars))
print(mpg_vs_wt)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5432 -2.3647 -0.1252  1.4096  6.8727 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***
## wt           -5.3445     0.5591  -9.559 1.29e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.046 on 30 degrees of freedom
## Multiple R-squared:  0.7528, Adjusted R-squared:  0.7446 
## F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10</code></pre>
<p>The focus of this document will be on interpreting the <code>Estimate</code> column of the <code>Coefficients</code> table. Let’s pull this table from the output:</p>
<pre class="r"><code>print(mpg_vs_wt$coefficients)</code></pre>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 37.285126   1.877627 19.857575 8.241799e-19
## wt          -5.344472   0.559101 -9.559044 1.293959e-10</code></pre>
<p>This is fairly straightforward to interpret. Looking at the <code>Estimate</code> column, the <code>(Intercept)</code> value describes the value of <span class="math inline">\(\beta_0\)</span> in our model, while the <code>wt</code> value describes the slope associated with the <code>wt</code> variable, i.e. <span class="math inline">\(\beta_1\)</span>. Our fitted model looks like this:</p>
<p><span class="math display">\[\hat{y} = 37.3- 5.3x\]</span> where <span class="math inline">\(x\)</span> is the <code>wt</code> variable and <span class="math inline">\(y\)</span> is the <code>mpg</code> variable, as mentioned above.</p>
<p>Let’s see how this looks.</p>
<pre class="r"><code>p.mpg_vs_wt + 
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Simple Linear Model: MPG vs. Weight&quot;) + 
  geom_abline(intercept = mpg_vs_wt$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;], 
              slope = mpg_vs_wt$coefficients[&quot;wt&quot;, &quot;Estimate&quot;]) + 
  coord_cartesian(xlim=c(0,6),ylim = c(5,40))</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>That looks pretty good. A second-order polynomial might better capture the data at the lower and higher <code>wt</code> values, but we won’t go into that here.</p>
</div>
<div id="S4" class="section level1">
<h1>Simple Linear Regression with Categorical Variables</h1>
<p>In the previous Section, we looked at how simple linear regression works when the predictor is a continuous variable, like <code>wt</code>. Here we will examine what happens when we model categorical variables. Recall that a categorical variable is a variable that takes on discrete, usually non-numerical, values. For example, sex is a categorical variable, with the values being male or female.</p>
<p>In the <code>mtcars</code> dataset, we’ll look at the <code>am</code> variable, which describes whether the car has manual or automatic transmission. Let’s explicitly express this as a factor and display the unique values.</p>
<pre class="r"><code>mtcars$am &lt;- as.factor(mtcars$am)
unique(mtcars$am)</code></pre>
<pre><code>## [1] 1 0
## Levels: 0 1</code></pre>
<p>So <code>am</code> has two possible values. Manual transmission is encoded as <code>am = 1</code> while automatic transmission is encoded as <code>am = 0</code> (refer to the <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html">dataset description</a>). As we did above, we can examine how <code>wt</code> varies with the <code>am</code> variable. Again we are fitting a model</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1A\]</span> where <span class="math inline">\(y\)</span> is the <code>wt</code> variable and <span class="math inline">\(A\)</span> is the <code>am</code> variable. Keep in mind that <span class="math inline">\(A\)</span> is binary. In <code>R</code>:</p>
<pre class="r"><code>mpg_vs_am &lt;- summary(lm(mpg ~ am, data=mtcars ))
print(mpg_vs_am$coefficients)</code></pre>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 17.147368   1.124603 15.247492 1.133983e-15
## am1          7.244939   1.764422  4.106127 2.850207e-04</code></pre>
<p>Thus we see that <span class="math inline">\(\beta_0\)</span> = 17.1 and <span class="math inline">\(\beta_1\)</span> = 7.2. Note here that in the <code>Coefficients</code> table, the slope estimate is associated with the label <code>am1</code>. This means that it displays the slope going from <code>am = 0</code> to <code>am = 1</code>. This is perhaps expected in this case since 1 is greater than 0, but in general categorical variables will not have numerical values. Consider again a variable describing sex. The two instances are “Male” and “Female”. There is no specific order in which to compute the slope. When building a model with categorical variables such as these, <code>R</code> will implicitly assign values of 0 and 1 to the levels of the variable. By default, <code>R</code>, assigns the <strong>reference level</strong>, i.e. a value of 0, to the value that is <strong>lowest in alphabetical order</strong>. For the sex variable, “Female” would be associated with 0, and “Male” with 1. So when running <code>lm()</code> on such a model and printing the output, the <code>Coefficients</code> table will have a row with a name like <code>sexMale</code>. The <code>Estimate</code> value associated with this describes the slope of the model going from <code>sex=Female</code> (which is implicitly defined as 0) to <code>sex=Male</code> (which is implicitly defined as 1). This might seem unnecessary right now, but it becomes important when trying to interpret the output from more complex models, as we’ll see below.</p>
<p>As above, we can plot this data and model.</p>
<pre class="r"><code>ggplot(mtcars, aes(x=am,y=mpg)) + 
  geom_jitter(width=0.1) + 
  geom_smooth(method=&quot;lm&quot;, formula = y~x, aes(group=1), se=F) + 
  labs(x = &quot;Transmission (0 = automatic)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Simple Linear Model: MPG vs. Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="S5" class="section level1">
<h1>Multiple Linear Regression in Practice</h1>
<p>In this Section we will combine the models built in the two previous Sections using multiple linear regression. Specifically, we will model <code>mpg</code> by <code>wt</code> and <code>am</code> together.</p>
<p>Let’s start by taking a look at the data.</p>
<pre class="r"><code>ggplot(mtcars, aes(x=wt, y=mpg, col=am)) + 
  geom_point() +  
   labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Fuel Efficiency, Weight, and Transmission&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Again, I’ve plotted <code>mpg</code> against <code>wt</code> using a scatter plot, but I’ve mapped the colour aesthetic to the <code>am</code> variable to see the group variation.</p>
<p>Mathematically, the model we will use looks like</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2A\]</span> where, as above, <span class="math inline">\(y\)</span> is the <code>mpg</code> variable, <span class="math inline">\(x\)</span> is the <code>wt</code> continuous variable, and <code>A</code> is the categorical <code>am</code> variable. <span class="math inline">\(\beta_1\)</span> is the slope associated with <span class="math inline">\(x\)</span> and <span class="math inline">\(\beta_2\)</span> is the slope associated with <span class="math inline">\(A\)</span>. Recall that <span class="math inline">\(A\)</span> is only 0 or 1.</p>
<p>Let’s go ahead and build the model in <code>R</code>.</p>
<pre class="r"><code>mpg_MLR &lt;- summary(lm(mpg ~ wt + am, data=mtcars))
print(mpg_MLR$coefficients)</code></pre>
<pre><code>##                Estimate Std. Error     t value     Pr(&gt;|t|)
## (Intercept) 37.32155131  3.0546385 12.21799285 5.843477e-13
## wt          -5.35281145  0.7882438 -6.79080719 1.867415e-07
## am1         -0.02361522  1.5456453 -0.01527855 9.879146e-01</code></pre>
<p>Again, keeping in mind the mathematical formula, we see that <code>(Intercept)</code> corresponds to <span class="math inline">\(\beta_0\)</span> = 37.32, <code>wt</code> corresponds to <span class="math inline">\(\beta_1\)</span> = -5.35 and <code>am1</code> corresponds to <span class="math inline">\(\beta_2\)</span> = -0.02. So how do we interpret this? How could we sketch this model on a scatter plot of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> like the one above? The way to go about it is to examine the two cases for our categorical variables <span class="math inline">\(A\)</span> = <code>am</code>. Recall that, regardless of what our factor levels are (0/1, Female/Male), <code>R</code> always encodes categorical variables as 0 and 1. Consequently we can always examine our mathematical model by setting the corresponding variable to 0 or 1. For <span class="math inline">\(A\)</span> = <code>am</code> = 0, we have</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span> In this case, when <span class="math inline">\(A\)</span> = 0, <span class="math inline">\(\beta_0\)</span> is the intercept of the line relating <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, and <span class="math inline">\(\beta_1\)</span> is the slope associated with <span class="math inline">\(x\)</span>. On the scatter plot, we would draw a line with this slope and intercept. What about when <span class="math inline">\(A = 1\)</span>?</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2\]</span> <span class="math display">\[= (\beta_0 + \beta_2) + \beta_1x\]</span> <span class="math display">\[= \beta_0&#39; + \beta_1x\]</span></p>
<p>We find that we actually have a new intercept value <span class="math inline">\(\beta_0&#39; = \beta_0 + \beta_2\)</span>, but the same slope <span class="math inline">\(\beta_1\)</span>. Thus the trend line associated with <span class="math inline">\(A = 1\)</span> has a different intercept than that associated with <span class="math inline">\(A = 0\)</span>. What we’ve discovered is that the <span class="math inline">\(\beta_2\)</span> parameter tells us the <strong>difference in the intercept values</strong> between the <code>am = 0</code> and <code>am = 1</code> groups, i.e. <span class="math inline">\(\beta_2 = \beta_0&#39; - \beta_0\)</span>. The <span class="math inline">\(\beta_1\)</span> parameter tells us the slope of the two lines.</p>
<p>Let’s see what this looks like.</p>
<pre class="r"><code>beta0 &lt;- mpg_MLR$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;]
beta1 &lt;- mpg_MLR$coefficients[&quot;wt&quot;,&quot;Estimate&quot;]
beta0_prime &lt;- mpg_MLR$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;] + 
                mpg_MLR$coefficients[&quot;am1&quot;,&quot;Estimate&quot;]

ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&quot;red&quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1, 
              col=&quot;blue&quot;,
              alpha=0.5) + 
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Multiple Linear Model: MPG ~ Weight + Transmission&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>There are actually two lines in this plot, a blue one and a red one. Given how small <span class="math inline">\(\beta_2\)</span> is we can’t see much of a difference, but the blue trend line should be slightly lower than the red line. Their slopes are the same. Let’s zoom in to be sure.</p>
<pre class="r"><code>ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&quot;red&quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1, 
              col=&quot;blue&quot;,
              alpha=0.5) + 
  coord_cartesian(xlim = c(3.4,3.5),ylim = c(17.5,19.5)) + 
    labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Multiple Linear Model: MPG ~ Weight + Transmission (Zoomed In)&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>There we go. This is the effect of a multiple linear model with no interactions. In general the trend lines for the two groups don’t have to be so close together. It all depends on the data. In this case we see that, when we impose a fixed slope (using <code>mpg ~ wt + am</code>), the trend lines describing the two groups are basically the same. Next, we’ll look at how adding interactions to our model results in slope variation.</p>
</div>
<div id="S6" class="section level1">
<h1>Multiple Linear Regression with Interactions</h1>
<p>In the previous Section we examined the use of multiple linear regression to model a response variable in terms of continuous and categorical predictors. We can take this a step further by including an <strong>interaction</strong> in our model. What does this mean? An interaction describes how a change in one predictor influences change in another predictor. In mathematics this is typically expressed by including a product or more complex term in an equation. A simple product of two variables is the simplest interaction term that we can write down. Consider once again our model of <code>mpg</code> vs. <code>wt</code> and <code>am</code>. This time we will add an interaction:</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2A + \beta_3xA\]</span></p>
<p>The model is as above, with the exception of the new interaction term <span class="math inline">\(\beta_3xA\)</span>. <span class="math inline">\(xA\)</span> is the product interaction while <span class="math inline">\(\beta_3\)</span> is the model parameter associated with this interaction.</p>
<p><strong>An important note</strong>: In <code>R</code> formula notation, interactions are expressed as <code>wt*am</code>. This includes all terms in the model, i.e. <code>wt*am</code> = <code>1 + wt + am + wt:am</code> where <code>1</code> stands in for the “variable” associated with the <span class="math inline">\(\beta_0\)</span> parameter, and the <strong>colon denotes a product</strong>. We can thus write our full interactive model as <code>mpg ~ wt*am</code>.</p>
<p>Let’s run this model through <code>lm()</code> and see what happens.</p>
<pre class="r"><code>mpg_int &lt;- summary(lm(mpg ~ wt*am,data=mtcars))
print(mpg_int$coefficients)</code></pre>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 31.416055  3.0201093 10.402291 4.001043e-11
## wt          -3.785908  0.7856478 -4.818836 4.551182e-05
## am1         14.878423  4.2640422  3.489277 1.621034e-03
## wt:am1      -5.298360  1.4446993 -3.667449 1.017148e-03</code></pre>
<p>We have four rows for our four parameters. Again each row corresponds to one <span class="math inline">\(\beta\)</span> parameter: <code>(Intercept)</code> corresponds to <span class="math inline">\(\beta_0\)</span> = 31.42, <code>wt</code> corresponds to <span class="math inline">\(\beta_1\)</span> = -3.79, <code>am1</code> corresponds to <span class="math inline">\(\beta_2\)</span> = 14.88 and <code>wt:am1</code> corresponds to <span class="math inline">\(\beta_3\)</span> = -5.3. Let’s interpret this, as we did in the previous Section, by examining the different values of the categorical variable <code>am</code>.</p>
<p>When <span class="math inline">\(A\)</span> = <code>am</code> = 0, we have</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span> which is just our simple model. So <span class="math inline">\(\beta_0\)</span> describes the intercept when <span class="math inline">\(A = 0\)</span> and <span class="math inline">\(\beta_1\)</span> describes the slope associated with <span class="math inline">\(x\)</span> when <span class="math inline">\(A = 0\)</span>. What happens when <span class="math inline">\(A = 1\)</span>?</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2 + \beta_3x\]</span> <span class="math display">\[= (\beta_0 + \beta_2) + (\beta_1 + \beta_3)x\]</span> <span class="math display">\[= \beta_0&#39; + \beta_1&#39;x\]</span></p>
<p>So we actually have a new intercept and a new slope! The new intercept is the same as in the previous Section: <span class="math inline">\(\beta_0&#39; = \beta_0 + \beta_2\)</span>. The new slope is <span class="math inline">\(\beta_1&#39; = \beta_1 + \beta_3\)</span>. Therefore, a simple product interaction like this one causes the slope of <span class="math inline">\(y\)</span> with <span class="math inline">\(x\)</span> to change as we move from <span class="math inline">\(A = 0\)</span> to <span class="math inline">\(A = 1\)</span>. Displaying this on a scatter plot of <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>, we would have two lines with different intercepts and different slopes.</p>
<p>Let’s take a look.</p>
<pre class="r"><code>beta0 = mpg_int$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;]
beta1 = mpg_int$coefficients[&quot;wt&quot;,&quot;Estimate&quot;]
beta0_prime = mpg_int$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;] + 
                mpg_int$coefficients[&quot;am1&quot;,&quot;Estimate&quot;]
beta1_prime = mpg_int$coefficients[&quot;wt&quot;,&quot;Estimate&quot;] + 
                mpg_int$coefficients[&quot;wt:am1&quot;,&quot;Estimate&quot;]

ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&quot;red&quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1_prime, 
              col=&quot;blue&quot;,
              alpha=0.5) + 
  coord_cartesian(xlim=c(0,6),
                  ylim = c(0,50)) +
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Weight-Transmission Interaction&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Perfect. Compared to the non-interactive model in the previous Section, we see that adding an interaction (and thus allowing the slope of <span class="math inline">\(y\)</span> with <span class="math inline">\(x\)</span> to vary between groups) better characterizes the data. If the data was truly not well characterized by an interaction, the equal-slope model of the previous Section would perform just as well as an interactive model of this kind. Of course this would have to be determined by examining the statistics associated with the models.</p>
<p>This concludes the majority of what I wanted to cover. In the next Section I’ll go into some of the heavier mathematical details regarding linear models. Read ahead at your own peril.</p>
</div>
<div id="S7" class="section level1">
<h1>Mathematical Embellishments</h1>
<p>The previous analysis was focussed on examining linear models built with a continuous and categorical predictor. We’re by no means restricted to this however. We can build a linear model with as many variables as we’d like. As we saw above, the case of one continuous predictor and one categorical predictor can be visualized fairly easily using a scatter plot, where the continous data is mapped to the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axes and the categorical data is mapped to a colour/shape/style aesthetic. This becomes harder to do when we’re examining models that use multiple continuous predictors, e.g. something of the form</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2\]</span></p>
<p>Here we have an interactive multiple linear regression with two continuous regressors <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. A practical example of this would be modelling the <code>mpg</code> variable in terms of the <code>wt</code> and horsepower, <code>hp</code>. Let’s plot this in the same way we did above, where the <code>hp</code> variable is mapped to the colour aesthetic.</p>
<pre class="r"><code>ggplot(mtcars, aes(x=wt, y=mpg, col=hp)) + 
  geom_point() +  
   labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Fuel Efficiency, Weight, and Horsepower&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Given the continuous nature of the <code>hp</code> variable, this is difficult to interpret by eye. That doesn’t invalidate the model however, and we can still estimate the optimal <span class="math inline">\(\beta\)</span> parameters.</p>
<pre class="r"><code>mpg_hp &lt;- summary(lm(mpg ~ wt*hp, data=mtcars))
print(mpg_hp$coefficients)</code></pre>
<pre><code>##                Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 49.80842343 3.60515580 13.815887 5.005761e-14
## wt          -8.21662430 1.26970814 -6.471270 5.199287e-07
## hp          -0.12010209 0.02469835 -4.862758 4.036243e-05
## wt:hp        0.02784815 0.00741958  3.753332 8.108307e-04</code></pre>
<p>The rows in this table still represent the <span class="math inline">\(\beta\)</span> parameters in the model, as before, but it is much harder to interpret this result in the context of a scatter plot of <code>mpg</code> vs. <code>wt</code>. We can’t just set <code>hp = 0</code> and <code>hp = 1</code> as we did previously. The equivalent here would be setting <code>hp</code> to an infinite number of incremental values. This isn’t the right way to think about this. This sort of brings us face to face with what the multiple linear model is actually saying. Consider again simple linear regression:</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span> This is clearly the expression for a line. The variable <span class="math inline">\(y\)</span> is modelled as a linear function of <span class="math inline">\(x\)</span>. As we know, we can plot this easily on a two-dimensional space, where <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> form the axes. Moving to multiple regression, we have</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2\]</span></p>
<p>In Sections <a href="#S5">5</a> and <a href="#S6">6</a>, we interpreted this model <strong>within the context of the scatter plot of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y\)</span></strong>. With <span class="math inline">\(x_2\)</span> as a categorical variable, this allowed us to interpret <span class="math inline">\(\beta_2\)</span> as the difference in intercept values on this scatter plot. Mathematically, however, this expression describes a <strong>two-dimensional plane</strong> characterised by the variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. We see that the intercept of the plane is <span class="math inline">\(\beta_0\)</span>, i.e. the value of <span class="math inline">\(y\)</span> at which both the parametric variables are 0. Moreover, <span class="math inline">\(\beta_1\)</span> is the slope of the plane with respect to <span class="math inline">\(x_1\)</span>, and <span class="math inline">\(\beta_2\)</span> is the slope of the plane with respect to <span class="math inline">\(x_2\)</span>. Specifically,</p>
<p><span class="math display">\[\beta_1 = \frac{\partial\hat{y}}{\partial x_1}\]</span></p>
<p>and</p>
<p><span class="math display">\[\beta_2 = \frac{\partial\hat{y}}{\partial x_2}\]</span> We can visualize this two-dimensional plane in a three-dimensional space, where the third axis is represented by the <span class="math inline">\(y\)</span> variable. You can do this easily by grabbing a piece of paper, or preferably a rigid flat object, and orienting it in front of you in real space. You can imagine the <span class="math inline">\(y\)</span> axis as the vertical axis, and the <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> axes as the horizontal axes. The idea of a flat plane in a multi-dimensional space extends to any number of predictors in our model, provided that the model is non-interactive. Given <span class="math inline">\(N\)</span> predictors <span class="math inline">\(\{x_1, x_2, ..., x_N\}\)</span>, a model of the form</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \sum_{a=1}^N{\beta_ax_a}\]</span></p>
<p>describes a N-dimensional hyperplane embedded in a (N+1)-dimensional space. Crazy. This is undoubtedly true of continuous variables, but is a little bit more nuanced for a categorical variable. Going back to our model of <code>mpg</code> vs. <code>wt</code> and <code>am</code>, we can still imagine this in a three-dimensional space. The axes of the space are <code>mpg</code>,<code>wt</code> and <code>am</code>, but notice that we aren’t actually dealing with a plane in this case, since <code>am</code> only takes on binary values. Rather we are dealing with two different lines embedded in this three-dimensional space. One line will occur at <code>am = 0</code>, while the other occurs at <code>am = 1</code>. Given this context, we can re-interpret the scatter plots from Section <a href="#S5">5</a> and <a href="#S6">6</a>. Here is the scatter plot and model from Section <a href="$S6">6</a>:</p>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Within the proper context of a three-dimensional space, this two-dimensional plot is actually the <strong>projection of the binary <code>am</code> axis onto <code>am = 0</code></strong>. Imagine the red line existing in your computer screen and the blue line actually existing outside of your computer screen, closer to you. I’ve used the interactive model here since the plot is nicer than that for the non-interactive model, but this leads us into a discussion of interactions with continuous variables.</p>
<p>Examining the plot above, you might already guess where this is going. Let’s consider a model with a simple product interaction of two continuous variables, like the one at the beginning of this Section:</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2\]</span></p>
<p>We saw that when <span class="math inline">\(\beta_3 = 0\)</span>, the model describes a two-dimensional plane in three-dimensions. What does the interaction do to this plane? Notice that the product interaction is actually a second-order term in the expression for <span class="math inline">\(\hat{y}\)</span>. From univariate calculus, we know that second order terms are responsible for the curvature of a function. The same is true in multivariate calculus. The result is that the plane is no longer a plane, but rather a <strong>curved</strong> two-dimensional surface (or manifold, if you want to be fancy), embedded in a three-dimensional space. The nature of this curvature is unique as well, since it involves coupling between the two regressors, i.e. the surface changes in the <span class="math inline">\(x_1\)</span> direction as we move along the <span class="math inline">\(x_2\)</span> direction, and vice versa. This is apparent when looking at the partial derivatives:</p>
<p><span class="math display">\[\frac{\partial \hat{y}}{\partial x_1} = \beta_1 + \beta_3x_2 \]</span></p>
<p>and</p>
<p><span class="math display">\[\frac{\partial \hat{y}}{\partial x_2} = \beta_2 + \beta_3x_1 \]</span></p>
<p>We can further characterize the modelled surface if we’d like. For instance, since there aren’t any single-variable higher order terms, e.g. <span class="math inline">\(x_1^2\)</span>, <span class="math inline">\(x_1^3\)</span>, etc., we know that, for a given value of one of the predictors, the response variable varies linearly with the other predictor. You can verify this by setting one of the predictors to 0. Moreover since there are no terms of order higher than 2, we know that this surface has a constant curvature. This can be verified by computing the 2nd order partial derivatives. These steps maybe aren’t necessary, but they give us an idea as to how to interpret the effect of a simple product interaction.</p>
<p>In general, a multiple linear regression with <span class="math inline">\(N\)</span> predictors and various interactions between those predictors will describe a curved surface or manifold in a (N+1)-dimensional space. The more complex the interactions between the predictors, the more elaborate the surface will be. In analogy to Section <a href="#S2">2</a> such a surface will be characterized by a function, <span class="math inline">\(f\)</span>, so that</p>
<p><span class="math display">\[y = f[\{x_a\}_{a=1}^N] + \epsilon\]</span> for a response variable <span class="math inline">\(y\)</span> and predictors <span class="math inline">\(x_a\)</span>. Estimating complex surfaces such as these is the purpose of most statistical and machine learning techniques.</p>
</div>
]]>
      </description>
    </item>
    
  </channel>
</rss>
