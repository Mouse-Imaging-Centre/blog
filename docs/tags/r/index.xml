<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">  
  <channel>
    <title>R on The Mouse Imaging Centre Blog</title>
    <link>/blog/tags/r/</link>
    <description>Recent content in R on The Mouse Imaging Centre Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 23 Feb 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/blog/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Preferential Spatial Gene Expression in Neuroanatomy</title>
      <link>/blog/post/2018-02-23_gene-expression/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-02-23_gene-expression/</guid>
      <description><![CDATA[
      <div id="intro" class="section level2">
<h2>Intro</h2>
<p>In this post I will demonstrate how to use my package <code>ABIgeneRMINC</code> to download, read and analyze mouse brain gene expression data from the Allen Brain Institute.</p>
<p>The Allen Brain Institute (ABI) has acquired and released genome-wide spatial gene expression maps for the mouse brain. The data is generated using <em>in situ</em> hybridization experiments (ISH), where nucleotide probes for specific genes bind directly to mouse brain tissue. The probe binding is then marked with a biotin label that can be used to locate regions where a gene is expressed.</p>
<p>For the analysis you will need two R packages <code>RMINC</code> and <code>ABIgeneRMINC</code>.</p>
<pre class="r"><code># devtools::install_github(repo=&quot;DJFernandes/ABIgeneRMINC&quot;)   # If you need to install
library(ABIgeneRMINC)
library(RMINC)</code></pre>
</div>
<div id="getting-the-data" class="section level2">
<h2>Getting the data</h2>
<p>With the packages load you can now look up your favourite gene. You need to know the gene acronym though, which you can find on the NCBI database. In this case, I want to look up Bdnf. The function below queries the Allen Brain API and finds all experiments conducted with Bdnf.</p>
<pre class="r"><code>fge=find.gene.experiment(&#39;Bdnf&#39;)
fge</code></pre>
<pre><code>##   gene   slices ExperimentID
## 1 Bdnf sagittal     75695642
## 2 Bdnf  coronal     79587720
##                                                                  URLs
## 1 http://api.brain-map.org/grid_data/download/75695642?include=energy
## 2 http://api.brain-map.org/grid_data/download/79587720?include=energy</code></pre>
<p>We are in luck! There are two experiments the Allen Brain Institute ran with Bdnf, identified by ExperimentIDs 79587720 and 75695642. The former was conducted on coronal slices in the brain, and the latter on sagittal slices. We will see why this is important later on. The URLs where you can download expression data is given in the final column. You can enter them in your internet browser and file should begin to download. If you don’t want to leave the wonderful world of R just to download (I don’t blame you), we can actually download and read the data within R itself.</p>
<pre class="r"><code>genedata1 = read.raw.gene(as.character(fge$URLs[1]),url = TRUE)</code></pre>
<pre><code>## Loading required package: bitops</code></pre>
<p>It is generally better to download outside R and save the file, so you don’t have to keep downloading. Obtain the path to the file, and use it as as argument to read as follows:</p>
<pre class="r"><code>genedata1 = read.raw.gene(&#39;/projects/egerek/matthijs/2015-07-Allen-Brain/Allen_Gene_Expression/raw_data/coronal/Bdnf_sid79587720/energy.raw&#39;)</code></pre>
</div>
<div id="visualizing-the-gene-expression" class="section level2">
<h2>Visualizing the gene expression</h2>
<p>The gene expression data is a 1D vector. It can easily be converted to a 3D array using the mincArray function (which we will do later). But there is an important note to talk about before going further. The 1D vector lists values going from X=Anterior-to-Posterior, Y=Superior-to-Inferior, and Z=Left-to-Right (dimensions written from fastest changing index to slowest). This is the ABI orientation. The RMINC vectors typically are 1D vectors going from X=Left-to-Right, Y=Posterior-to-Anterior, Z=Inferior-to-Superior (dimensions written from fastest changing index to slowest). This is the MNI orientation. You can make a choice as to which orientiation you want to analyze in but I will be choosing MNI orientation in this tutorial. Just make sure you are consistent with your orientations and you won’t have problems. The function below converts ABI orientation to MNI orientation:</p>
<pre class="r"><code>genedata1 = allenVectorTOmincVector(genedata1)
# genedata1 = mincVectorTOallenVector(genedata1)  #This is the inverse function</code></pre>
<p>Now, we can visualize the gene expression. Below is a sagittal slice</p>
<pre class="r"><code>image(mincArray(genedata1)[30,,], ylab=&#39;Superior-Inferior&#39; ,xlab=&#39;Anterior-Posterior&#39;)</code></pre>
<p><img src="/blog/post/2018-02-23_gene-expression_files/figure-html/6-1.png" width="672" /></p>
</div>
<div id="adding-an-anatomical-underlay" class="section level2">
<h2>Adding an anatomical underlay</h2>
<p>I am not a good mouse brain anatomist, and so I find it pretty difficult to tell from this expression heatmap where exactly the expression is in the brain. We will now overlay a background MRI template to tell us where the gene expression is and use RMINC to create slice series.</p>
<pre class="r"><code>anatfile=&#39;/projects/egerek/matthijs/2015-07-Allen-Brain/allenCCFV3_to_dorr_registration/allenCCFV3/atlas_in_200um/Dorr_resampled_200um.mnc&#39;
mincPlotSliceSeries(
  anatomy=mincArray(mincGetVolume(anatfile)),
  statistics=mincArray(genedata1),
  symmetric=FALSE,
  col=colorRampPalette(c(&quot;darkgreen&quot;,&quot;yellowgreen&quot;))(255),
  legend=&quot;Bdnf Expression&quot;,low=2,high=6.5)</code></pre>
<p><img src="/blog/post/2018-02-23_gene-expression_files/figure-html/7-1.png" width="672" /></p>
<p>Much better. I can tell there is high expression in the cortex and hippocampus.</p>
<p>Let us also look at the other Bdnf experiment (ID: 75695642).</p>
<pre class="r"><code>genedata2 = read.raw.gene(as.character(fge$URLs[2]),url = TRUE)
genedata2 = allenVectorTOmincVector(genedata2)
mincPlotSliceSeries(
  anatomy=mincArray(mincGetVolume(anatfile)),
  statistics=mincArray(genedata2),
  symmetric=FALSE,
  col=colorRampPalette(c(&quot;darkgreen&quot;,&quot;yellowgreen&quot;))(255),
  legend=&quot;Bdnf Expression&quot;,low=2,high=6.5)</code></pre>
<p><img src="/blog/post/2018-02-23_gene-expression_files/figure-html/8-1.png" width="672" /></p>
<p>We see that the sagittal slices only span half the brain. This was a deliberate choice by the ABI and most of the gene experiments are like this. Furthermore, slices are sampled every 200 microns for the sagittal datasets and every 100 microns for the coronal datasets. That is why we prefer using the coronal slices any chance we get, but there are still tools that help us work with sagittal data. We can reflect data across the sagittal midplane to fill in the missing hemisphere as so:</p>
<pre class="r"><code>genedata2.reflected=midplane.reflect(genedata2,reflect.dim=3)
mincPlotSliceSeries(
  anatomy=mincArray(mincGetVolume(anatfile)),
  statistics=mincArray(genedata2.reflected),
  symmetric=FALSE,
  col=colorRampPalette(c(&quot;darkgreen&quot;,&quot;yellowgreen&quot;))(255),
  legend=&quot;Bdnf Expression&quot;,low=2,high=6.5)</code></pre>
<p><img src="/blog/post/2018-02-23_gene-expression_files/figure-html/99-1.png" width="672" /></p>
<p>We can do better. Because there sagittal sections were only sampled every 200um, there is a lot of missing data due to slice misalignment. We can fill them in using nearest neighbour marching averages.</p>
<pre class="r"><code>labelfile=system.file(&#39;extdata/gridAnnotation.raw&#39;,package=&quot;ABIgeneRMINC&quot;)
mask=allenVectorTOmincVector(read.raw.gene(labelfile,labels=TRUE)&gt;0)
interp.gene=interpolate.gene(genedata2.reflected,mask)

mincPlotSliceSeries(
  anatomy=mincArray(mincGetVolume(anatfile)),
  statistics=mincArray(interp.gene),
  symmetric=FALSE,
  col=colorRampPalette(c(&quot;darkgreen&quot;,&quot;yellowgreen&quot;))(255),
  legend=&quot;Bdnf Expression&quot;,low=2,high=6.5)</code></pre>
<p><img src="/blog/post/2018-02-23_gene-expression_files/figure-html/9-1.png" width="672" /></p>
<p>Even with interpolation, the expression map is not that good.</p>
</div>
<div id="expression-statistics" class="section level2">
<h2>Expression statistics</h2>
<p>Moving back to the coronal maps, let’s generate summary statistics for each structure in the ABI atlas.</p>
<pre class="r"><code>labels=read.raw.gene(labelfile,labels=TRUE)
labels.to.sum=sort(unique(labels))
labels.to.sum=labels.to.sum[labels.to.sum!=0]

udf=unionize(grid.data=genedata1,             #vector to unionize
           labels.to.sum=labels.to.sum,       #sum all labels
           labels.grid=labels                 #the vector of labels
           )
udf=udf[order(udf$mean,decreasing=TRUE),]
head(udf)</code></pre>
<pre><code>##     labels        sum     mean     stdev
## 167    287  19.641589 4.910397 12.018447
## 474    875  48.372759 4.397524  6.287998
## 261    483 123.968708 3.178685  3.827543
## 171    292  34.930620 3.175511  6.002760
## 162    279   6.248917 3.124459  3.977935
## 22      50  12.193722 3.048430  1.094984</code></pre>
<p>Now let us read a csv with structure names in it that correspond to the label number and add that as a column in our data frame.</p>
<pre class="r"><code>labeldefs=read.csv(&quot;/projects/egerek/matthijs/2015-07-Allen-Brain/Allen_Gene_Expression/labels/allen_gridlabels_structures.csv&quot;) #this can be downloaded from ABI
udf$structures=labeldefs[match(labeldefs$id,udf$labels),&#39;name&#39;]
udf = udf[,c(&#39;structures&#39;,&#39;labels&#39;,&#39;mean&#39;,&#39;sum&#39;,&#39;stdev&#39;)]
head(udf)</code></pre>
<pre><code>##                                                                    structures
## 167                                Main olfactory bulb, outer plexiform layer
## 474 Bed nuclei of the stria terminalis, anterior division, anterolateral area
## 261                                                   Visceral area, layer 6b
## 171                                                Uvula (IX), granular layer
## 162                       Retrosplenial area, lateral agranular part, layer 1
## 22        Bed nuclei of the stria terminalis, anterior division, oval nucleus
##     labels     mean        sum     stdev
## 167    287 4.910397  19.641589 12.018447
## 474    875 4.397524  48.372759  6.287998
## 261    483 3.178685 123.968708  3.827543
## 171    292 3.175511  34.930620  6.002760
## 162    279 3.124459   6.248917  3.977935
## 22      50 3.048430  12.193722  1.094984</code></pre>
<p>Voila, we can now tell which structures have high gene expression for the gene we are interested in.</p>
</div>
<div id="outro" class="section level2">
<h2>Outro</h2>
<p>I plan to do tutorials on more fancy gene expression analyses in the future, but this is the base from which future tutorials will be built. I hope this gets you started on using gene expression to explore neuroanatomical phenotypes and gives you an understanding of some of the caveats associated with spatial gene expression analysis.</p>
</div>
]]>
      </description>
    </item>
    
    <item>
      <title>Finding and playing with peaks in RMINC</title>
      <link>/blog/post/2018-02-08_peaks-intro/</link>
      <pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-02-08_peaks-intro/</guid>
      <description><![CDATA[
      <script src="/blog/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/blog/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/blog/rmarkdown-libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="/blog/rmarkdown-libs/datatables-binding/datatables.js"></script>
<link href="/blog/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="/blog/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="/blog/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="/blog/rmarkdown-libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="/blog/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>


<p>So, peaks. When producing a statistical map, it’s good to get a report of the peaks (i.e. most significant findings). RMINC has had this support for a while now, though it has remained somewhat hidden. Here’s a bit of an intro, then.</p>
<p>I will walk through the example we used from the Mouse Imaging Summer School in 2017, which is data from this paper:</p>
<p>de Guzman AE, Gazdzinski LM, Alsop RJ, Stewart JM, Jaffray DA, Wong CS, Nieman BJ. Treatment age, dose and sex determine neuroanatomical outcome in irradiated juvenile mice. Radiat Res. 2015 May;183(5):541–9.</p>
<p>To keep it simple, however, I’ll only look at sex differences in that dataset for now.</p>
<p>Let’s start - load the libraries and read in the csv file that describes the data.</p>
<pre class="r"><code>suppressMessages(library(RMINC))
gf &lt;- read.csv(&quot;/hpf/largeprojects/MICe/jason/MISS2017/intro-stats/fixed_datatable_IRdose.csv&quot;)</code></pre>
<p>And run a linear model relating the Jacobian determinants to sex and radiation dose. I’ll use the segmentations file as a mask; it’ll be needed later on anyway.</p>
<pre class="r"><code>labelFile &lt;- &quot;/hpf/largeprojects/MICe/jason/MISS2017/intro-stats/atlas-registration/pipeline-18-08-2017-at-07-08-48_processed/nlin-3/voted.mnc&quot;
vs &lt;- mincLm(Jacobfile_scaled0.2 ~ Sex + Dose, gf, mask = labelFile)</code></pre>
<pre><code>## Method: lm
## Number of volumes: 41
## Volume sizes: 152 320 225
## N: 41 P: 3
## In slice 
##  0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151 
## Done</code></pre>
<p>Some more data preparation: read in the background anatomy file …</p>
<pre class="r"><code>anatFile &lt;- &quot;/projects/moush/lbernas/Irradiation_behaviour_project/fixed_build_masked_23mar13_nlin/nlin-3.mnc&quot;
anatVol &lt;- mincArray(mincGetVolume(anatFile))</code></pre>
<p>… and show the results at a somewhat arbitrary threshold.</p>
<pre class="r"><code>mincPlotSliceSeries(anatVol, mincArray(vs, &quot;tvalue-SexM&quot;), anatLow=10, anatHigh=15, low=2, high=6, symmetric = T,
                    begin=50, end=-50)</code></pre>
<p><img src="/blog/post/2018-02-08_peaks-intro_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>At this point we’ve run a linear model and visually assessed the results. Now we can locate the peak findings, using the <code>mincFindPeaks</code> command.</p>
<pre class="r"><code>peaks &lt;- mincFindPeaks(vs, &quot;tvalue-SexM&quot;, minDistance = 1, threshold = 4)</code></pre>
<pre><code>## Writing column tvalue-SexM to file /tmp/Rtmpuzd1xE/file1b495f159c51.mnc 
## Range: 10.627142 -5.694668</code></pre>
<p><code>mincFindPeaks</code> uses the <code>find_peaks</code> command from the MINC toolkit under the hood. You pass in the output of one of the RMINC modelling commands (mincLm in this case, but can be anything), along with the column from that model you want to get peaks from. You can then set the minimum distance between peaks (in mm) - i.e. how far apart do two statistical peaks have to be to be included? - as well as the threshold to be considered a peak. Optionally thresholds can be different for positive and negative peaks; as always, see <code>?mincFindPeaks</code> for more detail.</p>
<p>This is what we have at this point:</p>
<pre class="r"><code>peaks</code></pre>
<div id="htmlwidget-1" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37"],[75,154,104,124,94,176,158,102,74,131,122,67,78,105,107,51,110,60,138,85,148,38,56,104,120,143,29,107,30,168,159,32,122,70,89,175,107],[157,156,193,193,184,116,121,146,108,186,144,171,135,140,171,129,46,211,228,124,203,164,74,221,222,230,172,59,143,121,72,189,161,92,66,156,141],[50,52,56,52,75,83,83,105,96,75,108,41,52,67,107,62,45,114,136,63,132,58,60,80,125,105,96,116,65,47,91,76,131,61,34,127,46],[-2.126,2.298,-0.502,0.618,-1.062,3.53,2.522,-0.614,-2.182,1.01,0.506,-2.574,-1.958,-0.446,-0.334,-3.47,-0.165999999999999,-2.966,1.402,-1.566,1.962,-4.198,-3.19,-0.502,0.394,1.682,-4.702,-0.334,-4.646,3.082,2.578,-4.534,0.506,-2.406,-1.342,3.474,-0.334],[0.546000000000001,0.49,2.562,2.562,2.058,-1.75,-1.47,-0.0699999999999985,-2.198,2.17,-0.181999999999999,1.33,-0.685999999999999,-0.406,1.33,-1.022,-5.67,3.57,4.522,-1.302,3.122,0.938000000000001,-4.102,4.13,4.186,4.634,1.386,-4.942,-0.238,-1.47,-4.214,2.338,0.770000000000001,-3.094,-4.55,0.49,-0.35],[-1.456,-1.344,-1.12,-1.344,-0.056,0.392,0.392,1.624,1.12,-0.056,1.792,-1.96,-1.344,-0.504,1.736,-0.784,-1.736,2.128,3.36,-0.728,3.136,-1.008,-0.896,0.224,2.744,1.624,1.12,2.24,-0.616,-1.624,0.84,0,3.08,-0.84,-2.352,2.856,-1.68],[10.63,9.435,8.88,7.064,5.521,5.373,5.37,5.357,5.318,5.234,5.203,4.965,4.357,4.324,4.205,4.152,4.039,-5.695,-4.836,-4.832,-4.791,-4.711,-4.632,-4.612,-4.507,-4.506,-4.504,-4.439,-4.392,-4.345,-4.278,-4.209,-4.162,-4.142,-4.088,-4.054,-4.011]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>d1<\/th>\n      <th>d2<\/th>\n      <th>d3<\/th>\n      <th>x<\/th>\n      <th>y<\/th>\n      <th>z<\/th>\n      <th>value<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"columnDefs":[{"className":"dt-right","targets":[1,2,3,4,5,6,7]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
<p>There are 7 columns; the first three give the coordinates in voxel space, the next three the coordinates in world space, and then the peak value itself. There’s a further helpful command that labels the peaks with the atlas location within which the peak is located:</p>
<pre class="r"><code>defsFile &lt;- &quot;/hpf/largeprojects/MICe/tools/atlases/Dorr_2008_Steadman_2013_Ullmann_2013_Richards_2011_Qiu_2016_Egan_2015_40micron/mappings/DSURQE_40micron_R_mapping.csv&quot;
peaks &lt;- mincLabelPeaks(peaks, labelFile, defsFile)</code></pre>
<pre><code>## Warning in peaks$label[i] &lt;- mdefs$Structure[mdefs$value == peaks
## $label[i]]: number of items to replace is not a multiple of replacement
## length

## Warning in peaks$label[i] &lt;- mdefs$Structure[mdefs$value == peaks
## $label[i]]: number of items to replace is not a multiple of replacement
## length

## Warning in peaks$label[i] &lt;- mdefs$Structure[mdefs$value == peaks
## $label[i]]: number of items to replace is not a multiple of replacement
## length

## Warning in peaks$label[i] &lt;- mdefs$Structure[mdefs$value == peaks
## $label[i]]: number of items to replace is not a multiple of replacement
## length

## Warning in peaks$label[i] &lt;- mdefs$Structure[mdefs$value == peaks
## $label[i]]: number of items to replace is not a multiple of replacement
## length</code></pre>
<p>This adds an extra column containing the name of the label for each peak:</p>
<pre class="r"><code>peaks</code></pre>
<div id="htmlwidget-2" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37"],[75,154,104,124,94,176,158,102,74,131,122,67,78,105,107,51,110,60,138,85,148,38,56,104,120,143,29,107,30,168,159,32,122,70,89,175,107],[157,156,193,193,184,116,121,146,108,186,144,171,135,140,171,129,46,211,228,124,203,164,74,221,222,230,172,59,143,121,72,189,161,92,66,156,141],[50,52,56,52,75,83,83,105,96,75,108,41,52,67,107,62,45,114,136,63,132,58,60,80,125,105,96,116,65,47,91,76,131,61,34,127,46],[-2.126,2.298,-0.502,0.618,-1.062,3.53,2.522,-0.614,-2.182,1.01,0.506,-2.574,-1.958,-0.446,-0.334,-3.47,-0.165999999999999,-2.966,1.402,-1.566,1.962,-4.198,-3.19,-0.502,0.394,1.682,-4.702,-0.334,-4.646,3.082,2.578,-4.534,0.506,-2.406,-1.342,3.474,-0.334],[0.546000000000001,0.49,2.562,2.562,2.058,-1.75,-1.47,-0.0699999999999985,-2.198,2.17,-0.181999999999999,1.33,-0.685999999999999,-0.406,1.33,-1.022,-5.67,3.57,4.522,-1.302,3.122,0.938000000000001,-4.102,4.13,4.186,4.634,1.386,-4.942,-0.238,-1.47,-4.214,2.338,0.770000000000001,-3.094,-4.55,0.49,-0.35],[-1.456,-1.344,-1.12,-1.344,-0.056,0.392,0.392,1.624,1.12,-0.056,1.792,-1.96,-1.344,-0.504,1.736,-0.784,-1.736,2.128,3.36,-0.728,3.136,-1.008,-0.896,0.224,2.744,1.624,1.12,2.24,-0.616,-1.624,0.84,0,3.08,-0.84,-2.352,2.856,-1.68],[10.63,9.435,8.88,7.064,5.521,5.373,5.37,5.357,5.318,5.234,5.203,4.965,4.357,4.324,4.205,4.152,4.039,-5.695,-4.836,-4.832,-4.791,-4.711,-4.632,-4.612,-4.507,-4.506,-4.504,-4.439,-4.392,-4.345,-4.278,-4.209,-4.162,-4.142,-4.088,-4.054,-4.011],["left Medial amygdala","right Medial amygdala","left hypothalamus","right hypothalamus","left bed nucleus of stria terminalis","right subiculum","right MoDG","left LMol","left pre-para subiculum","right bed nucleus of stria terminalis","right CA1Rad","left amygdala","left CA3Py Inner","left fasciculus retroflexus","left CA30r","left CA1Py"," medulla","left Primary somatosensory cortex","right Frontal association cortex"," midbrain","right Primary motor cortex","left Piriform cortex","left paramedian lobule (lobule 7)","left Dorsal tenia tecta","right Cingulate cortex: area 32","right Lateral orbital cortex","left Secondary somatosensory cortex"," lobule 6: declive","left Dorsolateral entorhinal cortex","right Caudomedial entorhinal cortex","right crus 1: ansiform lobule (lobule 6)","left Insular region: not subdivided","right Cingulate cortex: area 24b'"," medulla"," medulla","right Primary somatosensory cortex: barrel field","left mammillary bodies"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>d1<\/th>\n      <th>d2<\/th>\n      <th>d3<\/th>\n      <th>x<\/th>\n      <th>y<\/th>\n      <th>z<\/th>\n      <th>value<\/th>\n      <th>label<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"columnDefs":[{"className":"dt-right","targets":[1,2,3,4,5,6,7]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
<p>At this point you have all the info about the most significant peaks in the dataset. There is one additional useful command, a shortcut to plot each peak. Plotting the two most positive and the two most negative peaks, for example:</p>
<pre class="r"><code>opar &lt;- par(mfrow=c(2,2), mar=c(0,0,0,0))
nP &lt;- nrow(peaks)
for (i in c(1:2, nP:(nP-1))) {
  mincPlotPeak(peaks[i,], anatVol, mincArray(vs, &quot;tvalue-SexM&quot;), anatLow=10, anatHigh=15, low=2, high=6, symmetric=T)
}</code></pre>
<p><img src="/blog/post/2018-02-08_peaks-intro_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="r"><code>par(opar)</code></pre>
<p>There is an additional argument to <code>mincPlotPeak</code> - a function to create a plot of the peak. This is illustrated below:</p>
<pre class="r"><code># load ggplot2 for plotting
library(ggplot2)
# the plotting function; needs to take a single argument, the peak
p &lt;- function(peak) {
  # read in the data for that particular peak
  gf$voxel &lt;- mincGetWorldVoxel(gf$Jacobfile_scaled0.2, peak[&quot;x&quot;], peak[&quot;y&quot;], peak[&quot;z&quot;])
  # and create a box-plot; also read in info from the peak for a meaningful title
  ggplot(gf) + aes(x=Sex, y=exp(voxel)) + geom_boxplot() + 
    ggtitle(paste(&quot;Peak:&quot;, peak[&quot;label&quot;]), subtitle = paste(&quot;T statistic:&quot;, peak[&quot;value&quot;]))
}
# and plot the peak with its plot
mincPlotPeak(peaks[1,], anatVol, mincArray(vs, &quot;tvalue-SexM&quot;), anatLow=10, anatHigh=15, low=2, high=6, 
             symmetric=T, plotFunction = p)</code></pre>
<pre><code>## Don&#39;t know how to automatically pick scale for object of type mincVoxel/vector. Defaulting to continuous.</code></pre>
<p><img src="/blog/post/2018-02-08_peaks-intro_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>And that’s it. A quick survey for how to extract peaks, view them in a table, and create figures of the most significant findings.</p>
]]>
      </description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>/blog/post/linearmodels/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/linearmodels/</guid>
      <description><![CDATA[
      <div id="S1" class="section level1">
<h1>Preamble</h1>
<p>The purpose of this post is to elucidate some of the concepts associated with statistical linear models.</p>
<p>Let’s start by loading some libraries.</p>
<pre class="r"><code>library(ggplot2)
library(datasets)</code></pre>
</div>
<div id="S2" class="section level1">
<h1>Background Theory</h1>
<p>The basic idea is as follows:</p>
<p>Given two variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, for which we’ve measured a set of data points <span class="math inline">\(\{x_i, y_i\}\)</span> with <span class="math inline">\(i = 1, ..., n\)</span>, we want to estimate a function, <span class="math inline">\(f(x)\)</span>, such that</p>
<p><span class="math display">\[y_i = f(x_i) + \epsilon_i\]</span></p>
<p>for each data point <span class="math inline">\((x_i,y_i)\)</span>. Here <span class="math inline">\(\epsilon_i\)</span> is the error in data point <span class="math inline">\(y_i\)</span> compared to the predicted value <span class="math inline">\(f(x_i)\)</span>. Specifically,</p>
<p><span class="math display">\[\epsilon_i = y_i - f(x_i)\]</span></p>
<p>We don’t know <span class="math inline">\(f(x)\)</span>, but the simplest functional form that we can assume is that of a linear function:</p>
<p><span class="math display">\[f(x) = \beta_0 + \beta_1x \]</span> where <span class="math inline">\(\beta_0\)</span> is the intercept of the line and <span class="math inline">\(\beta_1\)</span> is the slope associated with the variable <span class="math inline">\(x\)</span>. Thus for each data point <span class="math inline">\(x_i\)</span>, we predict a value <span class="math inline">\(\hat{y}_i = f(x_i)\)</span> so that</p>
<p><span class="math display">\[\hat{y}_i = \beta_0 + \beta_1x_i\]</span> This is known as <strong>simple linear regression</strong>. Having specified the form of <span class="math inline">\(f(x)\)</span>, the problem becomes one of optimizing the free parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> so that the predicted or trend line best describes the real data <span class="math inline">\(\{x_i, y_i\}\)</span>. This is usually accomplished using the “ordinary least-squares” method, in which we minimize the sum of squared errors with respect to the free parameters. Explicitly, if we write the sum of squared errors as</p>
<p><span class="math display">\[\chi^2 = \sum_{i=1}^n{\epsilon_i^2} = \sum_{i=1}^n{(y_i - \hat{y}_i)^2} = \sum_{i=1}^n{(y_i - \beta_0 - \beta_1x_i)^2}\]</span> we want to determine <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that</p>
<p><span class="math display">\[\frac{\partial\chi^2}{\partial\beta_0} = 0\]</span> and <span class="math display">\[\frac{\partial\chi^2}{\partial\beta_1} = 0\]</span></p>
<p>This can be solved analytically. In practice we let the computer do it for us.</p>
<p>Moving on , there’s no reason we need to restrict ourselves to one predictor for the response variable <span class="math inline">\(y\)</span>, so we can include multiple variables <span class="math inline">\(\{x_1, x_2, ..., x_N\}\)</span> in our model. Here <span class="math inline">\(N\)</span> is the total number of regressors that are included. This is known as <strong>multiple linear regression</strong>. The model then becomes</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \sum_{a=1}^N{} \beta_ax_a\]</span> where <span class="math inline">\(a\)</span> is an index summing over the predictors (rather than over the data points themselves as in the expression for <span class="math inline">\(\chi^2\)</span> above). Here I’ve expressed the model in terms of the variables, rather than individual data points. For an individual data point <span class="math inline">\(\{x_i,y_i\}\)</span>, we could write this as</p>
<p><span class="math display">\[\hat{y}_i = \beta_0 + \sum_{a=1}^N{} \beta_ax_{a,i}\]</span> where <span class="math inline">\(x_{a,i}\)</span> is the <span class="math inline">\(i\)</span>th data point of the <span class="math inline">\(a\)</span>th variable (e.g. the height, which is the variable, of a specific person). The two are identical representations. The optimization process for multiple linear regression is the same as that for simple linear regression, only involving more derivatives.</p>
<p>Alright that should be enough background math. In the next Section, we’ll look at this in practice.</p>
</div>
<div id="S3" class="section level1">
<h1>Simple Linear Regression in Practice</h1>
<p>We’ll be working with the <code>mtcars</code> dataset.</p>
<pre class="r"><code>str(mtcars)</code></pre>
<pre><code>## &#39;data.frame&#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</code></pre>
<p>The description of these variables can be found <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html">here</a>. We’re going to start by looking at the relationship between two continuous variables. Specifically, I’ve chosen to examine the relationship between car weight, <code>wt</code>, and fuel efficiency, <code>mpg</code>. Let’s start by creating a scatter plot to look at how the fuel efficiency varies with car weight.</p>
<pre class="r"><code>p.mpg_vs_wt &lt;- ggplot(mtcars, aes(x=wt,y=mpg)) + geom_point()
p.mpg_vs_wt + 
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Scatter Plot of MPG vs. Weight&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>This is as we would expect, since less energy is needed to move lighter cars. Moreover we suspect that this data might be well suited to a simple linear model. As described above, the model we will be building is</p>
<p><span class="math display">\[y = \beta_0 + \beta_1x + \epsilon\]</span></p>
<p>or</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the <code>mpg</code> variable in this case and <span class="math inline">\(x\)</span> is the <code>wt</code> variable. In <code>R</code> formula notation, we can express this as <code>mpg ~ wt</code>, where <code>~</code> means “is modelled by”. The way to build a linear model in <code>R</code> is using the <code>lm()</code> function, as follows:</p>
<pre class="r"><code>mpg_vs_wt &lt;- summary(lm(mpg ~ wt,data=mtcars))
print(mpg_vs_wt)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5432 -2.3647 -0.1252  1.4096  6.8727 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***
## wt           -5.3445     0.5591  -9.559 1.29e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.046 on 30 degrees of freedom
## Multiple R-squared:  0.7528, Adjusted R-squared:  0.7446 
## F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10</code></pre>
<p>The focus of this document will be on interpreting the <code>Estimate</code> column of the <code>Coefficients</code> table. Let’s pull this table from the output:</p>
<pre class="r"><code>print(mpg_vs_wt$coefficients)</code></pre>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 37.285126   1.877627 19.857575 8.241799e-19
## wt          -5.344472   0.559101 -9.559044 1.293959e-10</code></pre>
<p>This is fairly straightforward to interpret. Looking at the <code>Estimate</code> column, the <code>(Intercept)</code> value describes the value of <span class="math inline">\(\beta_0\)</span> in our model, while the <code>wt</code> value describes the slope associated with the <code>wt</code> variable, i.e. <span class="math inline">\(\beta_1\)</span>. Our fitted model looks like this:</p>
<p><span class="math display">\[\hat{y} = 37.3- 5.3x\]</span> where <span class="math inline">\(x\)</span> is the <code>wt</code> variable and <span class="math inline">\(y\)</span> is the <code>mpg</code> variable, as mentioned above.</p>
<p>Let’s see how this looks.</p>
<pre class="r"><code>p.mpg_vs_wt + 
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Simple Linear Model: MPG vs. Weight&quot;) + 
  geom_abline(intercept = mpg_vs_wt$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;], 
              slope = mpg_vs_wt$coefficients[&quot;wt&quot;, &quot;Estimate&quot;]) + 
  coord_cartesian(xlim=c(0,6),ylim = c(5,40))</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>That looks pretty good. A second-order polynomial might better capture the data at the lower and higher <code>wt</code> values, but we won’t go into that here.</p>
</div>
<div id="S4" class="section level1">
<h1>Simple Linear Regression with Categorical Variables</h1>
<p>In the previous Section, we looked at how simple linear regression works when the predictor is a continuous variable, like <code>wt</code>. Here we will examine what happens when we model categorical variables. Recall that a categorical variable is a variable that takes on discrete, usually non-numerical, values. For example, sex is a categorical variable, with the values being male or female.</p>
<p>In the <code>mtcars</code> dataset, we’ll look at the <code>am</code> variable, which describes whether the car has manual or automatic transmission. Let’s explicitly express this as a factor and display the unique values.</p>
<pre class="r"><code>mtcars$am &lt;- as.factor(mtcars$am)
unique(mtcars$am)</code></pre>
<pre><code>## [1] 1 0
## Levels: 0 1</code></pre>
<p>So <code>am</code> has two possible values. Manual transmission is encoded as <code>am = 1</code> while automatic transmission is encoded as <code>am = 0</code> (refer to the <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html">dataset description</a>). As we did above, we can examine how <code>wt</code> varies with the <code>am</code> variable. Again we are fitting a model</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1A\]</span> where <span class="math inline">\(y\)</span> is the <code>wt</code> variable and <span class="math inline">\(A\)</span> is the <code>am</code> variable. Keep in mind that <span class="math inline">\(A\)</span> is binary. In <code>R</code>:</p>
<pre class="r"><code>mpg_vs_am &lt;- summary(lm(mpg ~ am, data=mtcars ))
print(mpg_vs_am$coefficients)</code></pre>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 17.147368   1.124603 15.247492 1.133983e-15
## am1          7.244939   1.764422  4.106127 2.850207e-04</code></pre>
<p>Thus we see that <span class="math inline">\(\beta_0\)</span> = 17.1 and <span class="math inline">\(\beta_1\)</span> = 7.2. Note here that in the <code>Coefficients</code> table, the slope estimate is associated with the label <code>am1</code>. This means that it displays the slope going from <code>am = 0</code> to <code>am = 1</code>. This is perhaps expected in this case since 1 is greater than 0, but in general categorical variables will not have numerical values. Consider again a variable describing sex. The two instances are “Male” and “Female”. There is no specific order in which to compute the slope. When building a model with categorical variables such as these, <code>R</code> will implicitly assign values of 0 and 1 to the levels of the variable. By default, <code>R</code>, assigns the <strong>reference level</strong>, i.e. a value of 0, to the value that is <strong>lowest in alphabetical order</strong>. For the sex variable, “Female” would be associated with 0, and “Male” with 1. So when running <code>lm()</code> on such a model and printing the output, the <code>Coefficients</code> table will have a row with a name like <code>sexMale</code>. The <code>Estimate</code> value associated with this describes the slope of the model going from <code>sex=Female</code> (which is implicitly defined as 0) to <code>sex=Male</code> (which is implicitly defined as 1). This might seem unnecessary right now, but it becomes important when trying to interpret the output from more complex models, as we’ll see below.</p>
<p>As above, we can plot this data and model.</p>
<pre class="r"><code>ggplot(mtcars, aes(x=am,y=mpg)) + 
  geom_jitter(width=0.1) + 
  geom_smooth(method=&quot;lm&quot;, formula = y~x, aes(group=1), se=F) + 
  labs(x = &quot;Transmission (0 = automatic)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Simple Linear Model: MPG vs. Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="S5" class="section level1">
<h1>Multiple Linear Regression in Practice</h1>
<p>In this Section we will combine the models built in the two previous Sections using multiple linear regression. Specifically, we will model <code>mpg</code> by <code>wt</code> and <code>am</code> together.</p>
<p>Let’s start by taking a look at the data.</p>
<pre class="r"><code>ggplot(mtcars, aes(x=wt, y=mpg, col=am)) + 
  geom_point() +  
   labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Fuel Efficiency, Weight, and Transmission&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Again, I’ve plotted <code>mpg</code> against <code>wt</code> using a scatter plot, but I’ve mapped the colour aesthetic to the <code>am</code> variable to see the group variation.</p>
<p>Mathematically, the model we will use looks like</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2A\]</span> where, as above, <span class="math inline">\(y\)</span> is the <code>mpg</code> variable, <span class="math inline">\(x\)</span> is the <code>wt</code> continuous variable, and <code>A</code> is the categorical <code>am</code> variable. <span class="math inline">\(\beta_1\)</span> is the slope associated with <span class="math inline">\(x\)</span> and <span class="math inline">\(\beta_2\)</span> is the slope associated with <span class="math inline">\(A\)</span>. Recall that <span class="math inline">\(A\)</span> is only 0 or 1.</p>
<p>Let’s go ahead and build the model in <code>R</code>.</p>
<pre class="r"><code>mpg_MLR &lt;- summary(lm(mpg ~ wt + am, data=mtcars))
print(mpg_MLR$coefficients)</code></pre>
<pre><code>##                Estimate Std. Error     t value     Pr(&gt;|t|)
## (Intercept) 37.32155131  3.0546385 12.21799285 5.843477e-13
## wt          -5.35281145  0.7882438 -6.79080719 1.867415e-07
## am1         -0.02361522  1.5456453 -0.01527855 9.879146e-01</code></pre>
<p>Again, keeping in mind the mathematical formula, we see that <code>(Intercept)</code> corresponds to <span class="math inline">\(\beta_0\)</span> = 37.32, <code>wt</code> corresponds to <span class="math inline">\(\beta_1\)</span> = -5.35 and <code>am1</code> corresponds to <span class="math inline">\(\beta_2\)</span> = -0.02. So how do we interpret this? How could we sketch this model on a scatter plot of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> like the one above? The way to go about it is to examine the two cases for our categorical variables <span class="math inline">\(A\)</span> = <code>am</code>. Recall that, regardless of what our factor levels are (0/1, Female/Male), <code>R</code> always encodes categorical variables as 0 and 1. Consequently we can always examine our mathematical model by setting the corresponding variable to 0 or 1. For <span class="math inline">\(A\)</span> = <code>am</code> = 0, we have</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span> In this case, when <span class="math inline">\(A\)</span> = 0, <span class="math inline">\(\beta_0\)</span> is the intercept of the line relating <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, and <span class="math inline">\(\beta_1\)</span> is the slope associated with <span class="math inline">\(x\)</span>. On the scatter plot, we would draw a line with this slope and intercept. What about when <span class="math inline">\(A = 1\)</span>?</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2\]</span> <span class="math display">\[= (\beta_0 + \beta_2) + \beta_1x\]</span> <span class="math display">\[= \beta_0&#39; + \beta_1x\]</span></p>
<p>We find that we actually have a new intercept value <span class="math inline">\(\beta_0&#39; = \beta_0 + \beta_2\)</span>, but the same slope <span class="math inline">\(\beta_1\)</span>. Thus the trend line associated with <span class="math inline">\(A = 1\)</span> has a different intercept than that associated with <span class="math inline">\(A = 0\)</span>. What we’ve discovered is that the <span class="math inline">\(\beta_2\)</span> parameter tells us the <strong>difference in the intercept values</strong> between the <code>am = 0</code> and <code>am = 1</code> groups, i.e. <span class="math inline">\(\beta_2 = \beta_0&#39; - \beta_0\)</span>. The <span class="math inline">\(\beta_1\)</span> parameter tells us the slope of the two lines.</p>
<p>Let’s see what this looks like.</p>
<pre class="r"><code>beta0 &lt;- mpg_MLR$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;]
beta1 &lt;- mpg_MLR$coefficients[&quot;wt&quot;,&quot;Estimate&quot;]
beta0_prime &lt;- mpg_MLR$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;] + 
                mpg_MLR$coefficients[&quot;am1&quot;,&quot;Estimate&quot;]

ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&quot;red&quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1, 
              col=&quot;blue&quot;,
              alpha=0.5) + 
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Multiple Linear Model: MPG ~ Weight + Transmission&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>There are actually two lines in this plot, a blue one and a red one. Given how small <span class="math inline">\(\beta_2\)</span> is we can’t see much of a difference, but the blue trend line should be slightly lower than the red line. Their slopes are the same. Let’s zoom in to be sure.</p>
<pre class="r"><code>ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&quot;red&quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1, 
              col=&quot;blue&quot;,
              alpha=0.5) + 
  coord_cartesian(xlim = c(3.4,3.5),ylim = c(17.5,19.5)) + 
    labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Multiple Linear Model: MPG ~ Weight + Transmission (Zoomed In)&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>There we go. This is the effect of a multiple linear model with no interactions. In general the trend lines for the two groups don’t have to be so close together. It all depends on the data. In this case we see that, when we impose a fixed slope (using <code>mpg ~ wt + am</code>), the trend lines describing the two groups are basically the same. Next, we’ll look at how adding interactions to our model results in slope variation.</p>
</div>
<div id="S6" class="section level1">
<h1>Multiple Linear Regression with Interactions</h1>
<p>In the previous Section we examined the use of multiple linear regression to model a response variable in terms of continuous and categorical predictors. We can take this a step further by including an <strong>interaction</strong> in our model. What does this mean? An interaction describes how a change in one predictor influences change in another predictor. In mathematics this is typically expressed by including a product or more complex term in an equation. A simple product of two variables is the simplest interaction term that we can write down. Consider once again our model of <code>mpg</code> vs. <code>wt</code> and <code>am</code>. This time we will add an interaction:</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2A + \beta_3xA\]</span></p>
<p>The model is as above, with the exception of the new interaction term <span class="math inline">\(\beta_3xA\)</span>. <span class="math inline">\(xA\)</span> is the product interaction while <span class="math inline">\(\beta_3\)</span> is the model parameter associated with this interaction.</p>
<p><strong>An important note</strong>: In <code>R</code> formula notation, interactions are expressed as <code>wt*am</code>. This includes all terms in the model, i.e. <code>wt*am</code> = <code>1 + wt + am + wt:am</code> where <code>1</code> stands in for the “variable” associated with the <span class="math inline">\(\beta_0\)</span> parameter, and the <strong>colon denotes a product</strong>. We can thus write our full interactive model as <code>mpg ~ wt*am</code>.</p>
<p>Let’s run this model through <code>lm()</code> and see what happens.</p>
<pre class="r"><code>mpg_int &lt;- summary(lm(mpg ~ wt*am,data=mtcars))
print(mpg_int$coefficients)</code></pre>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 31.416055  3.0201093 10.402291 4.001043e-11
## wt          -3.785908  0.7856478 -4.818836 4.551182e-05
## am1         14.878423  4.2640422  3.489277 1.621034e-03
## wt:am1      -5.298360  1.4446993 -3.667449 1.017148e-03</code></pre>
<p>We have four rows for our four parameters. Again each row corresponds to one <span class="math inline">\(\beta\)</span> parameter: <code>(Intercept)</code> corresponds to <span class="math inline">\(\beta_0\)</span> = 31.42, <code>wt</code> corresponds to <span class="math inline">\(\beta_1\)</span> = -3.79, <code>am1</code> corresponds to <span class="math inline">\(\beta_2\)</span> = 14.88 and <code>wt:am1</code> corresponds to <span class="math inline">\(\beta_3\)</span> = -5.3. Let’s interpret this, as we did in the previous Section, by examining the different values of the categorical variable <code>am</code>.</p>
<p>When <span class="math inline">\(A\)</span> = <code>am</code> = 0, we have</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span> which is just our simple model. So <span class="math inline">\(\beta_0\)</span> describes the intercept when <span class="math inline">\(A = 0\)</span> and <span class="math inline">\(\beta_1\)</span> describes the slope associated with <span class="math inline">\(x\)</span> when <span class="math inline">\(A = 0\)</span>. What happens when <span class="math inline">\(A = 1\)</span>?</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2 + \beta_3x\]</span> <span class="math display">\[= (\beta_0 + \beta_2) + (\beta_1 + \beta_3)x\]</span> <span class="math display">\[= \beta_0&#39; + \beta_1&#39;x\]</span></p>
<p>So we actually have a new intercept and a new slope! The new intercept is the same as in the previous Section: <span class="math inline">\(\beta_0&#39; = \beta_0 + \beta_2\)</span>. The new slope is <span class="math inline">\(\beta_1&#39; = \beta_1 + \beta_3\)</span>. Therefore, a simple product interaction like this one causes the slope of <span class="math inline">\(y\)</span> with <span class="math inline">\(x\)</span> to change as we move from <span class="math inline">\(A = 0\)</span> to <span class="math inline">\(A = 1\)</span>. Displaying this on a scatter plot of <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>, we would have two lines with different intercepts and different slopes.</p>
<p>Let’s take a look.</p>
<pre class="r"><code>beta0 = mpg_int$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;]
beta1 = mpg_int$coefficients[&quot;wt&quot;,&quot;Estimate&quot;]
beta0_prime = mpg_int$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;] + 
                mpg_int$coefficients[&quot;am1&quot;,&quot;Estimate&quot;]
beta1_prime = mpg_int$coefficients[&quot;wt&quot;,&quot;Estimate&quot;] + 
                mpg_int$coefficients[&quot;wt:am1&quot;,&quot;Estimate&quot;]

ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&quot;red&quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1_prime, 
              col=&quot;blue&quot;,
              alpha=0.5) + 
  coord_cartesian(xlim=c(0,6),
                  ylim = c(0,50)) +
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Weight-Transmission Interaction&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Perfect. Compared to the non-interactive model in the previous Section, we see that adding an interaction (and thus allowing the slope of <span class="math inline">\(y\)</span> with <span class="math inline">\(x\)</span> to vary between groups) better characterizes the data. If the data was truly not well characterized by an interaction, the equal-slope model of the previous Section would perform just as well as an interactive model of this kind. Of course this would have to be determined by examining the statistics associated with the models.</p>
<p>This concludes the majority of what I wanted to cover. In the next Section I’ll go into some of the heavier mathematical details regarding linear models. Read ahead at your own peril.</p>
</div>
<div id="S7" class="section level1">
<h1>Mathematical Embellishments</h1>
<p>The previous analysis was focussed on examining linear models built with a continuous and categorical predictor. We’re by no means restricted to this however. We can build a linear model with as many variables as we’d like. As we saw above, the case of one continuous predictor and one categorical predictor can be visualized fairly easily using a scatter plot, where the continous data is mapped to the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axes and the categorical data is mapped to a colour/shape/style aesthetic. This becomes harder to do when we’re examining models that use multiple continuous predictors, e.g. something of the form</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2\]</span></p>
<p>Here we have an interactive multiple linear regression with two continuous regressors <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. A practical example of this would be modelling the <code>mpg</code> variable in terms of the <code>wt</code> and horsepower, <code>hp</code>. Let’s plot this in the same way we did above, where the <code>hp</code> variable is mapped to the colour aesthetic.</p>
<pre class="r"><code>ggplot(mtcars, aes(x=wt, y=mpg, col=hp)) + 
  geom_point() +  
   labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Fuel Efficiency, Weight, and Horsepower&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Given the continuous nature of the <code>hp</code> variable, this is difficult to interpret by eye. That doesn’t invalidate the model however, and we can still estimate the optimal <span class="math inline">\(\beta\)</span> parameters.</p>
<pre class="r"><code>mpg_hp &lt;- summary(lm(mpg ~ wt*hp, data=mtcars))
print(mpg_hp$coefficients)</code></pre>
<pre><code>##                Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 49.80842343 3.60515580 13.815887 5.005761e-14
## wt          -8.21662430 1.26970814 -6.471270 5.199287e-07
## hp          -0.12010209 0.02469835 -4.862758 4.036243e-05
## wt:hp        0.02784815 0.00741958  3.753332 8.108307e-04</code></pre>
<p>The rows in this table still represent the <span class="math inline">\(\beta\)</span> parameters in the model, as before, but it is much harder to interpret this result in the context of a scatter plot of <code>mpg</code> vs. <code>wt</code>. We can’t just set <code>hp = 0</code> and <code>hp = 1</code> as we did previously. The equivalent here would be setting <code>hp</code> to an infinite number of incremental values. This isn’t the right way to think about this. This sort of brings us face to face with what the multiple linear model is actually saying. Consider again simple linear regression:</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span> This is clearly the expression for a line. The variable <span class="math inline">\(y\)</span> is modelled as a linear function of <span class="math inline">\(x\)</span>. As we know, we can plot this easily on a two-dimensional space, where <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> form the axes. Moving to multiple regression, we have</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2\]</span></p>
<p>In Sections <a href="#S5">5</a> and <a href="#S6">6</a>, we interpreted this model <strong>within the context of the scatter plot of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y\)</span></strong>. With <span class="math inline">\(x_2\)</span> as a categorical variable, this allowed us to interpret <span class="math inline">\(\beta_2\)</span> as the difference in intercept values on this scatter plot. Mathematically, however, this expression describes a <strong>two-dimensional plane</strong> characterised by the variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. We see that the intercept of the plane is <span class="math inline">\(\beta_0\)</span>, i.e. the value of <span class="math inline">\(y\)</span> at which both the parametric variables are 0. Moreover, <span class="math inline">\(\beta_1\)</span> is the slope of the plane with respect to <span class="math inline">\(x_1\)</span>, and <span class="math inline">\(\beta_2\)</span> is the slope of the plane with respect to <span class="math inline">\(x_2\)</span>. Specifically,</p>
<p><span class="math display">\[\beta_1 = \frac{\partial\hat{y}}{\partial x_1}\]</span></p>
<p>and</p>
<p><span class="math display">\[\beta_2 = \frac{\partial\hat{y}}{\partial x_2}\]</span> We can visualize this two-dimensional plane in a three-dimensional space, where the third axis is represented by the <span class="math inline">\(y\)</span> variable. You can do this easily by grabbing a piece of paper, or preferably a rigid flat object, and orienting it in front of you in real space. You can imagine the <span class="math inline">\(y\)</span> axis as the vertical axis, and the <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> axes as the horizontal axes. The idea of a flat plane in a multi-dimensional space extends to any number of predictors in our model, provided that the model is non-interactive. Given <span class="math inline">\(N\)</span> predictors <span class="math inline">\(\{x_1, x_2, ..., x_N\}\)</span>, a model of the form</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \sum_{a=1}^N{\beta_ax_a}\]</span></p>
<p>describes a N-dimensional hyperplane embedded in a (N+1)-dimensional space. Crazy. This is undoubtedly true of continuous variables, but is a little bit more nuanced for a categorical variable. Going back to our model of <code>mpg</code> vs. <code>wt</code> and <code>am</code>, we can still imagine this in a three-dimensional space. The axes of the space are <code>mpg</code>,<code>wt</code> and <code>am</code>, but notice that we aren’t actually dealing with a plane in this case, since <code>am</code> only takes on binary values. Rather we are dealing with two different lines embedded in this three-dimensional space. One line will occur at <code>am = 0</code>, while the other occurs at <code>am = 1</code>. Given this context, we can re-interpret the scatter plots from Section <a href="#S5">5</a> and <a href="#S6">6</a>. Here is the scatter plot and model from Section <a href="$S6">6</a>:</p>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Within the proper context of a three-dimensional space, this two-dimensional plot is actually the <strong>projection of the binary <code>am</code> axis onto <code>am = 0</code></strong>. Imagine the red line existing in your computer screen and the blue line actually existing outside of your computer screen, closer to you. I’ve used the interactive model here since the plot is nicer than that for the non-interactive model, but this leads us into a discussion of interactions with continuous variables.</p>
<p>Examining the plot above, you might already guess where this is going. Let’s consider a model with a simple product interaction of two continuous variables, like the one at the beginning of this Section:</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2\]</span></p>
<p>We saw that when <span class="math inline">\(\beta_3 = 0\)</span>, the model describes a two-dimensional plane in three-dimensions. What does the interaction do to this plane? Notice that the product interaction is actually a second-order term in the expression for <span class="math inline">\(\hat{y}\)</span>. From univariate calculus, we know that second order terms are responsible for the curvature of a function. The same is true in multivariate calculus. The result is that the plane is no longer a plane, but rather a <strong>curved</strong> two-dimensional surface (or manifold, if you want to be fancy), embedded in a three-dimensional space. The nature of this curvature is unique as well, since it involves coupling between the two regressors, i.e. the surface changes in the <span class="math inline">\(x_1\)</span> direction as we move along the <span class="math inline">\(x_2\)</span> direction, and vice versa. This is apparent when looking at the partial derivatives:</p>
<p><span class="math display">\[\frac{\partial \hat{y}}{\partial x_1} = \beta_1 + \beta_3x_2 \]</span></p>
<p>and</p>
<p><span class="math display">\[\frac{\partial \hat{y}}{\partial x_2} = \beta_2 + \beta_3x_1 \]</span></p>
<p>We can further characterize the modelled surface if we’d like. For instance, since there aren’t any single-variable higher order terms, e.g. <span class="math inline">\(x_1^2\)</span>, <span class="math inline">\(x_1^3\)</span>, etc., we know that, for a given value of one of the predictors, the response variable varies linearly with the other predictor. You can verify this by setting one of the predictors to 0. Moreover since there are no terms of order higher than 2, we know that this surface has a constant curvature. This can be verified by computing the 2nd order partial derivatives. These steps maybe aren’t necessary, but they give us an idea as to how to interpret the effect of a simple product interaction.</p>
<p>In general, a multiple linear regression with <span class="math inline">\(N\)</span> predictors and various interactions between those predictors will describe a curved surface or manifold in a (N+1)-dimensional space. The more complex the interactions between the predictors, the more elaborate the surface will be. In analogy to Section <a href="#S2">2</a> such a surface will be characterized by a function, <span class="math inline">\(f\)</span>, so that</p>
<p><span class="math display">\[y = f[\{x_a\}_{a=1}^N] + \epsilon\]</span> for a response variable <span class="math inline">\(y\)</span> and predictors <span class="math inline">\(x_a\)</span>. Estimating complex surfaces such as these is the purpose of most statistical and machine learning techniques.</p>
</div>
]]>
      </description>
    </item>
    
  </channel>
</rss>
