<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on The Mouse Imaging Centre Blog</title>
    <link>/blog/post/</link>
    <description>Recent content in Posts on The Mouse Imaging Centre Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 09 Feb 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/blog/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Finding and playing with peaks in RMINC</title>
      <link>/blog/post/2018-02-08_peaks-intro/</link>
      <pubDate>Fri, 09 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-02-08_peaks-intro/</guid>
      <description>So, peaks. When producing a statistical map, it’s good to get a report of the peaks (i.e. most significant findings). RMINC has had this support for a while now, though it has remained somewhat hidden. Here’s a bit of an intro, then.
I will walk through the example we used from the Mouse Imaging Summer School in 2017, which is data from this paper:
de Guzman AE, Gazdzinski LM, Alsop RJ, Stewart JM, Jaffray DA, Wong CS, Nieman BJ.</description>
    </item>
    
    <item>
      <title>Bayesian Model Selection with PSIS-LOO</title>
      <link>/blog/post/2018-01-31_loo-intro/</link>
      <pubDate>Wed, 31 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-01-31_loo-intro/</guid>
      <description>Pitch In this post I’d like to provide an overview of Pareto-Smoothed Importance Sampling (PSIS-LOO) and how it can be used for bayesian model selection. Everything I discuss regarding this technique can be found in more detail in Vehtari, Gelman, and Gabry (2016). To lead up to PSIS-LOO I will introduce Akaike’s Information Criterion (AIC) to lay the foundation for model selection in general, then cover the expected log predictive density, the corner stone of bayesian model selection.</description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>/blog/post/linearmodels/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/linearmodels/</guid>
      <description>Preamble The purpose of this post is to elucidate some of the concepts associated with statistical linear models.
Let’s start by loading some libraries.
library(ggplot2) library(datasets)  Background Theory The basic idea is as follows:
Given two variables, \(x\) and \(y\), for which we’ve measured a set of data points \(\{x_i, y_i\}\) with \(i = 1, ..., n\), we want to estimate a function, \(f(x)\), such that
\[y_i = f(x_i) + \epsilon_i\]</description>
    </item>
    
    <item>
      <title>StanCon Highlights</title>
      <link>/blog/post/2018-01-13_stancon-highlights/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/2018-01-13_stancon-highlights/</guid>
      <description>Hi readers,
Recently I got back from StanCon 2018 Ansilomar. I had a little time waiting for one of my flights and I thought I’d reflect on the conference. Last year I was lucky enough to go to the first StanCon and it was nice to be able to see how the conference has grown. This year it was three days of tutorials, talks, and networking. I had a blast.</description>
    </item>
    
    <item>
      <title>Welcome</title>
      <link>/blog/post/welcome/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/welcome/</guid>
      <description>Hello World!
Welcome to the new Mouse Imaging Centre Blog.
We created this blog as a place to share things we’ve been thinking about lately. This blog will feature a mixture of technical writing about things we do in our lab including imaging, statistics, and biology. The blog will feature contributions from our staff, students, and PIs.
We hope you enjoy our writings.
To learn more about us please visit our website at http://www.</description>
    </item>
    
  </channel>
</rss>