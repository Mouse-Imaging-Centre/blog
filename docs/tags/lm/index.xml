<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">  
  <channel>
    <title>Lm on The Mouse Imaging Centre Blog</title>
    <link>/blog/tags/lm/</link>
    <description>Recent content in Lm on The Mouse Imaging Centre Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 25 Jan 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/blog/tags/lm/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Linear Models</title>
      <link>/blog/post/linearmodels/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/blog/post/linearmodels/</guid>
      <description><![CDATA[
      <div id="S1" class="section level1">
<h1>Preamble</h1>
<p>The purpose of this post is to elucidate some of the concepts associated with statistical linear models.</p>
<p>Let’s start by loading some libraries.</p>
<pre class="r"><code>library(ggplot2)
library(datasets)</code></pre>
</div>
<div id="S2" class="section level1">
<h1>Background Theory</h1>
<p>The basic idea is as follows:</p>
<p>Given two variables, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, for which we’ve measured a set of data points <span class="math inline">\(\{x_i, y_i\}\)</span> with <span class="math inline">\(i = 1, ..., n\)</span>, we want to estimate a function, <span class="math inline">\(f(x)\)</span>, such that</p>
<p><span class="math display">\[y_i = f(x_i) + \epsilon_i\]</span></p>
<p>for each data point <span class="math inline">\((x_i,y_i)\)</span>. Here <span class="math inline">\(\epsilon_i\)</span> is the error in data point <span class="math inline">\(y_i\)</span> compared to the predicted value <span class="math inline">\(f(x_i)\)</span>. Specifically,</p>
<p><span class="math display">\[\epsilon_i = y_i - f(x_i)\]</span></p>
<p>We don’t know <span class="math inline">\(f(x)\)</span>, but the simplest functional form that we can assume is that of a linear function:</p>
<p><span class="math display">\[f(x) = \beta_0 + \beta_1x \]</span> where <span class="math inline">\(\beta_0\)</span> is the intercept of the line and <span class="math inline">\(\beta_1\)</span> is the slope associated with the variable <span class="math inline">\(x\)</span>. Thus for each data point <span class="math inline">\(x_i\)</span>, we predict a value <span class="math inline">\(\hat{y}_i = f(x_i)\)</span> so that</p>
<p><span class="math display">\[\hat{y}_i = \beta_0 + \beta_1x_i\]</span> This is known as <strong>simple linear regression</strong>. Having specified the form of <span class="math inline">\(f(x)\)</span>, the problem becomes one of optimizing the free parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> so that the predicted or trend line best describes the real data <span class="math inline">\(\{x_i, y_i\}\)</span>. This is usually accomplished using the “ordinary least-squares” method, in which we minimize the sum of squared errors with respect to the free parameters. Explicitly, if we write the sum of squared errors as</p>
<p><span class="math display">\[\chi^2 = \sum_{i=1}^n{\epsilon_i^2} = \sum_{i=1}^n{(y_i - \hat{y}_i)^2} = \sum_{i=1}^n{(y_i - \beta_0 - \beta_1x_i)^2}\]</span> we want to determine <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that</p>
<p><span class="math display">\[\frac{\partial\chi^2}{\partial\beta_0} = 0\]</span> and <span class="math display">\[\frac{\partial\chi^2}{\partial\beta_1} = 0\]</span></p>
<p>This can be solved analytically. In practice we let the computer do it for us.</p>
<p>Moving on , there’s no reason we need to restrict ourselves to one predictor for the response variable <span class="math inline">\(y\)</span>, so we can include multiple variables <span class="math inline">\(\{x_1, x_2, ..., x_N\}\)</span> in our model. Here <span class="math inline">\(N\)</span> is the total number of regressors that are included. This is known as <strong>multiple linear regression</strong>. The model then becomes</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \sum_{a=1}^N{} \beta_ax_a\]</span> where <span class="math inline">\(a\)</span> is an index summing over the predictors (rather than over the data points themselves as in the expression for <span class="math inline">\(\chi^2\)</span> above). Here I’ve expressed the model in terms of the variables, rather than individual data points. For an individual data point <span class="math inline">\(\{x_i,y_i\}\)</span>, we could write this as</p>
<p><span class="math display">\[\hat{y}_i = \beta_0 + \sum_{a=1}^N{} \beta_ax_{a,i}\]</span> where <span class="math inline">\(x_{a,i}\)</span> is the <span class="math inline">\(i\)</span>th data point of the <span class="math inline">\(a\)</span>th variable (e.g. the height, which is the variable, of a specific person). The two are identical representations. The optimization process for multiple linear regression is the same as that for simple linear regression, only involving more derivatives.</p>
<p>Alright that should be enough background math. In the next Section, we’ll look at this in practice.</p>
</div>
<div id="S3" class="section level1">
<h1>Simple Linear Regression in Practice</h1>
<p>We’ll be working with the <code>mtcars</code> dataset.</p>
<pre class="r"><code>str(mtcars)</code></pre>
<pre><code>## &#39;data.frame&#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</code></pre>
<p>The description of these variables can be found <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html">here</a>. We’re going to start by looking at the relationship between two continuous variables. Specifically, I’ve chosen to examine the relationship between car weight, <code>wt</code>, and fuel efficiency, <code>mpg</code>. Let’s start by creating a scatter plot to look at how the fuel efficiency varies with car weight.</p>
<pre class="r"><code>p.mpg_vs_wt &lt;- ggplot(mtcars, aes(x=wt,y=mpg)) + geom_point()
p.mpg_vs_wt + 
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Scatter Plot of MPG vs. Weight&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>This is as we would expect, since less energy is needed to move lighter cars. Moreover we suspect that this data might be well suited to a simple linear model. As described above, the model we will be building is</p>
<p><span class="math display">\[y = \beta_0 + \beta_1x + \epsilon\]</span></p>
<p>or</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the <code>mpg</code> variable in this case and <span class="math inline">\(x\)</span> is the <code>wt</code> variable. In <code>R</code> formula notation, we can express this as <code>mpg ~ wt</code>, where <code>~</code> means “is modelled by”. The way to build a linear model in <code>R</code> is using the <code>lm()</code> function, as follows:</p>
<pre class="r"><code>mpg_vs_wt &lt;- summary(lm(mpg ~ wt,data=mtcars))
print(mpg_vs_wt)</code></pre>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5432 -2.3647 -0.1252  1.4096  6.8727 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***
## wt           -5.3445     0.5591  -9.559 1.29e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.046 on 30 degrees of freedom
## Multiple R-squared:  0.7528, Adjusted R-squared:  0.7446 
## F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10</code></pre>
<p>The focus of this document will be on interpreting the <code>Estimate</code> column of the <code>Coefficients</code> table. Let’s pull this table from the output:</p>
<pre class="r"><code>print(mpg_vs_wt$coefficients)</code></pre>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 37.285126   1.877627 19.857575 8.241799e-19
## wt          -5.344472   0.559101 -9.559044 1.293959e-10</code></pre>
<p>This is fairly straightforward to interpret. Looking at the <code>Estimate</code> column, the <code>(Intercept)</code> value describes the value of <span class="math inline">\(\beta_0\)</span> in our model, while the <code>wt</code> value describes the slope associated with the <code>wt</code> variable, i.e. <span class="math inline">\(\beta_1\)</span>. Our fitted model looks like this:</p>
<p><span class="math display">\[\hat{y} = 37.3- 5.3x\]</span> where <span class="math inline">\(x\)</span> is the <code>wt</code> variable and <span class="math inline">\(y\)</span> is the <code>mpg</code> variable, as mentioned above.</p>
<p>Let’s see how this looks.</p>
<pre class="r"><code>p.mpg_vs_wt + 
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Simple Linear Model: MPG vs. Weight&quot;) + 
  geom_abline(intercept = mpg_vs_wt$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;], 
              slope = mpg_vs_wt$coefficients[&quot;wt&quot;, &quot;Estimate&quot;]) + 
  coord_cartesian(xlim=c(0,6),ylim = c(5,40))</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>That looks pretty good. A second-order polynomial might better capture the data at the lower and higher <code>wt</code> values, but we won’t go into that here.</p>
</div>
<div id="S4" class="section level1">
<h1>Simple Linear Regression with Categorical Variables</h1>
<p>In the previous Section, we looked at how simple linear regression works when the predictor is a continuous variable, like <code>wt</code>. Here we will examine what happens when we model categorical variables. Recall that a categorical variable is a variable that takes on discrete, usually non-numerical, values. For example, sex is a categorical variable, with the values being male or female.</p>
<p>In the <code>mtcars</code> dataset, we’ll look at the <code>am</code> variable, which describes whether the car has manual or automatic transmission. Let’s explicitly express this as a factor and display the unique values.</p>
<pre class="r"><code>mtcars$am &lt;- as.factor(mtcars$am)
unique(mtcars$am)</code></pre>
<pre><code>## [1] 1 0
## Levels: 0 1</code></pre>
<p>So <code>am</code> has two possible values. Manual transmission is encoded as <code>am = 1</code> while automatic transmission is encoded as <code>am = 0</code> (refer to the <a href="https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/mtcars.html">dataset description</a>). As we did above, we can examine how <code>wt</code> varies with the <code>am</code> variable. Again we are fitting a model</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1A\]</span> where <span class="math inline">\(y\)</span> is the <code>wt</code> variable and <span class="math inline">\(A\)</span> is the <code>am</code> variable. Keep in mind that <span class="math inline">\(A\)</span> is binary. In <code>R</code>:</p>
<pre class="r"><code>mpg_vs_am &lt;- summary(lm(mpg ~ am, data=mtcars ))
print(mpg_vs_am$coefficients)</code></pre>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 17.147368   1.124603 15.247492 1.133983e-15
## am1          7.244939   1.764422  4.106127 2.850207e-04</code></pre>
<p>Thus we see that <span class="math inline">\(\beta_0\)</span> = 17.1 and <span class="math inline">\(\beta_1\)</span> = 7.2. Note here that in the <code>Coefficients</code> table, the slope estimate is associated with the label <code>am1</code>. This means that it displays the slope going from <code>am = 0</code> to <code>am = 1</code>. This is perhaps expected in this case since 1 is greater than 0, but in general categorical variables will not have numerical values. Consider again a variable describing sex. The two instances are “Male” and “Female”. There is no specific order in which to compute the slope. When building a model with categorical variables such as these, <code>R</code> will implicitly assign values of 0 and 1 to the levels of the variable. By default, <code>R</code>, assigns the <strong>reference level</strong>, i.e. a value of 0, to the value that is <strong>lowest in alphabetical order</strong>. For the sex variable, “Female” would be associated with 0, and “Male” with 1. So when running <code>lm()</code> on such a model and printing the output, the <code>Coefficients</code> table will have a row with a name like <code>sexMale</code>. The <code>Estimate</code> value associated with this describes the slope of the model going from <code>sex=Female</code> (which is implicitly defined as 0) to <code>sex=Male</code> (which is implicitly defined as 1). This might seem unnecessary right now, but it becomes important when trying to interpret the output from more complex models, as we’ll see below.</p>
<p>As above, we can plot this data and model.</p>
<pre class="r"><code>ggplot(mtcars, aes(x=am,y=mpg)) + 
  geom_jitter(width=0.1) + 
  geom_smooth(method=&quot;lm&quot;, formula = y~x, aes(group=1), se=F) + 
  labs(x = &quot;Transmission (0 = automatic)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Simple Linear Model: MPG vs. Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="S5" class="section level1">
<h1>Multiple Linear Regression in Practice</h1>
<p>In this Section we will combine the models built in the two previous Sections using multiple linear regression. Specifically, we will model <code>mpg</code> by <code>wt</code> and <code>am</code> together.</p>
<p>Let’s start by taking a look at the data.</p>
<pre class="r"><code>ggplot(mtcars, aes(x=wt, y=mpg, col=am)) + 
  geom_point() +  
   labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Fuel Efficiency, Weight, and Transmission&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Again, I’ve plotted <code>mpg</code> against <code>wt</code> using a scatter plot, but I’ve mapped the colour aesthetic to the <code>am</code> variable to see the group variation.</p>
<p>Mathematically, the model we will use looks like</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2A\]</span> where, as above, <span class="math inline">\(y\)</span> is the <code>mpg</code> variable, <span class="math inline">\(x\)</span> is the <code>wt</code> continuous variable, and <code>A</code> is the categorical <code>am</code> variable. <span class="math inline">\(\beta_1\)</span> is the slope associated with <span class="math inline">\(x\)</span> and <span class="math inline">\(\beta_2\)</span> is the slope associated with <span class="math inline">\(A\)</span>. Recall that <span class="math inline">\(A\)</span> is only 0 or 1.</p>
<p>Let’s go ahead and build the model in <code>R</code>.</p>
<pre class="r"><code>mpg_MLR &lt;- summary(lm(mpg ~ wt + am, data=mtcars))
print(mpg_MLR$coefficients)</code></pre>
<pre><code>##                Estimate Std. Error     t value     Pr(&gt;|t|)
## (Intercept) 37.32155131  3.0546385 12.21799285 5.843477e-13
## wt          -5.35281145  0.7882438 -6.79080719 1.867415e-07
## am1         -0.02361522  1.5456453 -0.01527855 9.879146e-01</code></pre>
<p>Again, keeping in mind the mathematical formula, we see that <code>(Intercept)</code> corresponds to <span class="math inline">\(\beta_0\)</span> = 37.32, <code>wt</code> corresponds to <span class="math inline">\(\beta_1\)</span> = -5.35 and <code>am1</code> corresponds to <span class="math inline">\(\beta_2\)</span> = -0.02. So how do we interpret this? How could we sketch this model on a scatter plot of <span class="math inline">\(y\)</span> vs <span class="math inline">\(x\)</span> like the one above? The way to go about it is to examine the two cases for our categorical variables <span class="math inline">\(A\)</span> = <code>am</code>. Recall that, regardless of what our factor levels are (0/1, Female/Male), <code>R</code> always encodes categorical variables as 0 and 1. Consequently we can always examine our mathematical model by setting the corresponding variable to 0 or 1. For <span class="math inline">\(A\)</span> = <code>am</code> = 0, we have</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span> In this case, when <span class="math inline">\(A\)</span> = 0, <span class="math inline">\(\beta_0\)</span> is the intercept of the line relating <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, and <span class="math inline">\(\beta_1\)</span> is the slope associated with <span class="math inline">\(x\)</span>. On the scatter plot, we would draw a line with this slope and intercept. What about when <span class="math inline">\(A = 1\)</span>?</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2\]</span> <span class="math display">\[= (\beta_0 + \beta_2) + \beta_1x\]</span> <span class="math display">\[= \beta_0&#39; + \beta_1x\]</span></p>
<p>We find that we actually have a new intercept value <span class="math inline">\(\beta_0&#39; = \beta_0 + \beta_2\)</span>, but the same slope <span class="math inline">\(\beta_1\)</span>. Thus the trend line associated with <span class="math inline">\(A = 1\)</span> has a different intercept than that associated with <span class="math inline">\(A = 0\)</span>. What we’ve discovered is that the <span class="math inline">\(\beta_2\)</span> parameter tells us the <strong>difference in the intercept values</strong> between the <code>am = 0</code> and <code>am = 1</code> groups, i.e. <span class="math inline">\(\beta_2 = \beta_0&#39; - \beta_0\)</span>. The <span class="math inline">\(\beta_1\)</span> parameter tells us the slope of the two lines.</p>
<p>Let’s see what this looks like.</p>
<pre class="r"><code>beta0 &lt;- mpg_MLR$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;]
beta1 &lt;- mpg_MLR$coefficients[&quot;wt&quot;,&quot;Estimate&quot;]
beta0_prime &lt;- mpg_MLR$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;] + 
                mpg_MLR$coefficients[&quot;am1&quot;,&quot;Estimate&quot;]

ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&quot;red&quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1, 
              col=&quot;blue&quot;,
              alpha=0.5) + 
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Multiple Linear Model: MPG ~ Weight + Transmission&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>There are actually two lines in this plot, a blue one and a red one. Given how small <span class="math inline">\(\beta_2\)</span> is we can’t see much of a difference, but the blue trend line should be slightly lower than the red line. Their slopes are the same. Let’s zoom in to be sure.</p>
<pre class="r"><code>ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&quot;red&quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1, 
              col=&quot;blue&quot;,
              alpha=0.5) + 
  coord_cartesian(xlim = c(3.4,3.5),ylim = c(17.5,19.5)) + 
    labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Multiple Linear Model: MPG ~ Weight + Transmission (Zoomed In)&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>There we go. This is the effect of a multiple linear model with no interactions. In general the trend lines for the two groups don’t have to be so close together. It all depends on the data. In this case we see that, when we impose a fixed slope (using <code>mpg ~ wt + am</code>), the trend lines describing the two groups are basically the same. Next, we’ll look at how adding interactions to our model results in slope variation.</p>
</div>
<div id="S6" class="section level1">
<h1>Multiple Linear Regression with Interactions</h1>
<p>In the previous Section we examined the use of multiple linear regression to model a response variable in terms of continuous and categorical predictors. We can take this a step further by including an <strong>interaction</strong> in our model. What does this mean? An interaction describes how a change in one predictor influences change in another predictor. In mathematics this is typically expressed by including a product or more complex term in an equation. A simple product of two variables is the simplest interaction term that we can write down. Consider once again our model of <code>mpg</code> vs. <code>wt</code> and <code>am</code>. This time we will add an interaction:</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2A + \beta_3xA\]</span></p>
<p>The model is as above, with the exception of the new interaction term <span class="math inline">\(\beta_3xA\)</span>. <span class="math inline">\(xA\)</span> is the product interaction while <span class="math inline">\(\beta_3\)</span> is the model parameter associated with this interaction.</p>
<p><strong>An important note</strong>: In <code>R</code> formula notation, interactions are expressed as <code>wt*am</code>. This includes all terms in the model, i.e. <code>wt*am</code> = <code>1 + wt + am + wt:am</code> where <code>1</code> stands in for the “variable” associated with the <span class="math inline">\(\beta_0\)</span> parameter, and the <strong>colon denotes a product</strong>. We can thus write our full interactive model as <code>mpg ~ wt*am</code>.</p>
<p>Let’s run this model through <code>lm()</code> and see what happens.</p>
<pre class="r"><code>mpg_int &lt;- summary(lm(mpg ~ wt*am,data=mtcars))
print(mpg_int$coefficients)</code></pre>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 31.416055  3.0201093 10.402291 4.001043e-11
## wt          -3.785908  0.7856478 -4.818836 4.551182e-05
## am1         14.878423  4.2640422  3.489277 1.621034e-03
## wt:am1      -5.298360  1.4446993 -3.667449 1.017148e-03</code></pre>
<p>We have four rows for our four parameters. Again each row corresponds to one <span class="math inline">\(\beta\)</span> parameter: <code>(Intercept)</code> corresponds to <span class="math inline">\(\beta_0\)</span> = 31.42, <code>wt</code> corresponds to <span class="math inline">\(\beta_1\)</span> = -3.79, <code>am1</code> corresponds to <span class="math inline">\(\beta_2\)</span> = 14.88 and <code>wt:am1</code> corresponds to <span class="math inline">\(\beta_3\)</span> = -5.3. Let’s interpret this, as we did in the previous Section, by examining the different values of the categorical variable <code>am</code>.</p>
<p>When <span class="math inline">\(A\)</span> = <code>am</code> = 0, we have</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span> which is just our simple model. So <span class="math inline">\(\beta_0\)</span> describes the intercept when <span class="math inline">\(A = 0\)</span> and <span class="math inline">\(\beta_1\)</span> describes the slope associated with <span class="math inline">\(x\)</span> when <span class="math inline">\(A = 0\)</span>. What happens when <span class="math inline">\(A = 1\)</span>?</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x + \beta_2 + \beta_3x\]</span> <span class="math display">\[= (\beta_0 + \beta_2) + (\beta_1 + \beta_3)x\]</span> <span class="math display">\[= \beta_0&#39; + \beta_1&#39;x\]</span></p>
<p>So we actually have a new intercept and a new slope! The new intercept is the same as in the previous Section: <span class="math inline">\(\beta_0&#39; = \beta_0 + \beta_2\)</span>. The new slope is <span class="math inline">\(\beta_1&#39; = \beta_1 + \beta_3\)</span>. Therefore, a simple product interaction like this one causes the slope of <span class="math inline">\(y\)</span> with <span class="math inline">\(x\)</span> to change as we move from <span class="math inline">\(A = 0\)</span> to <span class="math inline">\(A = 1\)</span>. Displaying this on a scatter plot of <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>, we would have two lines with different intercepts and different slopes.</p>
<p>Let’s take a look.</p>
<pre class="r"><code>beta0 = mpg_int$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;]
beta1 = mpg_int$coefficients[&quot;wt&quot;,&quot;Estimate&quot;]
beta0_prime = mpg_int$coefficients[&quot;(Intercept)&quot;,&quot;Estimate&quot;] + 
                mpg_int$coefficients[&quot;am1&quot;,&quot;Estimate&quot;]
beta1_prime = mpg_int$coefficients[&quot;wt&quot;,&quot;Estimate&quot;] + 
                mpg_int$coefficients[&quot;wt:am1&quot;,&quot;Estimate&quot;]

ggplot(mtcars, aes(x = wt, y = mpg, col = am)) + 
  geom_point() + 
  geom_abline(intercept = beta0,
         slope = beta1,
         col=&quot;red&quot;,
         alpha=0.5) + 
  geom_abline(intercept = beta0_prime,
              slope = beta1_prime, 
              col=&quot;blue&quot;,
              alpha=0.5) + 
  coord_cartesian(xlim=c(0,6),
                  ylim = c(0,50)) +
  labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Weight-Transmission Interaction&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Perfect. Compared to the non-interactive model in the previous Section, we see that adding an interaction (and thus allowing the slope of <span class="math inline">\(y\)</span> with <span class="math inline">\(x\)</span> to vary between groups) better characterizes the data. If the data was truly not well characterized by an interaction, the equal-slope model of the previous Section would perform just as well as an interactive model of this kind. Of course this would have to be determined by examining the statistics associated with the models.</p>
<p>This concludes the majority of what I wanted to cover. In the next Section I’ll go into some of the heavier mathematical details regarding linear models. Read ahead at your own peril.</p>
</div>
<div id="S7" class="section level1">
<h1>Mathematical Embellishments</h1>
<p>The previous analysis was focussed on examining linear models built with a continuous and categorical predictor. We’re by no means restricted to this however. We can build a linear model with as many variables as we’d like. As we saw above, the case of one continuous predictor and one categorical predictor can be visualized fairly easily using a scatter plot, where the continous data is mapped to the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axes and the categorical data is mapped to a colour/shape/style aesthetic. This becomes harder to do when we’re examining models that use multiple continuous predictors, e.g. something of the form</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2\]</span></p>
<p>Here we have an interactive multiple linear regression with two continuous regressors <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. A practical example of this would be modelling the <code>mpg</code> variable in terms of the <code>wt</code> and horsepower, <code>hp</code>. Let’s plot this in the same way we did above, where the <code>hp</code> variable is mapped to the colour aesthetic.</p>
<pre class="r"><code>ggplot(mtcars, aes(x=wt, y=mpg, col=hp)) + 
  geom_point() +  
   labs(x = &quot;Weight (1000 lbs)&quot;,
       y = &quot;Miles per Gallon&quot;,
       title = &quot;Fuel Efficiency, Weight, and Horsepower&quot;,
       colour=&quot;Transmission&quot;)</code></pre>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Given the continuous nature of the <code>hp</code> variable, this is difficult to interpret by eye. That doesn’t invalidate the model however, and we can still estimate the optimal <span class="math inline">\(\beta\)</span> parameters.</p>
<pre class="r"><code>mpg_hp &lt;- summary(lm(mpg ~ wt*hp, data=mtcars))
print(mpg_hp$coefficients)</code></pre>
<pre><code>##                Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 49.80842343 3.60515580 13.815887 5.005761e-14
## wt          -8.21662430 1.26970814 -6.471270 5.199287e-07
## hp          -0.12010209 0.02469835 -4.862758 4.036243e-05
## wt:hp        0.02784815 0.00741958  3.753332 8.108307e-04</code></pre>
<p>The rows in this table still represent the <span class="math inline">\(\beta\)</span> parameters in the model, as before, but it is much harder to interpret this result in the context of a scatter plot of <code>mpg</code> vs. <code>wt</code>. We can’t just set <code>hp = 0</code> and <code>hp = 1</code> as we did previously. The equivalent here would be setting <code>hp</code> to an infinite number of incremental values. This isn’t the right way to think about this. This sort of brings us face to face with what the multiple linear model is actually saying. Consider again simple linear regression:</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x\]</span> This is clearly the expression for a line. The variable <span class="math inline">\(y\)</span> is modelled as a linear function of <span class="math inline">\(x\)</span>. As we know, we can plot this easily on a two-dimensional space, where <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> form the axes. Moving to multiple regression, we have</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2\]</span></p>
<p>In Sections <a href="#S5">5</a> and <a href="#S6">6</a>, we interpreted this model <strong>within the context of the scatter plot of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y\)</span></strong>. With <span class="math inline">\(x_2\)</span> as a categorical variable, this allowed us to interpret <span class="math inline">\(\beta_2\)</span> as the difference in intercept values on this scatter plot. Mathematically, however, this expression describes a <strong>two-dimensional plane</strong> characterised by the variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. We see that the intercept of the plane is <span class="math inline">\(\beta_0\)</span>, i.e. the value of <span class="math inline">\(y\)</span> at which both the parametric variables are 0. Moreover, <span class="math inline">\(\beta_1\)</span> is the slope of the plane with respect to <span class="math inline">\(x_1\)</span>, and <span class="math inline">\(\beta_2\)</span> is the slope of the plane with respect to <span class="math inline">\(x_2\)</span>. Specifically,</p>
<p><span class="math display">\[\beta_1 = \frac{\partial\hat{y}}{\partial x_1}\]</span></p>
<p>and</p>
<p><span class="math display">\[\beta_2 = \frac{\partial\hat{y}}{\partial x_2}\]</span> We can visualize this two-dimensional plane in a three-dimensional space, where the third axis is represented by the <span class="math inline">\(y\)</span> variable. You can do this easily by grabbing a piece of paper, or preferably a rigid flat object, and orienting it in front of you in real space. You can imagine the <span class="math inline">\(y\)</span> axis as the vertical axis, and the <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> axes as the horizontal axes. The idea of a flat plane in a multi-dimensional space extends to any number of predictors in our model, provided that the model is non-interactive. Given <span class="math inline">\(N\)</span> predictors <span class="math inline">\(\{x_1, x_2, ..., x_N\}\)</span>, a model of the form</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \sum_{a=1}^N{\beta_ax_a}\]</span></p>
<p>describes a N-dimensional hyperplane embedded in a (N+1)-dimensional space. Crazy. This is undoubtedly true of continuous variables, but is a little bit more nuanced for a categorical variable. Going back to our model of <code>mpg</code> vs. <code>wt</code> and <code>am</code>, we can still imagine this in a three-dimensional space. The axes of the space are <code>mpg</code>,<code>wt</code> and <code>am</code>, but notice that we aren’t actually dealing with a plane in this case, since <code>am</code> only takes on binary values. Rather we are dealing with two different lines embedded in this three-dimensional space. One line will occur at <code>am = 0</code>, while the other occurs at <code>am = 1</code>. Given this context, we can re-interpret the scatter plots from Section <a href="#S5">5</a> and <a href="#S6">6</a>. Here is the scatter plot and model from Section <a href="$S6">6</a>:</p>
<p><img src="/blog/post/LinearModels_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Within the proper context of a three-dimensional space, this two-dimensional plot is actually the <strong>projection of the binary <code>am</code> axis onto <code>am = 0</code></strong>. Imagine the red line existing in your computer screen and the blue line actually existing outside of your computer screen, closer to you. I’ve used the interactive model here since the plot is nicer than that for the non-interactive model, but this leads us into a discussion of interactions with continuous variables.</p>
<p>Examining the plot above, you might already guess where this is going. Let’s consider a model with a simple product interaction of two continuous variables, like the one at the beginning of this Section:</p>
<p><span class="math display">\[\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_3x_1x_2\]</span></p>
<p>We saw that when <span class="math inline">\(\beta_3 = 0\)</span>, the model describes a two-dimensional plane in three-dimensions. What does the interaction do to this plane? Notice that the product interaction is actually a second-order term in the expression for <span class="math inline">\(\hat{y}\)</span>. From univariate calculus, we know that second order terms are responsible for the curvature of a function. The same is true in multivariate calculus. The result is that the plane is no longer a plane, but rather a <strong>curved</strong> two-dimensional surface (or manifold, if you want to be fancy), embedded in a three-dimensional space. The nature of this curvature is unique as well, since it involves coupling between the two regressors, i.e. the surface changes in the <span class="math inline">\(x_1\)</span> direction as we move along the <span class="math inline">\(x_2\)</span> direction, and vice versa. This is apparent when looking at the partial derivatives:</p>
<p><span class="math display">\[\frac{\partial \hat{y}}{\partial x_1} = \beta_1 + \beta_3x_2 \]</span></p>
<p>and</p>
<p><span class="math display">\[\frac{\partial \hat{y}}{\partial x_2} = \beta_2 + \beta_3x_1 \]</span></p>
<p>We can further characterize the modelled surface if we’d like. For instance, since there aren’t any single-variable higher order terms, e.g. <span class="math inline">\(x_1^2\)</span>, <span class="math inline">\(x_1^3\)</span>, etc., we know that, for a given value of one of the predictors, the response variable varies linearly with the other predictor. You can verify this by setting one of the predictors to 0. Moreover since there are no terms of order higher than 2, we know that this surface has a constant curvature. This can be verified by computing the 2nd order partial derivatives. These steps maybe aren’t necessary, but they give us an idea as to how to interpret the effect of a simple product interaction.</p>
<p>In general, a multiple linear regression with <span class="math inline">\(N\)</span> predictors and various interactions between those predictors will describe a curved surface or manifold in a (N+1)-dimensional space. The more complex the interactions between the predictors, the more elaborate the surface will be. In analogy to Section <a href="#S2">2</a> such a surface will be characterized by a function, <span class="math inline">\(f\)</span>, so that</p>
<p><span class="math display">\[y = f[\{x_a\}_{a=1}^N] + \epsilon\]</span> for a response variable <span class="math inline">\(y\)</span> and predictors <span class="math inline">\(x_a\)</span>. Estimating complex surfaces such as these is the purpose of most statistical and machine learning techniques.</p>
</div>
]]>
      </description>
    </item>
    
  </channel>
</rss>
